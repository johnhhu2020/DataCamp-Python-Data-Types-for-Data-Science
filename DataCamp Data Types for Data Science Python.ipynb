{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae49f5b-607f-4792-b96f-621b120b1bb0",
   "metadata": {},
   "source": [
    "## Fundamental data types\n",
    "\n",
    "This chapter will introduce you to the fundamental Python data types - lists, sets, and tuples. These data containers are critical as they provide the basis for storing and looping over ordered data. To make things interesting, you'll apply what you learn about these types to answer questions about the New York Baby Names dataset!\n",
    "\n",
    "    Introduction and lists    50 xp\n",
    "    Manipulating lists for fun and profit    100 xp\n",
    "    Looping over lists    100 xp\n",
    "    Meet the Tuples    50 xp\n",
    "    Data type usage    50 xp\n",
    "    Using and unpacking tuples    100 xp\n",
    "    Making tuples by accident    100 xp\n",
    "    Sets for unordered and unique data    50 xp\n",
    "    Finding all the data and the overlapping data between sets    100 xp\n",
    "    Determining set differences    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e493a5-caef-42d2-bef8-f4b7d04c7083",
   "metadata": {},
   "source": [
    "## Dictionaries - the root of Python\n",
    "\n",
    "At the root of all things Python is a dictionary. Herein, you'll learn how to use them to safely handle data that can viewed in a variety of ways to answer even more questions about the New York Baby Names dataset. You'll explore how to loop through data in a dictionary, access nested data, add new data, and come to appreciate all of the wonderful capabilities of Python dictionaries.\n",
    "\n",
    "    Using dictionaries    50 xp\n",
    "    Creating and looping through dictionaries    100 xp\n",
    "    Safely finding by key    100 xp\n",
    "    Dealing with nested data    100 xp\n",
    "    Altering dictionaries    50 xp\n",
    "    Adding and extending dictionaries    100 xp\n",
    "    Popping and deleting from dictionaries    100 xp\n",
    "    Pythonically using dictionaries    50 xp\n",
    "    Working with dictionaries more pythonically    100 xp\n",
    "    Checking dictionaries for data    100 xp\n",
    "    Working with CSV files    50 xp\n",
    "    Reading from a file using CSV reader    100 xp\n",
    "    Creating a dictionary from a file    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2feacc-b21d-4c14-8179-345d06edbb4c",
   "metadata": {},
   "source": [
    "## Meet the collections module\n",
    "\n",
    "The collections module is part of Python's standard library and holds some more advanced data containers. You'll learn how to use the Counter, defaultdict, OrderedDict and namedtuple in the context of answering questions about the Chicago transit dataset.\n",
    "\n",
    "    Counting made easy    50 xp\n",
    "    Using Counter on lists    100 xp\n",
    "    Finding most common elements    100 xp\n",
    "    Dictionaries of unknown structure - Defaultdict    50 xp\n",
    "    Creating dictionaries of an unknown structure    100 xp\n",
    "    Safely appending to a key's value list    100 xp\n",
    "    Maintaining Dictionary Order with OrderedDict    50 xp\n",
    "    Working with OrderedDictionaries    100 xp\n",
    "    Powerful Ordered popping    100 xp\n",
    "    What do you mean I don't have any class? Namedtuple    50 xp\n",
    "    Creating namedtuples for storing data    100 xp\n",
    "    Leveraging attributes on namedtuples    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b0290-d0b3-42dc-8b47-a0d839d51fdb",
   "metadata": {},
   "source": [
    "## Handling Dates and Times\n",
    "\n",
    "Handling times can seem daunting at time, but here, you'll dig in and learn how to create datetime objects, print them, look to the past and to the future. Additionally, you'll learn about some third party modules that can make all of this easier. You'll continue to use the Chicago Transit dataset to answer questions about transit times.\n",
    "\n",
    "    There and Back Again a DateTime Journey    50 xp\n",
    "    Strings to DateTimes    100 xp\n",
    "    Converting to a String    100 xp\n",
    "    Working with Datetime Components and current time    50 xp\n",
    "    Pieces of Time    100 xp\n",
    "    Creating DateTime Objects... Now    100 xp\n",
    "    Timezones    100 xp\n",
    "    Time Travel (Adding and Subtracting Time)    50 xp\n",
    "    Finding a time in the future and from the past    100 xp\n",
    "    Finding differences in DateTimes    100 xp\n",
    "    HELP! Libraries to make it easier    50 xp\n",
    "    Localizing time with pendulum    100 xp\n",
    "    Humanizing Differences with Pendulum    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf74e0-c74f-4e43-ae5d-c05305c1f325",
   "metadata": {},
   "source": [
    "## Answering Data Science Questions\n",
    "\n",
    "Time for a case study to reinforce all of your learning so far! You'll use all the containers and data types you've learned about to answer several real world questions about a dataset containing information about crime in Chicago. Have fun!\n",
    "\n",
    "    Counting within Date Ranges    50 xp\n",
    "    Reading your data with CSV Reader and Establishing your Data Containers    100 xp\n",
    "    Find the Months with the Highest Number of Crimes    100 xp\n",
    "    Transforming your Data Containers to Month and Location    100 xp\n",
    "    Find the Most Common Crimes by Location Type by Month in 2016    100 xp\n",
    "    Dictionaries with Time Windows for Keys    50 xp\n",
    "    Reading your Data with DictReader and Establishing your Data Containers    100 xp\n",
    "    Determine the Arrests by District by Year    100 xp\n",
    "    Unique Crimes by City Block    100 xp\n",
    "    Final thoughts    50 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18098b49-d2ca-49e0-995b-c3aa71c467bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825bb60-833a-443c-b970-bddc75c13070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7c2594b-7fae-4398-93b6-032fb203680f",
   "metadata": {},
   "source": [
    "## Introduction and lists\n",
    "\n",
    "\n",
    "\n",
    "## Data Types\n",
    "\n",
    "   **In most programming langurages, the data type system set the stage for the capability of the langurage. Understanding how to use the fundamental data types of a langurage greatly empowers you as a data scientist.  After already encountered integers and strings, lets start with container sequences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4c724-5a9d-413f-adab-1850c86c5015",
   "metadata": {},
   "source": [
    "## Container sequence\n",
    "\n",
    "   __Hold other types of data__\n",
    "   __Used for aggregation, sorting, and more__\n",
    "   __Can be Mutable(list, set) or Immutable(tuple)__\n",
    "   __Iterable__\n",
    "   \n",
    "   \n",
    "   **A container sequence gets its name because it holds a sequence of elements of other data types. We'll use these containers to store our data for aggregation, order, sorting, and more.  \n",
    "   \n",
    "   **Python provides several container sequences, such as list, set, and tuples etc. They can be mutable meaning that they can have elements added and removed from them.  Immutability, not able to be altered, allows us to protect our reference data, and replace individual data points with sum, averages, derivations etc.  \n",
    "   \n",
    "   **We can iterate, also know as looping, over the data contained within these containers. Being able to iterate over these sequences allows us to group data, aggregate it and process it over time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5014b09-59d5-4c40-83c7-4fc003d4a188",
   "metadata": {},
   "source": [
    "## List\n",
    "\n",
    "  __Orded__\n",
    "  __Mutable__\n",
    "  __Index__\n",
    "\n",
    "**Allows us to hold an orded collection of items, its mutable, and also allows us to access an individual element within them using an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed151d6-b686-473b-b6c8-8c49cef56e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocolate cookie', 'peanut butter', 'sugar']\n",
      "['chocolate cookie', 'peanut butter', 'sugar', 'Tirggel']\n",
      "sugar\n"
     ]
    }
   ],
   "source": [
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "\n",
    "print(cookies)\n",
    "\n",
    "cookies.append(\"Tirggel\")\n",
    "\n",
    "print(cookies)\n",
    "\n",
    "print(cookies[2])\n",
    "\n",
    "del cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a5cfacd3-f761-4691-8f1b-1832ef1071a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocolate cookie', 'peanut butter', 'sugar', 'strawberry', 'vanilla']\n"
     ]
    }
   ],
   "source": [
    "## Combine two list\n",
    "\n",
    "\n",
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "cakes = [\"strawberry\", \"vanilla\"]\n",
    "\n",
    "# *************************************************************************************************** #\n",
    "disert = cookies + cakes\n",
    "print(disert)\n",
    "\n",
    "del cookies, cakes, disert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "705814f6-50ae-49f0-9477-d8eeec2b48b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocolate cookie', 'peanut butter', 'sugar', 'strawberry', 'vanilla']\n"
     ]
    }
   ],
   "source": [
    "# Combine two list\n",
    "\n",
    "\n",
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "cakes = [\"strawberry\", \"vanilla\"]\n",
    "\n",
    "# *************************************************************************************************** #\n",
    "cookies.extend(cakes)\n",
    "print(cookies)\n",
    "\n",
    "del cookies, cakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6c922b-c047-42e8-b6f4-ab66916868cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Finding element in a list\n",
    "\n",
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "\n",
    "peanut_butter_position = cookies.index(\"peanut butter\")\n",
    "print(peanut_butter_position)\n",
    "\n",
    "del peanut_butter_position, cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4ae9a5-d411-4ba1-87c2-94ff7a1ae583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocolate cookie', 'sugar']\n"
     ]
    }
   ],
   "source": [
    "# Removing elements in a list\n",
    "\n",
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "\n",
    "cookies.pop(1)\n",
    "\n",
    "print(cookies)\n",
    "\n",
    "del cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "583139ab-41c1-4e5f-8836-1793cf3c4072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate cookie\n",
      "peanut butter\n",
      "sugar\n"
     ]
    }
   ],
   "source": [
    "# Iterating over list\n",
    "\n",
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "\n",
    "for i in cookies:\n",
    "    print(i)\n",
    "    \n",
    "del cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da5a99a-184f-4dca-b764-28f9fdc23d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chocolate cookie', 'peanut butter', 'sugar']\n"
     ]
    }
   ],
   "source": [
    "# Sorting list\n",
    "\n",
    "  # sorted() function sort data in numerical or alphabetical order and returns a new list\n",
    "\n",
    "cookies = [\"chocolate cookie\", \"peanut butter\", \"sugar\"]\n",
    "\n",
    "new_list = sorted(cookies)\n",
    "\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4724777-043f-4a74-b231-2d7b53fb1fea",
   "metadata": {},
   "source": [
    "## Manipulating lists for fun and profit\n",
    "\n",
    "You may be familiar with adding individual data elements to a list by using the .append() method. However, if you want to combine a list with another array type (list, set, tuple), you can use the .extend() method on the list.\n",
    "\n",
    "You can also use the .index() method to find the position of an item in a list. You can then use that position to remove the item with the .pop() method.\n",
    "\n",
    "In this exercise, you'll practice using all these methods!\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a list called baby_names with the names 'Ximena', 'Aliza', 'Ayden', and 'Calvin'.\n",
    "    Use the .extend() method on baby_names to add 'Rowen' and 'Sandeep' and print the list.\n",
    "    Use the .index() method to find the position of 'Aliza' in the list. Save the result as position.\n",
    "    Use the .pop() method with position to remove 'Aliza' from the list.\n",
    "    Print the baby_names list. This has been done for you, so hit 'Submit Answer' to see the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5928fe2a-fac3-4f86-bf51-16fb38ec228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ximena', 'Aliza', 'Ayden', 'Calvin', ['Rowen', 'Sandeep']]\n",
      "['Ximena', 'Ayden', 'Calvin', ['Rowen', 'Sandeep']]\n"
     ]
    }
   ],
   "source": [
    "# Create a list containing the names: baby_names\n",
    "baby_names = [\"Ximena\", \"Aliza\", \"Ayden\", \"Calvin\"]\n",
    "\n",
    "# Extend baby_names with 'Rowen' and 'Sandeep'\n",
    "baby_names.append([\"Rowen\", \"Sandeep\"])\n",
    "#         .extend\n",
    "\n",
    "# Print baby_names\n",
    "print(baby_names)\n",
    "\n",
    "# Find the position of 'Aliza': position\n",
    "position = baby_names.index(\"Aliza\")\n",
    "\n",
    "# Remove 'Aliza' from baby_names\n",
    "baby_names.pop(position)\n",
    "\n",
    "# Print baby_names\n",
    "print(baby_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b85ac81b-c9d2-472b-8ffb-1e2b0b72b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ximena', 'Aliza', 'Ayden', 'Calvin', 'Rowen', 'Sandeep']\n",
      "['Ximena', 'Ayden', 'Calvin', 'Rowen', 'Sandeep']\n"
     ]
    }
   ],
   "source": [
    "# Create a list containing the names: baby_names\n",
    "baby_names = [\"Ximena\", \"Aliza\", \"Ayden\", \"Calvin\"]\n",
    "\n",
    "# Extend baby_names with 'Rowen' and 'Sandeep'\n",
    "baby_names.extend([\"Rowen\", \"Sandeep\"])\n",
    "\n",
    "# Print baby_names\n",
    "print(baby_names)\n",
    "\n",
    "# Find the position of 'Aliza': position\n",
    "position = baby_names.index(\"Aliza\")\n",
    "\n",
    "# Remove 'Aliza' from baby_names\n",
    "baby_names.pop(position)\n",
    "\n",
    "# Print baby_names\n",
    "print(baby_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03846d93-b27a-4210-a4b5-df90db2414cd",
   "metadata": {},
   "source": [
    "## Looping over lists\n",
    "\n",
    "You can use a for loop to iterate through all the items in a list. You can take that a step further with the sorted() function which will sort the data in a list from lowest to highest in the case of numbers and alphabetical order if the list contains strings.\n",
    "\n",
    "# The sorted() function returns a new list and does not affect the list you passed into the function. \n",
    "You can learn more about sorted() in the Python documentation.\n",
    "\n",
    "A list of lists, records has been pre-loaded. If you explore it in the IPython Shell, you'll see that each entry is a list of this form:\n",
    "\n",
    "['2011', 'FEMALE', 'HISPANIC', 'GERALDINE', '13', '75']\n",
    "\n",
    "The name of the baby ('GERALDINE') is the fourth entry of this list. Your job in this exercise is to loop over this list of lists and append the names of each baby to a new list called baby_names.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create an empty list called baby_names.\n",
    "    Use a for loop to iterate over each row of records appending the name, found in the fourth element of row, to baby_names.\n",
    "    Print each name in baby_names in alphabetical order. To do this:\n",
    "        Use the sorted() function as part of a for loop to iterate over the sorted names, printing each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d442ee-11f9-4489-b461-2fecc4bc6798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aarya', '10', '40'],\n",
       " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Abby', '27', '23']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = [['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aarya', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Abby', '27', '23'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Abigail', '31', '19'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aisha', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aiza', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aleena', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexandra', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alice', '24', '26'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alicia', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alina', '41', '10'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alisa', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alisha', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Allison', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alyssa', '22', '28'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amanda', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amber', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amelia', '27', '23'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amina', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amy', '31', '19'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anaya', '32', '18'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anayah', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angel', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angela', '77', '5'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angelina', '38', '12'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anna', '41', '10'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Annabelle', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anne', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Annie', '30', '20'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anya', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aria', '26', '24'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ariana', '30', '20'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arianna', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ariel', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arisha', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arya', '28', '22'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ashley', '33', '17'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Athena', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Audrey', '31', '19'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ava', '35', '15'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Avery', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayesha', '28', '22'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Bella', '29', '21'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Carina', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Caroline', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Catherine', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cecilia', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Celine', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Charlotte', '31', '19'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Chelsea', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Chloe', '111', '2'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christina', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cindy', '25', '25'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Claire', '49', '9'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Crystal', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cynthia', '22', '28'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Doris', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Eileen', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Elaine', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Eleanor', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Elena', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Eliana', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Elina', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Elizabeth', '20', '30'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ella', '40', '11'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ellie', '26', '24'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Elsa', '23', '27'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Emily', '92', '4'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Emma', '96', '3'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Erica', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Erin', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Eshaal', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Esther', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Eva', '28', '22'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Evelyn', '26', '24'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Fatima', '23', '27'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Fiona', '37', '13'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Gabriella', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Gia', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Grace', '64', '7'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Hafsa', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Hailey', '23', '27'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Hana', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Hanna', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Hannah', '32', '18'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Inaaya', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Inaya', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Irene', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Iris', '21', '29'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Isabel', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Isabella', '58', '8'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Isabelle', '23', '27'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ivy', '30', '20'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Janice', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jannat', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jasmine', '36', '14'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jennifer', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jenny', '20', '30'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jessica', '35', '15'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jessie', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Joanna', '32', '18'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Jocelyn', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Joy', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Joyce', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Julia', '26', '24'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Kaitlyn', '20', '30'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Karina', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Kate', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Katelyn', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Katherine', '37', '13'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Katie', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Kayla', '23', '27'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Kaylee', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Kelly', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Khadija', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Kylie', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Layla', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Leah', '25', '25'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Lila', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Lillian', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Lily', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Lina', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Lisa', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Livia', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Luna', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Lydia', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Madeline', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Madelyn', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Madison', '28', '22'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Maggie', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Mandy', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Maryam', '20', '30'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Maya', '25', '25'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Megan', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Melody', '28', '22'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Mia', '67', '6'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Michelle', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Mila', '25', '25'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Miranda', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Naomi', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Natalie', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Nicole', '28', '22'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Nina', '21', '29'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Noor', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Olivia', '141', '1'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Penelope', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Phoebe', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Priscilla', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Queena', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Queenie', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Rachel', '24', '26'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Raina', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Rebecca', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Riley', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sabrina', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Safa', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Samantha', '18', '32'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Samara', '13', '37'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sara', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sarah', '35', '15'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Scarlett', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Selena', '14', '36'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Selina', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Serena', '24', '26'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sharon', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Shirley', '17', '33'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Shreya', '10', '40'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sofia', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sophia', '111', '2'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sophie', '29', '21'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Stella', '19', '31'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Stephanie', '21', '29'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Summer', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Syeda', '16', '34'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Sylvia', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Tenzin', '33', '17'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Tiffany', '29', '21'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Vanessa', '12', '38'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Vicky', '15', '35'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Victoria', '25', '25'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Vivian', '31', '19'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Winnie', '11', '39'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Zainab', '24', '26'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Zara', '37', '13'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Zoe', '34', '16'],\n",
    " ['2014', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Zoey', '24', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aaliyah', '67', '4'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Abigail', '41', '12'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Adrianna', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aicha', '23', '28'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aisha', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aissatou', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Alana', '16', '35'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Alexandria', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Alexis', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aliyah', '23', '28'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Alyssa', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amara', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amaya', '33', '18'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amelia', '28', '23'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amia', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amina', '27', '24'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aminata', '29', '22'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amira', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amirah', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amiya', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Amiyah', '18', '33'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Angelina', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aniya', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aniyah', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Annabelle', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aria', '19', '32'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Ariana', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Arianna', '37', '15'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Ariel', '45', '11'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Arielle', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Ashley', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aubree', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Aubrey', '21', '30'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Autumn', '41', '12'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Ava', '78', '2'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Avery', '25', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Bella', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Blessing', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Brianna', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Brielle', '30', '21'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Brooke', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Brooklyn', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Cali', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Chanel', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Charlotte', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Chelsea', '30', '21'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Chloe', '68', '3'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Christina', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Dakota', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Danielle', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Destiny', '28', '23'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Egypt', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Eliana', '23', '28'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Elizabeth', '25', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Ella', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Emily', '37', '15'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Emma', '23', '28'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Eva', '16', '35'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Faith', '40', '13'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Fanta', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Fatima', '32', '19'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Fatou', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Fatoumata', '47', '9'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Gabriella', '28', '23'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Gabrielle', '37', '15'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Genesis', '20', '31'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Gianna', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Grace', '20', '31'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Hailey', '21', '30'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Hannah', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Harmony', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Hawa', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Heaven', '25', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Hope', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Imani', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Isabella', '47', '9'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Isabelle', '20', '31'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jada', '18', '33'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jade', '22', '29'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Janelle', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jasmine', '25', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jayda', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jayla', '25', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jordan', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Jordyn', '21', '30'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Journee', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Journey', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Joy', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kadiatou', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kaitlyn', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kali', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kayla', '36', '16'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kaylee', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kelsey', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kennedy', '25', '26'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Khadijah', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Khloe', '39', '14'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kiara', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kimora', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kristen', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kyla', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kylee', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Kylie', '30', '21'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Laila', '27', '24'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Lailah', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Lauren', '37', '15'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Lauryn', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Layla', '26', '25'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Leah', '36', '16'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Leilani', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Lillian', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Logan', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'London', '61', '6'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Londyn', '26', '25'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Lyric', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mackenzie', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Madison', '130', '1'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Madisyn', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Makayla', '40', '13'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Malaysia', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Malia', '18', '33'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Maliyah', '20', '31'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mariah', '22', '29'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mariam', '47', '9'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mariama', '16', '35'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Maryam', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Maya', '36', '16'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'McKenzie', '30', '21'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Melanie', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mia', '61', '6'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Michelle', '22', '29'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mikayla', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mila', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Milan', '26', '25'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Miracle', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Morgan', '36', '16'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Mya', '26', '25'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Nahla', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Naomi', '32', '19'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Nevaeh', '31', '20'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Nia', '31', '20'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Nova', '11', '40'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Nyla', '36', '16'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Nylah', '34', '17'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Olivia', '52', '8'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Oumou', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Paige', '28', '23'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Paris', '29', '22'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Penelope', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Peyton', '34', '17'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Phoenix', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Riley', '20', '31'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Samantha', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sanaa', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sanai', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Saniyah', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sarah', '22', '29'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sarai', '15', '36'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sariah', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Savannah', '53', '7'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Serenity', '46', '10'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Shiloh', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Skye', '17', '34'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Skyla', '28', '23'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Skylah', '24', '27'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Skylar', '63', '5'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Skyler', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sofia', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sophia', '36', '16'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Summer', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Sydney', '21', '30'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Taylor', '45', '11'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Tiana', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Tianna', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Tori', '18', '33'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Trinity', '19', '32'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Victoria', '30', '21'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Violet', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Winter', '16', '35'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Wynter', '12', '39'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zaniyah', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zara', '14', '37'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zariah', '10', '41'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zendaya', '13', '38'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zoe', '23', '28'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zoey', '32', '19'],\n",
    " ['2014', 'FEMALE', 'BLACK NON HISPANIC', 'Zuri', '21', '30'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aaliyah', '55', '36'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Abigail', '104', '12'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Adelyn', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Adriana', '42', '44'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Adrianna', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aileen', '20', '63'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alana', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alanis', '26', '57'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alejandra', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alessandra', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alexa', '71', '30'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alexandra', '31', '52'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alexia', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alexis', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alice', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alicia', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alina', '30', '53'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alison', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alissa', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aliyah', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Allison', '103', '13'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Allyson', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alma', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alondra', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Alyssa', '31', '52'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Amalia', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Amanda', '31', '52'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Amaya', '27', '56'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Amber', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Amelia', '60', '33'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Amy', '73', '28'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ana', '33', '50'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Anabella', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Anabelle', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Anaya', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Andrea', '46', '40'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Angela', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Angelica', '30', '53'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Angelina', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Angeline', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Angelique', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Angie', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Anna', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Annabella', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Annabelle', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Annalise', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'April', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Arabella', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Arely', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aria', '28', '55'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ariana', '94', '17'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Arianna', '85', '23'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ariel', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ariella', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Arielle', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Arya', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ashley', '139', '9'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Athena', '23', '60'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aubree', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aubrey', '45', '41'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Audrey', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aurora', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Autumn', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ava', '79', '25'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Avery', '28', '55'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ayleen', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Aylin', '45', '41'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Bella', '62', '32'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Bianca', '22', '61'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Brenda', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Briana', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Brianna', '86', '22'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Brielle', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Brigitte', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Brittany', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Brooke', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Bryanna', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Camila', '193', '5'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Carla', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Carmen', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Carolina', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Caroline', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Cataleya', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Catalina', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Catherine', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Cecilia', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Celeste', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Celine', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Charlotte', '31', '52'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Chelsea', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Chloe', '68', '31'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Christina', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Crystal', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Cynthia', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Daisy', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Daleyza', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Daniela', '50', '38'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Daniella', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Danna', '31', '52'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Dayana', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Delilah', '41', '45'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Destiny', '25', '58'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Diana', '33', '50'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Eileen', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Elena', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Eliana', '29', '54'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Elisa', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Eliza', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Elizabeth', '40', '46'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ella', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Emely', '60', '33'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Emilia', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Emily', '170', '6'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Emma', '167', '7'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Erika', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Esmeralda', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Esther', '20', '63'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Eva', '33', '50'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Evangeline', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Evelyn', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Faith', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Francesca', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Gabriela', '45', '41'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Gabriella', '78', '26'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Genesis', '102', '14'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Gia', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Gianna', '43', '43'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Giselle', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Grace', '41', '45'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Guadalupe', '29', '54'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Hailey', '92', '18'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Hailie', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Hannah', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Haylee', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Hazel', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Heaven', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Heidi', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Helen', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Isabel', '42', '44'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Isabella', '331', '1'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Isabelle', '27', '56'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Itzel', '24', '59'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ivanna', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Izabella', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jacqueline', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jade', '41', '45'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jasmine', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jayla', '22', '61'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jaylah', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jayleen', '25', '58'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jaylene', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jaylin', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jazlyn', '22', '61'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jazmin', '33', '50'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jennifer', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jessica', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jessie', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jimena', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Jocelyn', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Joselyn', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Julia', '20', '63'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Juliana', '27', '56'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Julianna', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Juliet', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Juliette', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Julissa', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kailey', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kaitlyn', '25', '58'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kamila', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Karen', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Karla', '23', '60'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kate', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Katelyn', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Katherine', '33', '50'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Katie', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kayla', '44', '42'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kaylee', '76', '27'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kayleen', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kaylie', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kaylin', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Keila', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kelly', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kendra', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Keyla', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Khloe', '68', '31'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kiara', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kimberly', '72', '29'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Kylie', '52', '37'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Laila', '24', '59'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Laura', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lauren', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Layla', '50', '38'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Leah', '100', '15'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Leila', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Leilani', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Leslie', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lesly', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Leyla', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lia', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Liana', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Liliana', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lillian', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lily', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lindsay', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Litzy', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lizbeth', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lola', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'London', '29', '54'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Londyn', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Lucia', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Luciana', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Luna', '42', '44'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mackenzie', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Madeline', '23', '60'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Madelyn', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Madison', '90', '20'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Magaly', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Maia', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Makayla', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Maria', '57', '35'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mariah', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mariana', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Marilyn', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Marjorie', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Maya', '45', '41'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Megan', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Melanie', '106', '11'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Melany', '39', '47'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Melina', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Melissa', '29', '54'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Melody', '22', '61'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mia', '255', '3'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Michelle', '44', '42'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mikaela', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mila', '46', '40'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Miranda', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Monica', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Monserrat', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Montserrat', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Mya', '27', '56'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Naomi', '27', '56'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nashla', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Natalia', '45', '41'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Natalie', '28', '55'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nathalie', '24', '59'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nathaly', '19', '64'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nayeli', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nevaeh', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nicole', '48', '39'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nina', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nyla', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Nylah', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Olivia', '96', '16'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Paige', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Pamela', '13', '70'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Paris', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Penelope', '58', '34'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Peyton', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Rachel', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Rebecca', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Riley', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Rose', '32', '51'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ruby', '22', '61'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ruth', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sabrina', '18', '65'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Salome', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Samantha', '91', '19'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sara', '21', '62'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sarah', '39', '47'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sarai', '17', '66'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Savannah', '43', '43'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Scarlett', '80', '24'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Selena', '14', '69'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Serenity', '35', '48'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sherlyn', '22', '61'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Shirley', '15', '68'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sienna', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Skyla', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Skylar', '25', '58'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Skyler', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sofia', '199', '4'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sophia', '259', '2'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Sophie', '26', '57'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Stella', '16', '67'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Stephanie', '39', '47'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Tatiana', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Taylor', '23', '60'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Tiffany', '20', '63'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Trinity', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Valentina', '135', '10'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Valeria', '34', '49'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Valerie', '57', '35'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Valery', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Vanessa', '29', '54'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Victoria', '148', '8'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Violet', '27', '56'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Violeta', '12', '71'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Wendy', '11', '72'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Ximena', '26', '57'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Yaretzi', '20', '63'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Yasmin', '10', '73'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Zoe', '87', '21'],\n",
    " ['2014', 'FEMALE', 'HISPANIC', 'Zoey', '34', '49'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Abigail', '124', '13'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ada', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Addison', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adelaide', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adele', '24', '72'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adelina', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adeline', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adina', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adriana', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Adrianna', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ahuva', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aisha', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alessandra', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alessia', '20', '76'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alexa', '37', '59'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alexandra', '84', '26'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alexis', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alice', '56', '43'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alina', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alisa', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aliza', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Allison', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alma', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Alyssa', '36', '60'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Amanda', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Amelia', '73', '31'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Amelie', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Amina', '27', '69'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Amira', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ana', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Anastasia', '26', '70'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Angelica', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Angelina', '46', '50'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Anisa', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Anna', '95', '22'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Annabelle', '26', '70'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Anne', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Antonia', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aria', '45', '51'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ariana', '41', '55'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Arianna', '31', '65'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ariel', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ariella', '32', '64'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ashley', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Atara', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Athena', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Audrey', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aurora', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ava', '146', '9'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Avery', '66', '34'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Avigail', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Avital', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aviva', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aya', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ayala', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ayla', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Aziza', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Baila', '39', '57'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Barbara', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Batsheva', '32', '64'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Beatrice', '44', '52'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Bella', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Bianca', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Blake', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Blima', '29', '67'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Blimy', '27', '69'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Bracha', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Brianna', '16', '80'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Bridget', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Brielle', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Brooke', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Brucha', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Bruchy', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Brynn', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Caitlin', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Camila', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Camilla', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Camille', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Carly', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Carolina', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Caroline', '45', '51'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Cassidy', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Catherine', '37', '59'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Cecelia', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Cecilia', '24', '72'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Celia', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Celine', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Chana', '143', '10'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Chany', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Charlie', '20', '76'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Charlotte', '133', '12'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Chava', '39', '57'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Chavy', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Chaya', '194', '6'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Chloe', '81', '28'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Christina', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Claire', '43', '53'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Clara', '36', '60'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Clementine', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Colette', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Cora', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Dahlia', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Daniella', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Danielle', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Daphne', '16', '80'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Delilah', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Devora', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Devorah', '47', '49'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Diana', '25', '71'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Dina', '28', '68'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Dylan', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eden', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eleanor', '65', '35'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Elena', '27', '69'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eleni', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eliana', '49', '47'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Elisheva', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eliza', '21', '75'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Elizabeth', '101', '19'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ella', '112', '16'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Elle', '24', '72'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ellie', '31', '65'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eloise', '42', '54'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Elsa', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Emerson', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Emilia', '54', '45'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Emily', '143', '10'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Emma', '205', '5'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Emmeline', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Erin', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Esme', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ester', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Esther', '233', '2'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Esty', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Etty', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eva', '74', '30'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Evangeline', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Eve', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Evelyn', '64', '36'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Everly', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Evie', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Faiga', '24', '72'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Faigy', '74', '30'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Fatima', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Fiona', '36', '60'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Fradel', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Frady', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Fraidy', '29', '67'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Frances', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Francesca', '39', '57'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Frimet', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gabriela', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gabriella', '93', '23'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gabrielle', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gemma', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Genevieve', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Georgia', '26', '70'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gia', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gianna', '38', '58'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Giovanna', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Giselle', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Gitty', '68', '33'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Giuliana', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Golda', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Goldie', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Goldy', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Grace', '86', '25'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hadassah', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hadley', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hailey', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hana', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hanna', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hannah', '64', '36'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Harlow', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Harper', '41', '55'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hazel', '24', '72'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Helen', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Helena', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Henchy', '20', '76'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Henny', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hinda', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Hindy', '34', '62'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Iris', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Isabel', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Isabella', '117', '15'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Isabelle', '26', '70'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Isla', '31', '65'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Ivy', '20', '76'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jacqueline', '16', '80'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jana', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jane', '25', '71'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jasmina', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jasmine', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jenna', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jessica', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Jordyn', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Josephine', '39', '57'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Julia', '84', '26'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Juliana', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Julianna', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Julie', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Juliet', '31', '65'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Juliette', '32', '64'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'June', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Juniper', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kaitlyn', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kate', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Katherine', '42', '54'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kayla', '42', '54'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kaylee', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Keira', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kennedy', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Khloe', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kira', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Kylie', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Laila', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lana', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lara', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Laura', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lauren', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Layan', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Layla', '55', '44'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lea', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Leah', '206', '4'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Leila', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lena', '28', '68'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Leora', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lia', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Liana', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Liba', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Libby', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lila', '42', '54'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lilah', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lilian', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Liliana', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lillian', '40', '56'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lilly', '16', '80'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lily', '88', '24'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lina', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Linda', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Liv', '16', '80'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Livia', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lola', '20', '76'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Louisa', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lucia', '29', '67'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lucy', '58', '41'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Luna', '16', '80'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lydia', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Lyla', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mackenzie', '26', '70'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Madeleine', '35', '61'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Madeline', '59', '40'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Madelyn', '24', '72'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Madison', '68', '33'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mae', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Maeve', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Maisie', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Malak', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Malka', '84', '26'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Malky', '70', '32'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Margaret', '33', '63'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Margot', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Maria', '45', '51'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mariam', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Marielle', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mary', '28', '68'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Maryam', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Matilda', '18', '78'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Maya', '106', '18'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Melanie', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Menucha', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mia', '139', '11'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Michaela', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Michelle', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mikayla', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mila', '62', '38'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Milana', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mina', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mindy', '29', '67'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mira', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mirel', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Miriam', '143', '10'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Molly', '27', '69'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Morgan', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Mushka', '12', '84'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Nadia', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Naomi', '49', '47'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Natalia', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Natalie', '44', '52'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Nechama', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Nicole', '41', '55'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Nina', '41', '55'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Noa', '28', '68'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Noelle', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Noor', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Nora', '70', '32'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Olive', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Olivia', '248', '1'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Paige', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Parker', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Pearl', '23', '73'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Penelope', '63', '37'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Perel', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Perl', '17', '79'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Perry', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Pessy', '30', '66'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Peyton', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Phoebe', '27', '69'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Piper', '19', '77'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Poppy', '14', '82'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Quinn', '26', '70'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rachel', '228', '3'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Raizel', '15', '81'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Raizy', '84', '26'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rebecca', '43', '53'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Reese', '22', '74'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Reizy', '10', '86'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rena', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rifky', '11', '85'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Riley', '31', '65'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rivka', '100', '20'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rivky', '52', '46'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rochel', '31', '65'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Roizy', '13', '83'],\n",
    " ['2014', 'FEMALE', 'WHITE NON HISPANIC', 'Rosa', '10', '86']]\n",
    "\n",
    "records[:2]\n",
    "#records[:2,2]   # TypeError: list indices must be integers or slices, not tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7668d2-f94b-4479-8c95-dc22a8347abf",
   "metadata": {},
   "source": [
    "# TypeError: list indices must be integers or slices, not tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3cff9435-f22d-487c-b76f-0387cdf78d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aarya\n",
      "Abby\n",
      "Abigail\n",
      "Aisha\n",
      "Aiza\n",
      "Aleena\n",
      "Alexandra\n",
      "Alice\n",
      "Alicia\n",
      "Alina\n"
     ]
    }
   ],
   "source": [
    "# Create the empty list: baby_names\n",
    "baby_names = []\n",
    "\n",
    "# Loop over records \n",
    "for i in records:\n",
    "    # Add the name to the list\n",
    "    baby_names.append(i[3])\n",
    "    \n",
    "# Sort the names in alphabetical order\n",
    "for name in sorted(baby_names[:10]):\n",
    "    # Print each name\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952ec2b-22c4-418c-8f49-13e33bbabb76",
   "metadata": {},
   "source": [
    "## Meet the Tuples\n",
    "\n",
    "\n",
    "\n",
    "  __Hold data in order__\n",
    "  __Index__\n",
    "  __Immutable__\n",
    "  __Pairing__\n",
    "  __Unpackable__\n",
    "\n",
    "\n",
    "**Tuples are widely used internally many of the systems we depend on like databases.  \n",
    "\n",
    "**Tuples are very much like lists they hold data in the order, and we can access elements inside a tuple with an index\n",
    "\n",
    "**Tuples are easier to process and more memory efficient than list\n",
    "\n",
    "**Tuples are immutable, which means we cant add or remove elements from them.  \n",
    "    __This is powerful because we can use them to ensure that our data in not altered \n",
    "\n",
    "   ## We can create tuples by pairing up elements.  \n",
    "\n",
    "**Finally we can use something called unpacking to expand a tuple into name variable that represent each element in the tuple.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4186f2-dccf-4689-a2e8-fbd7699bdc5f",
   "metadata": {},
   "source": [
    "**Often, we have lists where we want to matchup elements into pairs, and the zip function enables us to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "be8b3a1b-59d1-4001-a2bb-e3f8b3bc820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chocolate Chip', 'Punjabi'), ('Brownies', 'Fruit Cake Rusk'), ('Peanut Butter', 'Marble Cookies'), ('Oreos', 'Kaju Pista Cookies'), ('Oatmeal Raisin', 'Almond Cookies')]\n",
      "\n",
      "Notice that tuples use parenthesis as they object representation\n"
     ]
    }
   ],
   "source": [
    "# Zippling tuples\n",
    "\n",
    "    # Here we got a list for the most popular cookies in the US and India, and I want to\n",
    "    # build a list of pairs by the popularity rank of the cookies in each country. \n",
    "\n",
    "\n",
    "us_cookies = [\"Chocolate Chip\", \"Brownies\", \"Peanut Butter\", \"Oreos\", \"Oatmeal Raisin\"]\n",
    "in_cookies = [\"Punjabi\", \"Fruit Cake Rusk\", \"Marble Cookies\", \"Kaju Pista Cookies\", \"Almond Cookies\"]\n",
    "\n",
    "top_pairs = list(zip(us_cookies, in_cookies))\n",
    "# .zip() method will zip list elements into tuple pairs\n",
    "# ************************************************************************************************ #\n",
    "\n",
    "print(top_pairs)\n",
    "print(\"\\nNotice that tuples use parenthesis as they object representation\")\n",
    "\n",
    "del top_pairs, us_cookies, in_cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba90bd-3aac-4a10-bc67-8b18ea04df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking tuples\n",
    "\n",
    "    # Tuple unpacking, also sometimes called tuple expansion, allow us to assign the elements\n",
    "    # of a tuple to named variables for later useThis syntax allows us to create more readable\n",
    "    # and less error prone code\n",
    "    \n",
    "# Unpacking tuples is a very expansive way for working with data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "412d828b-f968-49ee-82fe-61681d83b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate Chip\n",
      "Punjabi\n"
     ]
    }
   ],
   "source": [
    "# Unpacking tuples\n",
    "\n",
    "us_cookies = [\"Chocolate Chip\", \"Brownies\", \"Peanut Butter\", \"Oreos\", \"Oatmeal Raisin\"]\n",
    "in_cookies = [\"Punjabi\", \"Fruit Cake Rusk\", \"Marble Cookies\", \"Kaju Pista Cookies\", \"Almond Cookies\"]\n",
    "\n",
    "top_pairs = list(zip(us_cookies, in_cookies))\n",
    "# I start by putting both variables as the target of the assignment statement separated by a comma\n",
    "# Then assign the first tuple in our top pairs list to them\n",
    "\n",
    "us_num_1, in_num_1 = top_pairs[0]\n",
    "\n",
    "print(us_num_1)\n",
    "print(in_num_1)\n",
    "\n",
    "del us_num_1, in_num_1, top_pairs, us_cookies, in_cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d42e4c4-e440-4e85-93c2-33731bc8f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate Chip\n",
      "Punjabi\n",
      "\n",
      "Brownies\n",
      "Fruit Cake Rusk\n",
      "\n",
      "Peanut Butter\n",
      "Marble Cookies\n",
      "\n",
      "Oreos\n",
      "Kaju Pista Cookies\n",
      "\n",
      "Oatmeal Raisin\n",
      "Almond Cookies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unpacking tuples\n",
    "\n",
    "us_cookies = [\"Chocolate Chip\", \"Brownies\", \"Peanut Butter\", \"Oreos\", \"Oatmeal Raisin\"]\n",
    "in_cookies = [\"Punjabi\", \"Fruit Cake Rusk\", \"Marble Cookies\", \"Kaju Pista Cookies\", \"Almond Cookies\"]\n",
    "\n",
    "top_pairs = list(zip(us_cookies, in_cookies))\n",
    "\n",
    "for us_cookies, in_cookies in top_pairs:\n",
    "    print(us_cookies)\n",
    "    print(in_cookies)\n",
    "    print(\"\")\n",
    "    \n",
    "del us_cookies, in_cookies, top_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567c47d-cf41-44f8-ade4-7bc04c3e0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerating positions\n",
    "\n",
    "    # Often we want to know what the index is of an element in the iterable is. \n",
    "    # The enumerate function allows us to do that by creating tuples were the first element \n",
    "    # of the tuple is index of the element in the original list, then the element itself\n",
    "    \n",
    "      # Enumeration is used in loops to return the position and the data in that position while looping\n",
    "      # We can use this to track ranking in our data or skip elements weare not interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ed791e-ba71-4ac9-b40f-3ad2282ae3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chocolate Chip', 'Punjabi'), ('Brownies', 'Fruit Cake Rusk'), ('Peanut Butter', 'Marble Cookies'), ('Oreos', 'Kaju Pista Cookies'), ('Oatmeal Raisin', 'Almond Cookies')]\n",
      "0 Chocolate Chip Punjabi\n",
      "1 Brownies Fruit Cake Rusk\n",
      "2 Peanut Butter Marble Cookies\n",
      "3 Oreos Kaju Pista Cookies\n",
      "4 Oatmeal Raisin Almond Cookies\n"
     ]
    }
   ],
   "source": [
    "# Enumerating positions\n",
    "\n",
    "us_cookies = [\"Chocolate Chip\", \"Brownies\", \"Peanut Butter\", \"Oreos\", \"Oatmeal Raisin\"]\n",
    "in_cookies = [\"Punjabi\", \"Fruit Cake Rusk\", \"Marble Cookies\", \"Kaju Pista Cookies\", \"Almond Cookies\"]\n",
    "\n",
    "top_pairs = list(zip(us_cookies, in_cookies))\n",
    "print(top_pairs)\n",
    "\n",
    "# When we are unpacking values into variables using tuple unpacking, the number of variables \n",
    "# on the left side tuple must exactly match the number of values on the right side tuple. \n",
    "# Otherwise, we'll get a ValueError.\n",
    "\n",
    "\n",
    "for idx, items in enumerate(top_pairs):\n",
    "    us_cookies, in_cookies = items\n",
    "    print(idx, us_cookies, in_cookies)\n",
    "    \n",
    "del us_cookies, in_cookies, top_pairs, idx, items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d9d28-2cf9-42d5-81f9-dd24b0388b68",
   "metadata": {},
   "source": [
    "## Using and unpacking tuples\n",
    "\n",
    "Tuples are made of several items just like a list, but they cannot be modified in any way. It is very common for tuples to be used to represent data from a database. If you have a tuple like ('chocolate chip cookies', 15) and you want to access each part of the data, you can use an index just like a list. However, you can also \"unpack\" the tuple into multiple variables such as type, count = ('chocolate chip cookies', 15) that will set type to 'chocolate chip cookies' and count to 15.\n",
    "\n",
    "Often you'll want to pair up multiple array data types. The zip() function does just that. It will return a list of tuples containing one element from each list passed into zip().\n",
    "\n",
    "When looping over a list, you can also track your position in the list by using the enumerate() function. The function returns the index of the list item you are currently on in the list and the list item itself.\n",
    "\n",
    "You'll practice using the enumerate() and zip() functions in this exercise, in which your job is to pair up the most common boy and girl names. Two lists - girl_names and boy_names - have been pre-loaded into your workspace.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Use the zip() function to pair up girl_names and boy_names into a variable called pairs.\n",
    "    Use a for loop to loop through pairs, using enumerate() to keep track of your position. Unpack pairs into the variables idx and pair.\n",
    "    Inside the for loop:\n",
    "        Unpack pair into the variables girl_name and boy_name.\n",
    "        Print the rank, girl name, and boy name, in that order. The rank is contained in idx.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1a41ab31-2325-4b86-8973-bea41ba82c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('JADA', 'JOSIAH'), ('Emily', 'ETHAN'), ('Ava', 'David'), ('SERENITY', 'Jayden'), ('Claire', 'MASON'), ('SOPHIA', 'RYAN'), ('Sarah', 'CHRISTIAN'), ('ASHLEY', 'ISAIAH'), ('CHAYA', 'JAYDEN'), ('ABIGAIL', 'Michael'), ('Zoe', 'NOAH'), ('LEAH', 'SAMUEL'), ('HAILEY', 'SEBASTIAN'), ('AVA', 'Noah'), ('Olivia', 'Dylan'), ('EMMA', 'LUCAS'), ('CHLOE', 'JOSHUA'), ('Sophia', 'ANGEL'), ('AALIYAH', 'Jacob'), ('Angela', 'Matthew'), ('Camila', 'Josiah'), ('Savannah', 'JACOB'), ('Serenity', 'Muhammad'), ('Chloe', 'ALEXANDER'), ('Fatoumata', 'Jason'), ('ISABELLA', 'Ethan'), ('MIA', 'DANIEL'), ('FIONA', 'Joseph'), ('Skylar', 'AIDEN'), ('Ashley', 'Moshe'), ('Rachel', 'Jeremiah'), ('Sofia', 'William'), ('Alina', 'Alexander'), ('MADISON', 'Sebastian'), ('RACHEL', 'ERIC'), ('CAMILA', 'MOSHE'), ('CHANA', 'Jack'), ('TAYLOR', 'Eric'), ('Kayla', 'MUHAMMAD'), ('Miriam', 'Lucas'), ('Leah', 'BENJAMIN'), ('Grace', 'Aiden'), ('ANGELA', 'Ryan'), ('Isabella', 'Liam'), ('Emma', 'JASON'), ('KAYLA', 'KEVIN'), ('SOFIA', 'Elijah'), ('Madison', 'Angel'), ('Aaliyah', 'JAMES'), ('Taylor', 'Daniel'), ('GENESIS', 'Samuel'), ('Esther', 'Amir'), ('MAKAYLA', 'Mason'), ('Victoria', 'Joshua'), ('Chaya', 'ANTHONY'), ('Brielle', 'JOSEPH'), ('Anna', 'Benjamin'), ('Samantha', 'JUSTIN'), ('ESTHER', 'JEREMIAH'), ('GRACE', 'MATTHEW'), ('Mariam', 'Carter'), ('Mia', 'James'), ('NEVAEH', 'TYLER'), ('GABRIELLE', 'DAVID'), ('EMILY', 'JACK'), ('London', 'ELIJAH'), ('TIFFANY', 'MICHAEL'), ('Chana', 'CHRISTOPHER')]\n",
      "Rank 0: JADA and JOSIAH\n",
      "Rank 1: Emily and ETHAN\n",
      "Rank 2: Ava and David\n",
      "Rank 3: SERENITY and Jayden\n",
      "Rank 4: Claire and MASON\n",
      "Rank 5: SOPHIA and RYAN\n",
      "Rank 6: Sarah and CHRISTIAN\n",
      "Rank 7: ASHLEY and ISAIAH\n",
      "Rank 8: CHAYA and JAYDEN\n",
      "Rank 9: ABIGAIL and Michael\n"
     ]
    }
   ],
   "source": [
    "girl_names = ['JADA', 'Emily', 'Ava', 'SERENITY', 'Claire', 'SOPHIA', 'Sarah', 'ASHLEY', \n",
    " 'CHAYA', 'ABIGAIL', 'Zoe', 'LEAH', 'HAILEY', 'AVA', 'Olivia', 'EMMA', 'CHLOE', 'Sophia',\n",
    " 'AALIYAH', 'Angela', 'Camila', 'Savannah', 'Serenity', 'Chloe', 'Fatoumata', 'ISABELLA',\n",
    " 'MIA', 'FIONA', 'Skylar', 'Ashley', 'Rachel', 'Sofia', 'Alina', 'MADISON', 'RACHEL',\n",
    " 'CAMILA', 'CHANA', 'TAYLOR', 'Kayla', 'Miriam', 'Leah', 'Grace', 'ANGELA', 'Isabella',\n",
    " 'Emma', 'KAYLA', 'SOFIA', 'Madison', 'Aaliyah', 'Taylor', 'GENESIS', 'Esther', 'MAKAYLA',\n",
    " 'Victoria', 'Chaya', 'Brielle', 'Anna', 'Samantha', 'ESTHER', 'GRACE', 'Mariam', 'Mia',\n",
    " 'NEVAEH', 'GABRIELLE', 'EMILY', 'London', 'TIFFANY', 'Chana', 'Valentina', 'OLIVIA',\n",
    " 'LONDON', 'MIRIAM', 'SARAH', 'ELLA']\n",
    "\n",
    "boy_names = ['JOSIAH', 'ETHAN', 'David', 'Jayden', 'MASON', 'RYAN', 'CHRISTIAN', 'ISAIAH',\n",
    " 'JAYDEN', 'Michael', 'NOAH', 'SAMUEL', 'SEBASTIAN', 'Noah', 'Dylan', 'LUCAS', 'JOSHUA',\n",
    " 'ANGEL', 'Jacob', 'Matthew', 'Josiah', 'JACOB', 'Muhammad', 'ALEXANDER', 'Jason', 'Ethan',\n",
    " 'DANIEL', 'Joseph', 'AIDEN', 'Moshe', 'Jeremiah', 'William', 'Alexander', 'Sebastian',\n",
    " 'ERIC', 'MOSHE', 'Jack', 'Eric', 'MUHAMMAD', 'Lucas', 'BENJAMIN', 'Aiden', 'Ryan', 'Liam',\n",
    " 'JASON', 'KEVIN', 'Elijah', 'Angel', 'JAMES', 'Daniel', 'Samuel', 'Amir', 'Mason', 'Joshua',\n",
    " 'ANTHONY', 'JOSEPH', 'Benjamin', 'JUSTIN', 'JEREMIAH', 'MATTHEW', 'Carter', 'James', 'TYLER',\n",
    " 'DAVID', 'JACK', 'ELIJAH', 'MICHAEL', 'CHRISTOPHER']\n",
    "\n",
    "# Pair up the girl and boy names: pairs\n",
    "pairs = [i for i in zip(girl_names, boy_names)]\n",
    "                    # because zip() gives you the tuple pairs of origianl list element\n",
    "\n",
    "print(pairs)\n",
    "\n",
    "# Iterate over pairs\n",
    "for idx, pair in enumerate(pairs[:10]):\n",
    "    # Unpack pair: girl_name, boy_name\n",
    "    girl_name, boy_name = pair\n",
    "    # Print the rank and names associated with each rank\n",
    "    print('Rank {}: {} and {}'.format(idx, girl_name, boy_name))\n",
    "    \n",
    "\n",
    "del girl_names, boy_names, pairs, idx, boy_name, girl_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b1266-3dfa-470f-8f1d-83e48378350c",
   "metadata": {},
   "source": [
    "## Making tuples by accident\n",
    "\n",
    "Tuples are very powerful and useful, and it's super easy to make one by accident. All you have to do is create a variable and follow the assignment with a comma. This becomes an error when you try to use the variable later expecting it to be a string or a number.\n",
    "\n",
    "You can verify the data type of a variable with the type() function. In this exercise, you'll see for yourself how easy it is to make a tuple by accident.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a variable named normal and set it equal to 'simple'.\n",
    "    Create a variable named error and set it equal 'trailing comma',.\n",
    "    Print the type of the normal and error variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce11bdb6-02f2-457c-8b57-4b7cebd5d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# Create the normal variable: normal\n",
    "normal = \"simple\"\n",
    "\n",
    "# Create the mistaken variable: error\n",
    "error = \"trailing comma\",\n",
    "\n",
    "# Print the types of the variables\n",
    "print(type(normal))\n",
    "print(type(error))\n",
    "\n",
    "del normal, error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9378cab-8554-4027-b699-7358f813a6f9",
   "metadata": {},
   "source": [
    "## Sets for unordered and unique data\n",
    "\n",
    "  __Unique__\n",
    "  __Unorded__\n",
    "  __Mutable__\n",
    "\n",
    "\n",
    "**Now that you've learned about lists and tuples lets look at our last build-in array data type - set\n",
    "   \n",
    "   **Set are execellent for finding all the unique values in a column of your data, a list of elements, or even rows from a file. \n",
    "   **We use sets when we want to store unique data elements in an unorded fashion, for example, we might want to store a list of each type of cookie we had without any duplicates. \n",
    "   **Sets are also mutable, so we can add or remove elements from them.  \n",
    "   \n",
    "   \n",
    "#   **A set is slmost always created from a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb7ae917-1387-4633-9834-66b5df45b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oatmeal cream', 'peanut butter', 'chocolate chip'}\n"
     ]
    }
   ],
   "source": [
    "# Creating sets\n",
    "\n",
    "cookies_eaten_today = [\"chocolate chip\", \"peanut butter\", \"chocolate chip\", \"oatmeal cream\",\n",
    "                       \"chocolate chip\"]\n",
    "\n",
    "# We can make a set of them by passing them into the set constructor\n",
    "\n",
    "types_of_cookies_eaten_today = set(cookies_eaten_today)\n",
    "print(types_of_cookies_eaten_today)\n",
    "\n",
    "del cookies_eaten_today, types_of_cookies_eaten_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e3ca9d2b-76d5-4771-99e0-96c7360ecf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oatmeal cream', 'peanut butter', 'chocolate chip'}\n",
      "{'oreos', 'chocolate chip', 'chocolate chips', 'oatmeal cream', 'biscotti', 'peanut butter'}\n"
     ]
    }
   ],
   "source": [
    "# Modifying sets & Update sets\n",
    "\n",
    "   # When working witha set we will use .add() method to add new element to the set\n",
    "   # Also we can add multiple items using the .update() method, mergers in another set or list\n",
    "    \n",
    "    \n",
    "cookies_eaten_today = [\"chocolate chip\", \"peanut butter\", \"chocolate chip\", \"oatmeal cream\",\n",
    "                       \"chocolate chip\"]\n",
    "\n",
    "# We can make a set of them by passing them into the set constructor\n",
    "\n",
    "types_of_cookies_eaten_today = set(cookies_eaten_today)\n",
    "print(types_of_cookies_eaten_today)\n",
    "\n",
    "types_of_cookies_eaten_today.add(\"biscotti\")\n",
    "types_of_cookies_eaten_today.update([\"biscotti\", \"chocolate chips\", \"oreos\"])\n",
    "\n",
    "print(types_of_cookies_eaten_today)\n",
    "\n",
    "del cookies_eaten_today, types_of_cookies_eaten_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "699777dd-5a8f-4195-b706-4c20eec18966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oatmeal cream', 'peanut butter', 'chocolate chip'}\n",
      "{'oreos', 'chocolate chip', 'chocolate chips', 'oatmeal cream', 'biscotti', 'peanut butter'}\n",
      "{'oreos', 'chocolate chip', 'chocolate chips', 'oatmeal cream', 'peanut butter'}\n",
      "{'chocolate chip', 'chocolate chips', 'oatmeal cream', 'peanut butter'}\n"
     ]
    }
   ],
   "source": [
    "# Removing data from sets\n",
    "\n",
    "   # When removing data from a set, we cna use the .discard() method to safely remove an element\n",
    "   # from the set by its value, no error will be thrown if the value is not found\n",
    "   # We can also use the .pop() method to remove and return an arbitrary element from the set\n",
    "    \n",
    "    \n",
    "cookies_eaten_today = [\"chocolate chip\", \"peanut butter\", \"chocolate chip\", \"oatmeal cream\",\n",
    "                       \"chocolate chip\"]\n",
    "\n",
    "# We can make a set of them by passing them into the set constructor\n",
    "\n",
    "types_of_cookies_eaten_today = set(cookies_eaten_today)\n",
    "print(types_of_cookies_eaten_today)\n",
    "\n",
    "types_of_cookies_eaten_today.add(\"biscotti\")\n",
    "types_of_cookies_eaten_today.update([\"biscotti\", \"chocolate chips\", \"oreos\"])\n",
    "print(types_of_cookies_eaten_today)\n",
    "\n",
    "types_of_cookies_eaten_today.discard(\"biscotti\")\n",
    "print(types_of_cookies_eaten_today)\n",
    "                                    # ************************************************************\n",
    "types_of_cookies_eaten_today.pop()  # set.pop() takes no argument, it removes the first element\n",
    "print(types_of_cookies_eaten_today)\n",
    "\n",
    "del cookies_eaten_today, types_of_cookies_eaten_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc6a4db9-b7d6-4616-b338-4678f9e00a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anzac', 'peanut butter', 'oatmeal cream', 'chocolate chip'}\n",
      "{'chocolate chip'}\n"
     ]
    }
   ],
   "source": [
    "# Set operations - Similarities\n",
    "\n",
    "   # .union() method on a set accepts a set as argument and returns all the unique elements\n",
    "   # from both sets as a new one\n",
    "   # .intersection() set method also accepts a set and returns the overlapping elements\n",
    "   # founded in both sets\n",
    "\n",
    "\n",
    "                    # TypeError: set expected at most 1 argument, got 3\n",
    "cookies_jason_ate = set([\"chocolate chip\", \"oatmeal cream\", \"peanut butter\"])\n",
    "cookies_hugo_ate= set([\"chocolate chip\", \"anzac\"])\n",
    "\n",
    "all_ate_cookies = cookies_jason_ate.union(cookies_hugo_ate)\n",
    "print(all_ate_cookies)\n",
    "\n",
    "both_ate_cookies = cookies_jason_ate.intersection(cookies_hugo_ate)\n",
    "print(both_ate_cookies)\n",
    "\n",
    "del cookies_jason_ate, cookies_hugo_ate, all_ate_cookies, both_ate_cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192f34b-ee9f-448b-82bd-373f07bef598",
   "metadata": {},
   "source": [
    "# Set operations - Differences\n",
    "\n",
    "   # .difference() method identifies data present in the set on which the method was used\n",
    "   # that is not in the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea29cd5f-1e5e-498f-a263-3d01f20e8415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oatmeal cream', 'peanut butter'}\n",
      "{'anzac'}\n"
     ]
    }
   ],
   "source": [
    "# Set operations - Differences\n",
    "\n",
    "   # .difference() method identifies data present in the set on which the method was used\n",
    "   # that is not in the arguments\n",
    "\n",
    "\n",
    "                    # TypeError: set expected at most 1 argument, got 3\n",
    "cookies_jason_ate = set([\"chocolate chip\", \"oatmeal cream\", \"peanut butter\"])\n",
    "cookies_hugo_ate= set([\"chocolate chip\", \"anzac\"])\n",
    "\n",
    "cookies_only_jason = cookies_jason_ate.difference(cookies_hugo_ate)\n",
    "print(cookies_only_jason)\n",
    "cookies_only_hugo = cookies_hugo_ate.difference(cookies_jason_ate)\n",
    "print(cookies_only_hugo)\n",
    "\n",
    "del cookies_jason_ate, cookies_hugo_ate, cookies_only_jason, cookies_only_hugo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd195e-d0a5-4490-bd1b-bfb7b608d415",
   "metadata": {},
   "source": [
    "## Finding all the data and the overlapping data between sets\n",
    "\n",
    "Sets have several methods to combine, compare, and study them all based on mathematical set theory. The .union() method returns a set of all the names found in the set you used the method on plus any sets passed as arguments to the method. You can also look for overlapping data in sets by using the .intersection() method on a set and passing another set as an argument. It will return an empty set if nothing matches.\n",
    "\n",
    "Your job in this exercise is to find the union and intersection in the names from 2011 and 2014. For this purpose, two sets have been pre-loaded into your workspace: baby_names_2011 and baby_names_2014.\n",
    "\n",
    "One quirk in the baby names dataset is that names in 2011 and 2012 are all in upper case, while names in 2013 and 2014 are in title case (where the first letter of each name is capitalized). Consequently, if you were to compare the 2011 and 2014 data in this form, you would find no overlapping names between the two years! To remedy this, we converted the names in 2011 to title case using Python's .title() method.\n",
    "\n",
    "Real-world data can often come with quirks like this - it's important to catch them to ensure your results are meaningful.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Combine all the names in baby_names_2011 and baby_names_2014 by computing their union. Store the result as all_names.\n",
    "    Print the number of names that occur in all_names. You can use the len() function to compute the number of names in all_names.\n",
    "    Find all the names that occur in both baby_names_2011 and baby_names_2014 by computing their intersection. Store the result as overlapping_names.\n",
    "    Print the number of names that occur in overlapping_names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4518a1ef-f409-4380-be73-71cc0f756124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n",
      "792\n"
     ]
    }
   ],
   "source": [
    "baby_names_2011 = {'Khloe', 'Ahmed', 'Byron', 'Kira', 'Tzvi', 'Alex', 'Isabella', 'Jamir', 'Mathew',\n",
    " 'Sophia', 'Adrianna', 'Mauricio', 'Miri', 'Kayden', 'Chloe', 'Tziporah', 'Travis', 'Brianny', \n",
    " 'Amare', 'Julissa', 'Josue', 'Malak', 'Saniyah', 'Melanie', 'Abdullah', 'Mohamed', 'Yakov',\n",
    " 'Brandon', 'Augustus', 'Ezekiel', 'Joy', 'Tamar', 'Yandel', 'Joanna', 'Amir', 'Jonathan', 'James',\n",
    " 'Malcolm', 'Anna', 'Karen', 'Jayla', 'Emanuel', 'Binyomin', 'Farhan', 'Winnie', 'Carlos', 'Egypt',\n",
    " 'Jacqueline', 'Faith', 'Alexander', 'Ryder', 'Viviana', 'Nathaniel', 'Amira', 'Cindy', 'Victoria',\n",
    " 'Mamadou', 'Atara', 'Melissa', 'Chase', 'Alisson', 'Matteo', 'Miles', 'Alisa', 'Josiah', 'Richard',\n",
    " 'Erik', 'Jose', 'Bianca', 'Avraham', 'William', 'Gemma', 'Miah', 'Abraham', 'Sanai', 'Adriana',\n",
    " 'Kaitlyn', 'Tianna', 'Syeda', 'Alejandra', 'Amelia', 'Jasmin', 'Kieran', 'Nathalia', 'Jessica', \n",
    " 'Abigail', 'Erica', 'Tabitha', 'Rebecca', 'Rohan', 'Yadiel', 'Daisy', 'Nathaly', 'Siena', 'Damian', \n",
    " 'Zara', 'Antonio', 'Dante', 'Januel', 'Solomon', 'Lukas', 'Fanta', 'Thomas', 'Terrell', 'Brynn', \n",
    " 'Carmelo', 'Shloimy', 'Ousmane', 'Kenneth', 'Leonel', 'Sean', 'Isabelle', 'Ricardo', 'Jeancarlos',\n",
    " 'Malik', 'Elina', 'Kelsey', 'Armando', 'Jane', 'Jocelyn', 'Bryson', 'Cayden', 'Nahla', 'Bryce',\n",
    " 'Aniya', 'Johanna', 'Kiyan', 'Logan', 'Zoey', 'Alison', 'Erin', 'Ashley', 'Brendan', 'Heaven',\n",
    " 'Johnathan', 'Dwayne', 'Aliza', 'Abel', 'Shea', 'Frank', 'Youssef', 'Marcos', 'Selina', 'Yahir',\n",
    " 'Austin', 'Dahlia', 'Ruth', 'Zaria', 'Sincere', 'Troy', 'Ali', 'David', 'August', 'Nora', 'Lesly',\n",
    " 'Esther', 'Mia', 'Katelynn', 'Janiya', 'Mohammed', 'Mckenzie', 'Lola', 'Ayesha', 'Janice', 'Freddy',\n",
    " 'Tristan', 'Chana', 'Emilio', 'Pinchas', 'Yechezkel', 'Alec', 'Cynthia', 'Queenie', 'Juliana', \n",
    " 'Elianna', 'Kaelyn', 'Phoebe', 'Emely', 'Abby', 'Hugo', 'Hadassa', 'Aurora', 'Eva', 'Makayla',\n",
    " 'Giovanny', 'Juan', 'Zain', 'Nadia', 'Jeffrey', 'Luz', 'Avrohom', 'Morris', 'Colette', 'Barbara',\n",
    " 'Esteban', 'Chany', 'Victor', 'Alina', 'Reed', 'Liam','Menachem', 'Cody', 'Haley', 'Zissy', 'Melody',\n",
    " 'Jared', 'Declan', 'Kaylin', 'Pinchus', 'Maliyah', 'Edward', 'Jack', 'Imani', 'Giovanna', 'Marilyn',\n",
    " 'Ian', 'Stanley', 'Shneur', 'Shawn', 'Abdiel', 'Yitzchak', 'Noel', 'Diana', 'Shaniya', 'Finley',\n",
    " 'Jesse', 'Cesar', 'Rifky', 'Renee', 'Miguel', 'Christine', 'Ayden', 'Wesley', 'Rafael', 'Yaniel',\n",
    " 'Yisroel', 'Alicia', 'Mordechai', 'Brayan', 'Sofia', 'Sylvia', 'Athena', 'Irene', 'Darren', 'Bryan',\n",
    " 'Luna', 'Roberto', 'Yitty', 'Gianna', 'Alessandra', 'Filip', 'Shaindy', 'Eloise', 'Jayson', \n",
    " 'Fernanda', 'Chad', 'Briana', 'Juliet', 'Jenna', 'Jessie', 'Sam', 'Jefferson', 'Israel', 'Tenzin', \n",
    " 'Aicha', 'Moussa', 'Shulem', 'Ada', 'Francesca', 'Nana', 'Shmiel', 'Henry', 'Johnny', 'Joselyn', \n",
    " 'Luke', 'Joaquin', 'Nataly', 'Max', 'Salma', 'Camila', 'Martin', 'Jason', 'Gia', 'Brianna', 'Jade', \n",
    " 'Annabel', 'Hunter', 'Junior', 'Leon', 'Shania', 'Fatima', 'Dov', 'Stacy', 'Sarai', 'Jenny', \n",
    " 'Brittany', 'Faigy', 'Ibrahim', 'Kaden', 'Gavriel', 'Danna', 'Armaan', 'Alyssa', 'Jacob', 'Jazmin', \n",
    " 'Katherine', 'Yisrael', 'Mohammad', 'Devora', 'Jeremiah', 'Amirah', 'Diego', 'Paige', 'Jelani', \n",
    " 'Chaya', 'Felix', 'Avigail', 'Carolina', 'Vera', 'Alana', 'Batya', 'Myles', 'Isabela', 'Gregory', \n",
    " 'Jaylyn', 'Bridget', 'Xin', 'Kayleen', 'Leela', 'Silas', 'Chavy', 'Shmuel', 'Bennett', 'Zev','Peter', \n",
    " 'Heidy', 'Paola', 'Quinn', 'Cheyenne', 'Zuri', 'Adele', 'Julia', 'Yaseen', 'Kamari', 'Saniya', \n",
    " 'Amelie', 'Dennis', 'Mark', 'Elianny', 'Keily', 'Rose', 'Mya', 'Savannah', 'Angely', 'London', \n",
    " 'Nicole', 'Isis', 'Ilana', 'Molly', 'Kamiyah', 'Angie', 'Alijah', 'Gabrielle', 'Julio', 'Kayla', \n",
    " 'Alyson', 'Lipa', 'Marco', 'Arjun', 'Brady', 'Allyson', 'Britney', 'Fradel', 'Genesis', 'Shaya', \n",
    " 'Giselle', 'Jaiden', 'Akiva', 'Dina', 'Aisha', 'Beatrice', 'Pedro', 'Ariel', 'Daniella', 'Lindsay', \n",
    " 'Jaelynn', 'Calvin', 'Liana', 'Riley', 'Mikayla', 'Nicolas', 'Megan', 'Layla', 'Griffin', 'Lina', \n",
    " 'Yerik', 'Zahra', 'Dakota', 'Lea', 'Leora', 'Liba', 'Rosa', 'Lesley', 'Jayden', 'Samuel', 'Brooke', \n",
    " 'Rachel', 'Liliana', 'Kymani', 'Axel', 'Lia', 'Itzel', 'Shaindel', 'Amaya', 'Stella', 'Alexis', \n",
    " 'Valentina', 'Jalen', 'Hersh', 'Alejandro', 'Paris', 'Albert', 'Hershel', 'Nicholas', 'Trinity', \n",
    " 'Danielle', 'Harry', 'Mike', 'Kyla', 'Juliette',  'Michaela', 'Jariel', 'Allen', 'Dariel', 'Darwin',\n",
    " 'Sienna', 'Mendel', 'Mariana', 'Mila', 'Rodney', 'Valerie', 'Zachary', 'Jace', 'Julie', 'Caden',\n",
    " 'Carmen', 'Devorah', 'Denise', 'Jaden', 'Michal', 'Parker', 'Kylie', 'Emilia', 'Elijah', 'Elias',\n",
    " 'Dylan', 'Yousef', 'Ashly', 'Shimon', 'Alexandra', 'Meilech', 'Kameron', 'Harmony', 'Scarlett', \n",
    " 'Roman', 'Annabelle', 'Princess', 'Wendy', 'Sarah', 'Cristofer', 'Suri', 'Naomi', 'Julian',\n",
    " 'Nehemiah', 'Emily', 'Kimora', 'Elliana', 'Selena', 'Morgan', 'Leandro', 'Veronica', 'Joyce', \n",
    " 'Jermaine', 'Ariana', 'Cheskel', 'Alessia', 'George', 'Miley', 'Penelope', 'Kenny', 'Abdoul',\n",
    " 'Armani', 'Angelina', 'Mina', 'Kimberly', 'Summer', 'Jayce', 'Janiyah', 'Simone', 'Avi', 'Adam',\n",
    " 'Fatoumata', 'Caleb', 'Yachet', 'Clara', 'Devin', 'Judah', 'Nancy', 'Jeremias', 'Maggie', 'Georgia',\n",
    " 'Efraim', 'Preston', 'Sloane', 'Emerson', 'Maria', 'Ruby', 'Quincy', 'Destiny', 'Jada', 'Jean',\n",
    " 'Tony', 'Timothy', 'Jeremy', 'Rowan', 'Aditya', 'Aliyah', 'Esmeralda', 'Cristian', 'Marisol',\n",
    " 'Andrew', 'Vanessa', 'Zariah', 'Shoshana', 'Bradley', 'Julius', 'Natalia', 'Keith', 'Lauren',\n",
    " 'Batsheva', 'Asher', 'Jonah', 'Pessy', 'Annie', 'Brian', 'John', 'Brielle', 'Sabrina', 'Josephine',\n",
    " 'Golda', 'Jia', 'Gitty', 'Amiyah', 'Adina', 'Lamar', 'Eli', 'Valeria', 'Camille', 'Taraji',\n",
    " 'Charles', 'Chris', 'Blimy', 'Kacper', 'Jake', 'Allan', 'Nicolette', 'Darius', 'Arya', 'Marquis',\n",
    " 'Paul', 'Carter', 'Peyton', 'Delilah', 'Henny', 'Andy', 'Kaylee', 'Adeline', 'Kennedy', 'Lara',\n",
    " 'Ethan', 'Hershy', 'Berish', 'Patrick', 'Leila', 'Cameron', 'Matthew', 'Sima', 'Lena', 'Eliza',\n",
    " 'Skye', 'Laila', 'Makai', 'Owen', 'Kevin', 'Donovan', 'Aryeh', 'Madison', 'Jaliyah', 'Avery',\n",
    " 'Tessa', 'Derek', 'Lizbeth', 'Casey', 'Zyaire', 'Adonis', 'Evelyn', 'Sebastian', 'Eshal', 'Elliot',\n",
    " 'Lyric', 'Levi', 'Ruben', 'Brucha', 'Bonnie', 'Alfredo', 'Amari', 'Rifka', 'Malachi', 'Charlie',\n",
    " 'Zaire', 'Ari', 'Dominic', 'Elle', 'Toby', 'Jaheim', 'Amber', 'Eason', 'Alexa', 'Conor', 'Jimena',\n",
    " 'Kaiden', 'Tori', 'Rochel', 'Dana', 'Noam', 'Leonardo', 'Alice', 'Ester', 'Simi', 'Haylee',\n",
    " 'Angela', 'Vicky', 'Yaakov', 'Alberto', 'Jaylin', 'Rocco', 'Maya', 'Keyla', 'Jaylah', 'Oliver',\n",
    " 'Yair', 'Shane', 'Scarlet', 'Mariah', 'Pearl', 'Terry', 'Seth', 'Angelo', 'Amina', 'Omar', 'Ezra',\n",
    " 'Raphael', 'Isaiah', 'Theodore', 'Noa', 'Cristina', 'Pablo', 'Bryant', 'Ivy', 'Marvin', 'Lucy', \n",
    " 'Emiliano', 'Roizy', 'Hudson', 'Wyatt', 'Essence', 'Elena', 'Xavier', 'Naftuli', 'Shiloh', 'Ana', \n",
    " 'Nolan', 'Giovanni', 'Nikolas', 'Harrison', 'Aubrey', 'Hadassah', 'Amrom', 'Lila', 'Fatou', \n",
    " 'Ishmael', 'Giuliana', 'Hector', 'Ricky', 'Benzion', 'Boruch', 'Alondra', 'Hindy', 'Claire', \n",
    " 'Julien', 'Johann', 'Roger', 'Syed', 'Angelique', 'Terrence', 'Dulce', 'Perel', 'Ella', 'Nigel',\n",
    " 'Reid', 'Moshe', 'Salvatore', 'Janiel', 'Kathryn', 'Alan', 'Zoe', 'Malky', 'Daniela', 'Greta',\n",
    " 'Aria', 'Yu', 'Eve', 'Meir', 'Ava', 'Khalil', 'Messiah', 'Lyla', 'Rihanna', 'Francisco', 'Sadie',\n",
    " 'Maxim', 'Ahron', 'Kyle', 'Nathalie', 'Shira', 'Philip', 'Cassandra', 'Piper', 'Raquel', 'Milan', \n",
    " 'Nasir', 'Luca', 'Aldo', 'Tamia', 'Raizel', 'Jaylen', 'Aiden', 'Vincenzo', 'Dominick', 'Symphony',\n",
    " 'Celia', 'Moises', 'Bruchy', 'Aissatou', 'Lorenzo', 'Dovid', 'Beckett', 'Kenya', 'Micah', 'Adriel',\n",
    " 'Johan', 'Isabel', 'Samantha', 'Hannah', 'Usher', 'Etty', 'Kailey', 'Gavin', 'Tzipora', 'Perla',\n",
    " 'Savanna', 'Kylee', 'Baruch', 'Aharon', 'Nylah', 'Shevy', 'Aaliyah', 'Arianny', 'Zahara', 'Shifra',\n",
    " 'Anastasia', 'Maximus', 'Sasha', 'Brayden', 'Leilani', 'Jaslene', 'Melany', 'Carmine', 'Yamilet',\n",
    " 'Sydney', 'Lilah', 'Christopher', 'Santino', 'Edwin', 'Aarav', 'Bella', 'Elaine', 'Roselyn', 'Ilan',\n",
    " 'Danny', 'Madeleine', 'Elimelech', 'Aron', 'Giovani', 'Derick', 'Adrian', 'Blake', 'Crystal',\n",
    " 'Ezequiel', 'Serenity', 'Shirley', 'Emmanuel', 'Lawrence', 'Amy', 'Helen', 'Zainab', 'Izabella',\n",
    " 'Yonah', 'Sophie', 'Caroline', 'Maeve', 'Sara', 'Enrique', 'Royce', 'Yehuda', 'Milo', 'Kelvin',\n",
    " 'Samiyah', 'Cora', 'Kaylie', 'Eduardo', 'Katelyn', 'Marlon', 'Raizy', 'Goldy', 'Michelle',\n",
    " 'Eliezer', 'Mackenzie', 'Gabriella', 'Shia', 'Stephanie', 'Jadiel', 'Yidel', 'Lazer', 'Krystal',\n",
    " 'Rayan', 'Moses', 'Yaretzi', 'Stephen', 'Connor', 'Gustavo', 'Samiya', 'Marcus', 'Mason', 'Dalia',\n",
    " 'Ibrahima', 'Maximiliano', 'Menashe', 'Ben', 'Christina', 'Shlomo', 'Ahuva', 'Sherlyn', 'Kelly',\n",
    " 'Erick', 'Samir', 'Hassan', 'Shaina', 'Angelica', 'Luka', 'Aden', 'Gittel', 'Hayley', 'Joseph',\n",
    " 'Sury', 'Justin', 'Celeste', 'Shlome', \"Amar'E\", 'Rodrigo', 'Hailey', 'Kaliyah', 'Leah', 'Faiga',\n",
    " 'Aryan', 'Margaret', 'Jude', 'Matias', 'Gerardo', 'Mayer', 'Sade', 'Nevaeh', 'Isiah', 'Jayda',\n",
    " 'Aimee', 'Jaime', 'Ximena', 'Vivienne', 'Tomas', 'Steven', 'Mariam', 'Yael', 'Tyler', 'Sharon',\n",
    " 'Yitzchok', 'Oscar', 'Maximilian', 'Lucas', 'Mekhi', 'Malia', 'Addison', 'Nelson', 'Chance', \n",
    " 'Eitan', 'Mandy', 'Eliyahu', 'Hawa', 'Ellie', 'Cristopher', 'Janelle', 'Fraidy', 'Ruchy', 'Hanna',\n",
    " 'Milena', 'Analia', 'Derrick', 'Reese', 'Yechiel', 'Sidney', 'Muhammad', 'Colin', 'Audrey', 'Anika',\n",
    " 'Aidan', 'Mary', 'Jahmir', 'Dashiell', 'Andres', 'Brenda', 'April', 'Yida', 'Yasmine', 'Joel', \n",
    " 'Chelsea', 'Karla', 'Lily', 'Elizabeth', 'Autumn', 'Arely', 'Blima', 'Moishe', 'Iris', 'Tiffany',\n",
    " 'Jorge', 'Harper', 'Lillian', 'Tzippy', 'Elisheva', 'Grayson', 'Edgar', 'Yoel', 'Ashton', 'Anderson',\n",
    " 'Genevieve', 'Mateo', 'Amadou', 'Carson', 'Omari', 'Anson', 'Stephany', 'Louis', 'Leslie', 'Hazel',\n",
    " 'Christian', 'Diya', 'Mariama', 'Maxwell', 'Shloime', 'Aaron', 'Bryanna', 'Eileen', 'Eddie',\n",
    " 'Kaleb', 'Wilson', 'Chaim', 'Baila', 'Erika', 'Serena', 'Jasiah', 'Angel', 'Yehudah', 'Ishaan',\n",
    " 'Zaniyah', 'Olivia', 'Mikaela', 'Noemi', 'Emma', 'Leyla', 'Jerry', 'Jamel'}\n",
    "\n",
    "baby_names_2014 = {'Miracle', 'Khloe', 'Ahmed', 'Kira', 'Tzvi', 'Madisyn', 'Archer', 'Alex', \n",
    " 'Isabella', 'Sophia', 'Mathew', 'Mordche', 'Hailie', 'Adrianna', 'Mouhamed', 'Kayden', 'Chloe', \n",
    " 'Tziporah', 'Travis', 'Zamir', 'Amare', 'Julissa', 'Doris', 'Josue', 'Malak', 'Saniyah',\n",
    " 'Melanie', 'Abdullah', 'Adelyn', 'Mohamed', 'Yakov', 'Wynter', 'Brandon', 'Duvid', 'Damon',\n",
    " 'Ezekiel', 'Joy', 'Daphne', 'Nashla', 'Tamar', 'Yandel', 'Joanna', 'Amir', 'Jonathan', 'James',\n",
    " 'Anna', 'Karen', 'Margot', 'Liv', 'Jayla', 'Emanuel', 'Binyomin', 'Winnie', 'Carlos', 'Frederick',\n",
    " 'Jacqueline', 'Egypt', 'Faith', 'Hillel', 'Ryder', 'Alexander', 'Nathaniel', 'Amira', 'Cindy',\n",
    " 'Victoria', 'Mamadou', 'Atara', 'Aya', 'Melissa', 'Chase', 'Matteo', 'Miles', 'Richard', 'Alisa',\n",
    " 'Josiah', 'Erik', 'William', 'Jose', 'Bianca', 'Avraham', 'Gemma', 'Kimi', 'Abraham', 'Henchy',\n",
    " 'Adriana', 'Sanai', 'Kaitlyn', 'Lisa', 'Alejandra', 'Amelia', 'Jessica', 'Kieran', 'Tianna',\n",
    " 'Abigail', 'Erica', 'Syeda', 'Skyla', 'Rebecca', 'Rohan', 'Yadiel', 'Daisy', 'Nathaly', 'Siena',\n",
    " 'Damian', 'Zara', 'Antonio', 'Dante', 'Solomon', 'Lukas', 'Fanta', 'Thomas', 'Kenneth', 'Brynn',\n",
    " 'Carmelo', 'Shloimy', 'Magaly', 'Sean', 'Isabelle', 'Ricardo', 'Malik', 'Elina', 'Kelsey', 'Jane',\n",
    " 'Jocelyn', 'Bryson', 'Cayden', 'Nahla', 'Bryce', 'Aniya', 'Rhys', 'Logan', 'Khadijah', 'Zoey',\n",
    " 'Tess', 'Lucien', 'Alison', 'Winter', 'Ashley', 'Erin', 'Alfred', 'Brendan', 'Heaven', 'Carly',\n",
    " 'Aliza', 'Abel', 'Shea', 'Frank', 'Youssef', 'Marcos', 'Selina', 'Yahir', 'Austin', 'Dahlia',\n",
    " 'Ruth', 'Giuseppe', 'Sincere', 'Ali', 'David', 'August', 'Nora', 'Mia', 'Esther', 'Mohammed',\n",
    " 'Mckenzie', 'Imran', 'Lola', 'Walter', 'Janice', 'Ayesha', 'Tristan', 'Chana', 'Linda', 'Emilio',\n",
    " 'Pinchas', 'Aziza', 'Samira', 'Arisha', 'Goldie', 'Cynthia', 'Queenie', 'Juliana', 'Orlando',\n",
    " 'Phoebe', 'Emely', 'Abby', 'Karina', 'Edison', 'Hugo', 'Aurora', 'Eva', 'Makayla', 'Juan',\n",
    " 'Khadija', 'Zain', 'Nadia', 'Jeffrey', 'Avrohom', 'Morris', 'Colette', 'Gordon', 'Barbara', \n",
    " 'Avital', 'Camilla', 'Esteban', 'Chany', 'Victor', 'Alina', 'Reed', 'Liam', 'Menachem', 'Melody',\n",
    " 'Ahmad', 'Zissy', 'Jared', 'Declan', 'Kaylin', 'Pinchus', 'Maliyah', 'Edward', 'Jack', 'Imani', \n",
    " 'Esme', 'Keila', 'Giovanna', 'Ronan', 'Marilyn', 'Ian', 'Stanley', 'Shawn', 'Thiago', 'Yitzchak',\n",
    " 'Yehoshua', 'Marcel', 'Monica', 'Diana', 'Noel', 'Jesse', 'Cesar', 'Rifky', 'Miguel', 'Ayden',\n",
    " 'Wesley', 'Rafael', 'Yisroel', 'Alicia', 'Yasmin', 'Arvin', 'Mordechai', 'Brayan', 'Sofia', \n",
    " 'Sylvia', 'Athena', 'Skylah', 'Darren', 'Lauryn', 'Irene', 'Bryan', 'Luna', 'Roberto', 'Cassidy',\n",
    " 'Yitty', 'Hana', 'Alessandra', 'Filip', 'Gianna', 'Shaindy', 'Eloise', 'Jayson', 'Briana', 'Juliet',\n",
    " 'Jenna', 'Jessie', 'Sam', 'Jefferson', 'Israel', 'Oumar', 'Tenzin', 'Aicha', 'Moussa', 'Shulem', \n",
    " 'Ada', 'Francesca', 'Alexandria', 'Shmiel', 'Henry', 'Johnny', 'Joselyn', 'Hope', 'Luke', 'Joaquin',\n",
    " 'Max', 'Milana', 'Salma', 'Camila', 'Martin', 'Aarya', 'Jason', 'Gia', 'Brianna', 'Jade',\n",
    " 'Clementine', 'Hunter', 'Ayala', 'Junior', 'Leon', 'Fatima', 'Dov', 'Sarai', 'Jenny', 'Salome', \n",
    " 'Faigy', 'Brittany', 'Ibrahim', 'Kaden', 'Frady', 'Gavriel', 'Danna', 'Armaan', 'Alyssa', 'Jacob',\n",
    " 'Jazmin', 'Katherine', 'Yisrael', 'Mohammad', 'Devora', 'Jeremiah', 'Amirah', 'Diego', 'Paige',\n",
    " 'Anisa', 'Chaya', 'Felix', 'Avigail', 'Carolina', 'Vera', 'Alana', 'Theo', 'Lana', 'Azaan',\n",
    " 'Safa', 'Amara', 'Myles', 'Gregory', 'Bridget', 'Kayleen', 'Silas', 'Chavy', 'Shmuel', 'Noor',\n",
    " 'Bennett', 'Zev', 'Peter', 'Neymar', 'Quinn', 'Zuri', 'Shreya', 'Adele', 'Julia', 'Yaseen', 'Kamari',\n",
    " 'Amelie', 'Dennis', 'Jaxson', 'Mark', 'Brigitte', 'Sawyer', 'Everly', 'Rose', 'Mya', 'Savannah',\n",
    " 'Lilian', 'London', 'Nicole', 'Layan', 'Ayan', 'Trany', 'Anya', 'Molly', 'Angie', 'Kyrie', 'Jonas',\n",
    " 'Ahnaf', 'Adelina', 'Gabrielle', 'Julio', 'Savion', 'Kayla', 'Violeta', 'Yossi', 'Lipa', 'Arjun', \n",
    " 'Brady', 'Allyson', 'Marco', 'Dina', 'Fradel', 'Genesis', 'Shaya', 'Giselle', 'Jaiden', 'Akiva',\n",
    " 'Aisha', 'Beatrice', 'Pedro', 'Ariel', 'Daniella', 'Lindsay', 'Calvin', 'Chanel', 'Liana', 'Riley',\n",
    " 'Mikayla', 'Nicolas', 'Adan', 'Megan', 'Griffin', 'Layla', 'Lina', 'Dakota', 'Lea', 'Leora', 'Liba',\n",
    " 'Rosa', 'Jayden', 'Samuel', 'Brooke', 'Rachel', 'Liliana', 'Axel', 'Lia', 'Itzel', 'Shaindel',\n",
    " 'Amaya', 'Stella', 'Alexis', 'Valentina', 'Jalen', 'Hersh', 'Alejandro', 'Rory', 'Paris', 'Perl',\n",
    " 'Albert', 'Hershel', 'Nicholas', 'Trinity', 'Danielle', 'Harry', 'Mike', 'Kyla', 'June', 'Juliette',\n",
    " 'Michaela', 'Jariel', 'Allen', 'Dariel', 'Skyler', 'Darwin', 'Sienna', 'Mendel', 'Mariana', 'Mila',\n",
    " 'Adyan', 'Valerie', 'Zachary', 'Jace', 'Julie', 'Major', 'Caden', 'Carmen', 'Devorah', 'Marielle',\n",
    " 'Jaden', 'Kylie', 'Parker', 'Keira', 'Emilia', 'Elijah', 'Elias', 'Dylan', 'Yousef', 'Yasmina',\n",
    " 'Denis', 'Shimon', 'Alexandra', 'Meilech', 'Annabelle', 'Harmony', 'Scarlett', 'Roman', 'Menucha',\n",
    " 'Frances', 'Wendy', 'Suri', 'Emily', 'Naomi', 'Julian', 'Sarah', 'Leandro', 'Kimora', 'Selena',\n",
    " 'Lesly', 'Queena', 'Morgan', 'Veronica', 'Joyce', 'Enzo', 'Ariana', 'Cheskel', 'Alessia', 'George',\n",
    " 'Priscilla', 'Penelope', 'Kenny', 'Abdoul', 'Armani', 'Angelina', 'Mina', 'Kimberly', 'Everett',\n",
    " 'Summer', 'Jayce', 'Simone', 'Sylvie', 'Adam', 'Avi', 'Fatoumata', 'Caleb', 'Clara', 'Devin',\n",
    " 'Judah', 'Efraim', 'Maggie', 'Georgia', 'Preston', 'Sloane', 'Emerson', 'Maria', 'Ruby', 'Quincy',\n",
    " 'Destiny', 'Jada', 'Jean', 'Jael', 'Montserrat', 'Tony', 'Jeremy', 'Timothy', 'Amia', 'Rowan',\n",
    " 'Kingston', 'Aliyah', 'Esmeralda', 'Cristian', 'Andrew', 'Zariah', 'Shoshana', 'Bradley', 'Natalia',\n",
    " 'Lauren', 'Batsheva', 'Asher', 'Jonah', 'Pessy', 'Annie', 'Brian', 'Perry', 'Nova', 'John',\n",
    " 'Brielle', 'Sabrina', 'Josephine', 'Golda', 'Gitty', 'Amiyah', 'Adina', 'Eli', 'Valeria', 'Weston',\n",
    " 'Camille', 'Drew', 'Binyamin', 'Ruchel', 'Emmett', 'Charles', 'Chris', 'Blimy', 'Jake', 'Allan',\n",
    " 'Arya', 'Princeton', 'Paul', 'Carter', 'Olive', 'Peyton', 'Delilah', 'Reuben', 'Henny', 'Andy',\n",
    " 'Anabelle', 'Kaylee', 'Adeline', 'Kennedy', 'Lara', 'Ethan', 'Hershy', 'Mathias', 'Berish', 'Evie',\n",
    " 'Leila', 'Patrick', 'Cameron', 'Matthew', 'Sima', 'Lena', 'Eliza', 'Skye', 'Laila', 'Owen',\n",
    " 'Kevin', 'Donovan', 'Reizy', 'Aryeh', 'Madison', 'Avery', 'Tessa', 'Derek', 'Lizbeth', 'Evelyn',\n",
    " 'Adonis', 'Sebastian', 'Brucha', 'Cecelia', 'Elliot', 'Lyric', 'Levi', 'Ruben', 'Amari', 'Malachi',\n",
    " 'Charlie', 'Zaire', 'Ari', 'Dominic', 'Elle', 'Melina', 'Amber', 'Eason', 'Alexa', 'Conor',\n",
    " 'Jimena', 'Kaiden', 'Tori', 'Rochel', 'Noam', 'Leonardo', 'Alice', 'Hinda', 'Ester', 'Haylee',\n",
    " 'Anton', 'Angela', 'Vicky', 'Kristian', 'Alberto', 'Zendaya', 'Jaylin', 'Catalina', 'Antonia',\n",
    " 'Yaakov', 'Jaylah', 'Keyla', 'Maya', 'Rocco', 'Oliver', 'Shane', 'Mariah', 'Pearl', 'Livia',\n",
    " 'Seth', 'Angelo', 'Amina', 'Omar', 'Ezra', 'Raphael', 'Isaiah', 'Journee', 'Noa', 'Theodore', \n",
    " 'Pablo', 'Bryant', 'Ivy', 'Marvin', 'Lucy', 'Emiliano', 'Arabella', 'Roizy', 'Hudson', 'Lincoln',\n",
    " 'Juniper', 'Inaya', 'Wyatt', 'Elena', 'Xavier', 'Naftuli', 'Shiloh', 'Aayan', 'Ana', 'Nolan',\n",
    " 'Giovanni', 'Annalise', 'Harrison', 'Lailah', 'Malaysia', 'Aubrey', 'Hadassah', 'Lila', 'Fatou',\n",
    " 'Giuliana', 'Alexia', 'Hector', 'Ricky', 'Benzion', 'Boruch', 'Alondra', 'Hindy', 'Claire', 'Mushka',\n",
    " 'Syed', 'Angelique', 'Perel', 'Harlow', 'Ella', 'Moshe', 'Reid', 'Salvatore', 'Daniela', 'Alan',\n",
    " 'Zoe', 'Malky', 'Eve', 'Mae', 'Aria', 'Khalil', 'Toby', 'Meir', 'Ava', 'Messiah', 'Lyla',\n",
    " 'Francisco', 'Sadie', 'Maxim', 'Inaaya', 'Nikita', 'Kyle', 'Nathalie', 'Shira', 'Philip', 'Piper',\n",
    " 'Nasir', 'Milan', 'Zane', 'Adelaide', 'Luca', 'Angeline', 'Oumou', 'Jaylen', 'Raizel', 'Amiya',\n",
    " 'Marjorie', 'Aiden', 'Vincenzo', 'Dominick', 'Celia', 'Moises', 'Bruchy', 'Aissatou', 'Hamza', \n",
    " 'Lorenzo', 'Dovid', 'Beckett', 'Gary', 'Zelda', 'Micah', 'Mira', 'Adriel', 'Johan', 'Isabel',\n",
    " 'Samantha', 'Hannah', 'Usher', 'Etty', 'Kailey', 'Cataleya', 'Tzipora', 'Gavin', 'Kylee', 'Baruch',\n",
    " 'Valery', 'Aharon', 'Nylah', 'Leonidas', 'Aaliyah', 'Ivanna', 'Shifra', 'Anastasia', 'Maia',\n",
    " 'Maximus', 'Sasha', 'Brayden', 'Louisa', 'Leilani', 'Poppy', 'Monserrat', 'Melany', 'Sydney',\n",
    " 'Lilah', 'Christopher', 'Santino', 'Edwin', 'Aydin', 'Bella', 'Musa', 'Elaine', 'Danny', 'Fabian',\n",
    " 'Rehan', 'Madeleine', 'Aron', 'Elimelech', 'Phoenix', 'Derick', 'Adrian', 'Blake', 'Crystal',\n",
    " 'Serenity', 'Ezequiel', 'Shirley', 'Emmanuel', 'Lawrence', 'Amy', 'Helen', 'Zainab', 'Izabella',\n",
    " 'Sophie', 'Caroline', 'Maeve', 'Sara', 'Enrique', 'Vanessa', 'Yehuda', 'Milo', 'Cora', 'Kaylie',\n",
    " 'Eduardo', 'Katelyn', 'Marlon', 'Eddy', 'Raizy', 'Goldy', 'Michelle', 'Eliezer', 'Mackenzie',\n",
    " 'Gabriella', 'Litzy', 'Issac', 'Jasmina', 'Shia', 'Stephanie', 'Jadiel', 'Yidel', 'Kristen',\n",
    " 'Lazer', 'Rayan', 'Evangeline', 'Yaretzi', 'Moses', 'Stephen', 'Connor', 'Marcus', 'Mason',\n",
    " 'Ibrahima', 'Maximiliano', 'Menashe', 'Ben', 'Christina', 'Brooks', 'Shlomo', 'Arlo', 'Ahuva',\n",
    " 'Caiden', 'Sherlyn', 'Kelly', 'Erick', 'Samir', 'Hassan', 'Shayan', 'Shaina', 'Angelica',\n",
    " 'Luka', 'Aden', 'Justin', 'Joseph', 'Sury', 'Maisie', 'Annabella', 'Shlome', 'Celeste', 'Rodrigo',\n",
    " 'Hailey', 'Leibish', 'Leah', 'Faiga', 'Aryan', 'Margaret', 'Jude', 'Matias', 'Gerardo', 'Mayer',\n",
    " 'Nevaeh', 'Ximena', 'Jayda', 'Vivienne', 'Zahir', 'Elsa', 'Winston', 'Steven', 'Mariam', 'Emmeline',\n",
    " 'Karas', 'Yael', 'Tyler', 'Sharon', 'Yitzchok', 'Oscar', 'Maximilian', 'Kendrick', 'Lucas', 'Mekhi',\n",
    " 'Malia', 'Addison', 'Zayden', 'Nelson', 'Chance', 'Eitan', 'Mandy', 'Thea', 'Eliyahu', 'Hawa',\n",
    " 'Ellie', 'Blessing', 'Mirel', 'Janelle', 'Ruchy', 'Fraidy', 'Hanna', 'Yechiel', 'Reese', 'Kadiatou',\n",
    " 'Aidan', 'Julius', 'Muhammad', 'Colin', 'Audrey', 'Mary', 'Brenda', 'Dashiell', 'Andres', 'April',\n",
    " 'Joel', 'Yasmine', 'Karla', 'Chelsea', 'Lily', 'Elizabeth', 'Kali', 'Autumn', 'Arely', 'Blima',\n",
    " 'Moishe', 'Iris', 'Tiffany', 'Jorge', 'Neil', 'Lillian', 'Harper', 'Aahil', 'Elisheva', 'Lydia',\n",
    " 'Jana', 'Grayson', 'Edgar', 'Arham', 'Yoel', 'Ashton', 'Anderson', 'Genevieve', 'Mateo', 'Amadou',\n",
    " 'Carson', 'Omari'}\n",
    "\n",
    "# Find the union: all_names\n",
    "all_names = baby_names_2011.union(baby_names_2014)\n",
    "\n",
    "# Print the count of names \n",
    "print(len(all_names))\n",
    "\n",
    "# Find the intersection: overlapping_names\n",
    "overlapping_names = baby_names_2011.intersection(baby_names_2014)\n",
    "\n",
    "# Print the count of names in overlapping_names\n",
    "print(len(overlapping_names))\n",
    "\n",
    "del baby_names_2011, baby_names_2014, all_names, overlapping_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cc37b-1035-438e-ae5c-c68e7ec4f082",
   "metadata": {},
   "source": [
    "## Determining set differences\n",
    "\n",
    "Another way of comparing sets is to use the difference() method. It returns all the items found in one set but not another. It's important to remember the set you call the method on will be the one from which the items are returned. Unlike tuples, you can add() items to a set. A set will only add items that do not exist in the set.\n",
    "\n",
    "In this exercise, you'll explore what names were common in 2011, but are no longer common in 2014. The set baby_names_2014 has been pre-loaded into your workspace. As in the previous exercise, the names have been converted to title case to ensure a proper comparison.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create an empty set called baby_names_2011. You can do this using set().\n",
    "    Use a for loop to iterate over each row in records:\n",
    "        If the first column of each row in records is '2011', add its fourth column to baby_names_2011. Remember that Python is 0-indexed!\n",
    "    Find the difference between baby_names_2011 and baby_names_2014. Store the result as differences.\n",
    "    Print the differences. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7ac0e4ae-a562-44f9-96ae-845a2be23baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "766\n",
      "{'Ingrid', 'Abdul', 'Samara', 'Jamal', 'Benson', 'Stacy', 'Kai', 'Savanna', 'Ariella', 'Tatiana', 'Jacky', 'Serena', 'Breindy', 'Kaelyn', 'Ayaan', 'Justice', 'Finley', 'Jahmir', 'Aleena', 'Elliana', 'Angel', 'Zayan', 'Fiona', 'Perla', 'Hadassa', 'Tzippy', 'Hayley', 'Isabela', 'Keily', 'Jaylyn', 'Ariela', 'Rivka', 'Scarlet', 'Esty', 'Lucia', 'Yachet', 'Abdoulaye', 'Yides', 'Isaac', 'Wilson', 'Yocheved', 'Johanna', 'Jasiah', 'Jia', 'Kamila', 'Isiah', 'Hazel', 'Troy', 'Guadalupe', 'Veronika', 'Jordan', 'Corey', 'Eric', 'Nancy', 'Jay', 'Alisha', 'Matilda', 'Evan', 'Viviana', 'Aiza', 'Yehudis', 'Lilly', 'Jackson', 'Ryan', 'Kate', 'Chava', 'Kameron', 'Anson', 'Eleanor', 'Gabriela', 'Kathryn', 'Louis', 'Christy', 'Chad', 'Terry', 'Heidi', 'Rivky', 'Allison', 'Raymond', 'Anthony', 'Michal', 'Alijah', 'Ayla', 'Jazmine', 'Marquis', 'Alfredo', 'Terrell', 'Luz', 'Jamie', 'Leslie', 'Miriam', 'Paola', \"Amar'E\", 'Jasmin', 'Jermaine', 'Andre', 'Malcolm', 'Nicolette', 'Simi', 'Zyaire', 'Kayleigh', 'Benjamin', 'Darius', 'Isla', 'Arturo', 'Yamilet', 'Ishmael', 'Lesley', 'Jasmine', 'Madelyn', 'Grace', 'Kailyn', 'Amanda', 'Jennifer', 'Elvis', 'Ishaan', 'Aldo', 'Jerry', 'Christine', 'Elise', 'Nayeli', 'Makai', 'Michael', 'Christian', 'Kymani', 'Shevy', 'Arianna', 'Kiara', 'Ivan', 'Krystal', 'Aviva', 'Caitlin', 'Joshua', 'Cody', 'Landon', 'Kiyan', 'Daniel', 'Jamel', 'Miah', 'King', 'Armando', 'Robert', 'Tiana', 'Jamir', 'Heidy', 'Keith', 'Jazlyn', 'Willa', 'Jayleen', 'Bonnie', 'Nathan', 'Aditya', 'Jordyn', 'Nina', 'Frimet', 'Jaheim', 'Maurice', 'Stephany', 'Kaleb', 'Miley', 'Haley', 'Isis', 'Malka', 'Dwayne', 'Violet', 'Abdiel', 'Nathalia', 'Aaron', 'Prince', 'Jaelynn', 'Greta', 'Simon', 'Annabel', 'Rifka', 'Rihanna', 'Katelynn', 'Eliana', 'Marisol', 'Libby', 'Hayden', 'Ilana', 'Madeline', 'Sekou', 'Raquel', 'Katie', 'Nana', 'Charlotte', 'Noemi', 'Zion', 'Casey', 'Taylor', 'Miri', 'Bracha', 'Catherine', 'Farhan', 'Emma', 'Gabriel', 'Natasha', 'Roselyn', 'Geraldine', 'Laura', 'Nataly', 'Sariah', 'Kingsley', 'Jelani', 'Anika', 'Batya', 'Nyla', 'Leo', 'Nigel', 'Idy', 'Miranda', 'Mindy', 'Jaslene', 'Skylar', 'Vincent', 'Eden', 'Devon', 'Noah', 'Leyla', 'Mikaela', 'Rodney', 'Byron', 'Talia', 'Nehemiah', 'Dalia', 'Baila', 'Aarav', 'Alvin', 'Marc', 'Cecilia', 'Natalie', 'Gittel', 'Julianna', 'Vivian', 'Lamar', 'Olivia', 'Jaylene', 'Tzivia', 'Brooklyn', 'Nechama', 'Milena', 'Terrence', 'Ousmane', 'Derrick'}\n"
     ]
    }
   ],
   "source": [
    "baby_names_2011 = {'Khloe', 'Ahmed', 'Byron', 'Kira', 'Tzvi', 'Alex', 'Isabella', 'Jamir', 'Mathew',\n",
    " 'Sophia', 'Adrianna', 'Mauricio', 'Miri', 'Kayden', 'Chloe', 'Tziporah', 'Travis', 'Brianny', \n",
    " 'Amare', 'Julissa', 'Josue', 'Malak', 'Saniyah', 'Melanie', 'Abdullah', 'Mohamed', 'Yakov',\n",
    " 'Brandon', 'Augustus', 'Ezekiel', 'Joy', 'Tamar', 'Yandel', 'Joanna', 'Amir', 'Jonathan', 'James',\n",
    " 'Malcolm', 'Anna', 'Karen', 'Jayla', 'Emanuel', 'Binyomin', 'Farhan', 'Winnie', 'Carlos', 'Egypt',\n",
    " 'Jacqueline', 'Faith', 'Alexander', 'Ryder', 'Viviana', 'Nathaniel', 'Amira', 'Cindy', 'Victoria',\n",
    " 'Mamadou', 'Atara', 'Melissa', 'Chase', 'Alisson', 'Matteo', 'Miles', 'Alisa', 'Josiah', 'Richard',\n",
    " 'Erik', 'Jose', 'Bianca', 'Avraham', 'William', 'Gemma', 'Miah', 'Abraham', 'Sanai', 'Adriana',\n",
    " 'Kaitlyn', 'Tianna', 'Syeda', 'Alejandra', 'Amelia', 'Jasmin', 'Kieran', 'Nathalia', 'Jessica', \n",
    " 'Abigail', 'Erica', 'Tabitha', 'Rebecca', 'Rohan', 'Yadiel', 'Daisy', 'Nathaly', 'Siena', 'Damian', \n",
    " 'Zara', 'Antonio', 'Dante', 'Januel', 'Solomon', 'Lukas', 'Fanta', 'Thomas', 'Terrell', 'Brynn', \n",
    " 'Carmelo', 'Shloimy', 'Ousmane', 'Kenneth', 'Leonel', 'Sean', 'Isabelle', 'Ricardo', 'Jeancarlos',\n",
    " 'Malik', 'Elina', 'Kelsey', 'Armando', 'Jane', 'Jocelyn', 'Bryson', 'Cayden', 'Nahla', 'Bryce',\n",
    " 'Aniya', 'Johanna', 'Kiyan', 'Logan', 'Zoey', 'Alison', 'Erin', 'Ashley', 'Brendan', 'Heaven',\n",
    " 'Johnathan', 'Dwayne', 'Aliza', 'Abel', 'Shea', 'Frank', 'Youssef', 'Marcos', 'Selina', 'Yahir',\n",
    " 'Austin', 'Dahlia', 'Ruth', 'Zaria', 'Sincere', 'Troy', 'Ali', 'David', 'August', 'Nora', 'Lesly',\n",
    " 'Esther', 'Mia', 'Katelynn', 'Janiya', 'Mohammed', 'Mckenzie', 'Lola', 'Ayesha', 'Janice', 'Freddy',\n",
    " 'Tristan', 'Chana', 'Emilio', 'Pinchas', 'Yechezkel', 'Alec', 'Cynthia', 'Queenie', 'Juliana', \n",
    " 'Elianna', 'Kaelyn', 'Phoebe', 'Emely', 'Abby', 'Hugo', 'Hadassa', 'Aurora', 'Eva', 'Makayla',\n",
    " 'Giovanny', 'Juan', 'Zain', 'Nadia', 'Jeffrey', 'Luz', 'Avrohom', 'Morris', 'Colette', 'Barbara',\n",
    " 'Esteban', 'Chany', 'Victor', 'Alina', 'Reed', 'Liam','Menachem', 'Cody', 'Haley', 'Zissy', 'Melody',\n",
    " 'Jared', 'Declan', 'Kaylin', 'Pinchus', 'Maliyah', 'Edward', 'Jack', 'Imani', 'Giovanna', 'Marilyn',\n",
    " 'Ian', 'Stanley', 'Shneur', 'Shawn', 'Abdiel', 'Yitzchak', 'Noel', 'Diana', 'Shaniya', 'Finley',\n",
    " 'Jesse', 'Cesar', 'Rifky', 'Renee', 'Miguel', 'Christine', 'Ayden', 'Wesley', 'Rafael', 'Yaniel',\n",
    " 'Yisroel', 'Alicia', 'Mordechai', 'Brayan', 'Sofia', 'Sylvia', 'Athena', 'Irene', 'Darren', 'Bryan',\n",
    " 'Luna', 'Roberto', 'Yitty', 'Gianna', 'Alessandra', 'Filip', 'Shaindy', 'Eloise', 'Jayson', \n",
    " 'Fernanda', 'Chad', 'Briana', 'Juliet', 'Jenna', 'Jessie', 'Sam', 'Jefferson', 'Israel', 'Tenzin', \n",
    " 'Aicha', 'Moussa', 'Shulem', 'Ada', 'Francesca', 'Nana', 'Shmiel', 'Henry', 'Johnny', 'Joselyn', \n",
    " 'Luke', 'Joaquin', 'Nataly', 'Max', 'Salma', 'Camila', 'Martin', 'Jason', 'Gia', 'Brianna', 'Jade', \n",
    " 'Annabel', 'Hunter', 'Junior', 'Leon', 'Shania', 'Fatima', 'Dov', 'Stacy', 'Sarai', 'Jenny', \n",
    " 'Brittany', 'Faigy', 'Ibrahim', 'Kaden', 'Gavriel', 'Danna', 'Armaan', 'Alyssa', 'Jacob', 'Jazmin', \n",
    " 'Katherine', 'Yisrael', 'Mohammad', 'Devora', 'Jeremiah', 'Amirah', 'Diego', 'Paige', 'Jelani', \n",
    " 'Chaya', 'Felix', 'Avigail', 'Carolina', 'Vera', 'Alana', 'Batya', 'Myles', 'Isabela', 'Gregory', \n",
    " 'Jaylyn', 'Bridget', 'Xin', 'Kayleen', 'Leela', 'Silas', 'Chavy', 'Shmuel', 'Bennett', 'Zev','Peter', \n",
    " 'Heidy', 'Paola', 'Quinn', 'Cheyenne', 'Zuri', 'Adele', 'Julia', 'Yaseen', 'Kamari', 'Saniya', \n",
    " 'Amelie', 'Dennis', 'Mark', 'Elianny', 'Keily', 'Rose', 'Mya', 'Savannah', 'Angely', 'London', \n",
    " 'Nicole', 'Isis', 'Ilana', 'Molly', 'Kamiyah', 'Angie', 'Alijah', 'Gabrielle', 'Julio', 'Kayla', \n",
    " 'Alyson', 'Lipa', 'Marco', 'Arjun', 'Brady', 'Allyson', 'Britney', 'Fradel', 'Genesis', 'Shaya', \n",
    " 'Giselle', 'Jaiden', 'Akiva', 'Dina', 'Aisha', 'Beatrice', 'Pedro', 'Ariel', 'Daniella', 'Lindsay', \n",
    " 'Jaelynn', 'Calvin', 'Liana', 'Riley', 'Mikayla', 'Nicolas', 'Megan', 'Layla', 'Griffin', 'Lina', \n",
    " 'Yerik', 'Zahra', 'Dakota', 'Lea', 'Leora', 'Liba', 'Rosa', 'Lesley', 'Jayden', 'Samuel', 'Brooke', \n",
    " 'Rachel', 'Liliana', 'Kymani', 'Axel', 'Lia', 'Itzel', 'Shaindel', 'Amaya', 'Stella', 'Alexis', \n",
    " 'Valentina', 'Jalen', 'Hersh', 'Alejandro', 'Paris', 'Albert', 'Hershel', 'Nicholas', 'Trinity', \n",
    " 'Danielle', 'Harry', 'Mike', 'Kyla', 'Juliette',  'Michaela', 'Jariel', 'Allen', 'Dariel', 'Darwin',\n",
    " 'Sienna', 'Mendel', 'Mariana', 'Mila', 'Rodney', 'Valerie', 'Zachary', 'Jace', 'Julie', 'Caden',\n",
    " 'Carmen', 'Devorah', 'Denise', 'Jaden', 'Michal', 'Parker', 'Kylie', 'Emilia', 'Elijah', 'Elias',\n",
    " 'Dylan', 'Yousef', 'Ashly', 'Shimon', 'Alexandra', 'Meilech', 'Kameron', 'Harmony', 'Scarlett', \n",
    " 'Roman', 'Annabelle', 'Princess', 'Wendy', 'Sarah', 'Cristofer', 'Suri', 'Naomi', 'Julian',\n",
    " 'Nehemiah', 'Emily', 'Kimora', 'Elliana', 'Selena', 'Morgan', 'Leandro', 'Veronica', 'Joyce', \n",
    " 'Jermaine', 'Ariana', 'Cheskel', 'Alessia', 'George', 'Miley', 'Penelope', 'Kenny', 'Abdoul',\n",
    " 'Armani', 'Angelina', 'Mina', 'Kimberly', 'Summer', 'Jayce', 'Janiyah', 'Simone', 'Avi', 'Adam',\n",
    " 'Fatoumata', 'Caleb', 'Yachet', 'Clara', 'Devin', 'Judah', 'Nancy', 'Jeremias', 'Maggie', 'Georgia',\n",
    " 'Efraim', 'Preston', 'Sloane', 'Emerson', 'Maria', 'Ruby', 'Quincy', 'Destiny', 'Jada', 'Jean',\n",
    " 'Tony', 'Timothy', 'Jeremy', 'Rowan', 'Aditya', 'Aliyah', 'Esmeralda', 'Cristian', 'Marisol',\n",
    " 'Andrew', 'Vanessa', 'Zariah', 'Shoshana', 'Bradley', 'Julius', 'Natalia', 'Keith', 'Lauren',\n",
    " 'Batsheva', 'Asher', 'Jonah', 'Pessy', 'Annie', 'Brian', 'John', 'Brielle', 'Sabrina', 'Josephine',\n",
    " 'Golda', 'Jia', 'Gitty', 'Amiyah', 'Adina', 'Lamar', 'Eli', 'Valeria', 'Camille', 'Taraji',\n",
    " 'Charles', 'Chris', 'Blimy', 'Kacper', 'Jake', 'Allan', 'Nicolette', 'Darius', 'Arya', 'Marquis',\n",
    " 'Paul', 'Carter', 'Peyton', 'Delilah', 'Henny', 'Andy', 'Kaylee', 'Adeline', 'Kennedy', 'Lara',\n",
    " 'Ethan', 'Hershy', 'Berish', 'Patrick', 'Leila', 'Cameron', 'Matthew', 'Sima', 'Lena', 'Eliza',\n",
    " 'Skye', 'Laila', 'Makai', 'Owen', 'Kevin', 'Donovan', 'Aryeh', 'Madison', 'Jaliyah', 'Avery',\n",
    " 'Tessa', 'Derek', 'Lizbeth', 'Casey', 'Zyaire', 'Adonis', 'Evelyn', 'Sebastian', 'Eshal', 'Elliot',\n",
    " 'Lyric', 'Levi', 'Ruben', 'Brucha', 'Bonnie', 'Alfredo', 'Amari', 'Rifka', 'Malachi', 'Charlie',\n",
    " 'Zaire', 'Ari', 'Dominic', 'Elle', 'Toby', 'Jaheim', 'Amber', 'Eason', 'Alexa', 'Conor', 'Jimena',\n",
    " 'Kaiden', 'Tori', 'Rochel', 'Dana', 'Noam', 'Leonardo', 'Alice', 'Ester', 'Simi', 'Haylee',\n",
    " 'Angela', 'Vicky', 'Yaakov', 'Alberto', 'Jaylin', 'Rocco', 'Maya', 'Keyla', 'Jaylah', 'Oliver',\n",
    " 'Yair', 'Shane', 'Scarlet', 'Mariah', 'Pearl', 'Terry', 'Seth', 'Angelo', 'Amina', 'Omar', 'Ezra',\n",
    " 'Raphael', 'Isaiah', 'Theodore', 'Noa', 'Cristina', 'Pablo', 'Bryant', 'Ivy', 'Marvin', 'Lucy', \n",
    " 'Emiliano', 'Roizy', 'Hudson', 'Wyatt', 'Essence', 'Elena', 'Xavier', 'Naftuli', 'Shiloh', 'Ana', \n",
    " 'Nolan', 'Giovanni', 'Nikolas', 'Harrison', 'Aubrey', 'Hadassah', 'Amrom', 'Lila', 'Fatou', \n",
    " 'Ishmael', 'Giuliana', 'Hector', 'Ricky', 'Benzion', 'Boruch', 'Alondra', 'Hindy', 'Claire', \n",
    " 'Julien', 'Johann', 'Roger', 'Syed', 'Angelique', 'Terrence', 'Dulce', 'Perel', 'Ella', 'Nigel',\n",
    " 'Reid', 'Moshe', 'Salvatore', 'Janiel', 'Kathryn', 'Alan', 'Zoe', 'Malky', 'Daniela', 'Greta',\n",
    " 'Aria', 'Yu', 'Eve', 'Meir', 'Ava', 'Khalil', 'Messiah', 'Lyla', 'Rihanna', 'Francisco', 'Sadie',\n",
    " 'Maxim', 'Ahron', 'Kyle', 'Nathalie', 'Shira', 'Philip', 'Cassandra', 'Piper', 'Raquel', 'Milan', \n",
    " 'Nasir', 'Luca', 'Aldo', 'Tamia', 'Raizel', 'Jaylen', 'Aiden', 'Vincenzo', 'Dominick', 'Symphony',\n",
    " 'Celia', 'Moises', 'Bruchy', 'Aissatou', 'Lorenzo', 'Dovid', 'Beckett', 'Kenya', 'Micah', 'Adriel',\n",
    " 'Johan', 'Isabel', 'Samantha', 'Hannah', 'Usher', 'Etty', 'Kailey', 'Gavin', 'Tzipora', 'Perla',\n",
    " 'Savanna', 'Kylee', 'Baruch', 'Aharon', 'Nylah', 'Shevy', 'Aaliyah', 'Arianny', 'Zahara', 'Shifra',\n",
    " 'Anastasia', 'Maximus', 'Sasha', 'Brayden', 'Leilani', 'Jaslene', 'Melany', 'Carmine', 'Yamilet',\n",
    " 'Sydney', 'Lilah', 'Christopher', 'Santino', 'Edwin', 'Aarav', 'Bella', 'Elaine', 'Roselyn', 'Ilan',\n",
    " 'Danny', 'Madeleine', 'Elimelech', 'Aron', 'Giovani', 'Derick', 'Adrian', 'Blake', 'Crystal',\n",
    " 'Ezequiel', 'Serenity', 'Shirley', 'Emmanuel', 'Lawrence', 'Amy', 'Helen', 'Zainab', 'Izabella',\n",
    " 'Yonah', 'Sophie', 'Caroline', 'Maeve', 'Sara', 'Enrique', 'Royce', 'Yehuda', 'Milo', 'Kelvin',\n",
    " 'Samiyah', 'Cora', 'Kaylie', 'Eduardo', 'Katelyn', 'Marlon', 'Raizy', 'Goldy', 'Michelle',\n",
    " 'Eliezer', 'Mackenzie', 'Gabriella', 'Shia', 'Stephanie', 'Jadiel', 'Yidel', 'Lazer', 'Krystal',\n",
    " 'Rayan', 'Moses', 'Yaretzi', 'Stephen', 'Connor', 'Gustavo', 'Samiya', 'Marcus', 'Mason', 'Dalia',\n",
    " 'Ibrahima', 'Maximiliano', 'Menashe', 'Ben', 'Christina', 'Shlomo', 'Ahuva', 'Sherlyn', 'Kelly',\n",
    " 'Erick', 'Samir', 'Hassan', 'Shaina', 'Angelica', 'Luka', 'Aden', 'Gittel', 'Hayley', 'Joseph',\n",
    " 'Sury', 'Justin', 'Celeste', 'Shlome', \"Amar'E\", 'Rodrigo', 'Hailey', 'Kaliyah', 'Leah', 'Faiga',\n",
    " 'Aryan', 'Margaret', 'Jude', 'Matias', 'Gerardo', 'Mayer', 'Sade', 'Nevaeh', 'Isiah', 'Jayda',\n",
    " 'Aimee', 'Jaime', 'Ximena', 'Vivienne', 'Tomas', 'Steven', 'Mariam', 'Yael', 'Tyler', 'Sharon',\n",
    " 'Yitzchok', 'Oscar', 'Maximilian', 'Lucas', 'Mekhi', 'Malia', 'Addison', 'Nelson', 'Chance', \n",
    " 'Eitan', 'Mandy', 'Eliyahu', 'Hawa', 'Ellie', 'Cristopher', 'Janelle', 'Fraidy', 'Ruchy', 'Hanna',\n",
    " 'Milena', 'Analia', 'Derrick', 'Reese', 'Yechiel', 'Sidney', 'Muhammad', 'Colin', 'Audrey', 'Anika',\n",
    " 'Aidan', 'Mary', 'Jahmir', 'Dashiell', 'Andres', 'Brenda', 'April', 'Yida', 'Yasmine', 'Joel', \n",
    " 'Chelsea', 'Karla', 'Lily', 'Elizabeth', 'Autumn', 'Arely', 'Blima', 'Moishe', 'Iris', 'Tiffany',\n",
    " 'Jorge', 'Harper', 'Lillian', 'Tzippy', 'Elisheva', 'Grayson', 'Edgar', 'Yoel', 'Ashton', 'Anderson',\n",
    " 'Genevieve', 'Mateo', 'Amadou', 'Carson', 'Omari', 'Anson', 'Stephany', 'Louis', 'Leslie', 'Hazel',\n",
    " 'Christian', 'Diya', 'Mariama', 'Maxwell', 'Shloime', 'Aaron', 'Bryanna', 'Eileen', 'Eddie',\n",
    " 'Kaleb', 'Wilson', 'Chaim', 'Baila', 'Erika', 'Serena', 'Jasiah', 'Angel', 'Yehudah', 'Ishaan',\n",
    " 'Zaniyah', 'Olivia', 'Mikaela', 'Noemi', 'Emma', 'Leyla', 'Jerry', 'Jamel'}\n",
    "\n",
    "baby_names_2014 = {'Miracle', 'Khloe', 'Ahmed', 'Kira', 'Tzvi', 'Madisyn', 'Archer', 'Alex', \n",
    " 'Isabella', 'Sophia', 'Mathew', 'Mordche', 'Hailie', 'Adrianna', 'Mouhamed', 'Kayden', 'Chloe', \n",
    " 'Tziporah', 'Travis', 'Zamir', 'Amare', 'Julissa', 'Doris', 'Josue', 'Malak', 'Saniyah',\n",
    " 'Melanie', 'Abdullah', 'Adelyn', 'Mohamed', 'Yakov', 'Wynter', 'Brandon', 'Duvid', 'Damon',\n",
    " 'Ezekiel', 'Joy', 'Daphne', 'Nashla', 'Tamar', 'Yandel', 'Joanna', 'Amir', 'Jonathan', 'James',\n",
    " 'Anna', 'Karen', 'Margot', 'Liv', 'Jayla', 'Emanuel', 'Binyomin', 'Winnie', 'Carlos', 'Frederick',\n",
    " 'Jacqueline', 'Egypt', 'Faith', 'Hillel', 'Ryder', 'Alexander', 'Nathaniel', 'Amira', 'Cindy',\n",
    " 'Victoria', 'Mamadou', 'Atara', 'Aya', 'Melissa', 'Chase', 'Matteo', 'Miles', 'Richard', 'Alisa',\n",
    " 'Josiah', 'Erik', 'William', 'Jose', 'Bianca', 'Avraham', 'Gemma', 'Kimi', 'Abraham', 'Henchy',\n",
    " 'Adriana', 'Sanai', 'Kaitlyn', 'Lisa', 'Alejandra', 'Amelia', 'Jessica', 'Kieran', 'Tianna',\n",
    " 'Abigail', 'Erica', 'Syeda', 'Skyla', 'Rebecca', 'Rohan', 'Yadiel', 'Daisy', 'Nathaly', 'Siena',\n",
    " 'Damian', 'Zara', 'Antonio', 'Dante', 'Solomon', 'Lukas', 'Fanta', 'Thomas', 'Kenneth', 'Brynn',\n",
    " 'Carmelo', 'Shloimy', 'Magaly', 'Sean', 'Isabelle', 'Ricardo', 'Malik', 'Elina', 'Kelsey', 'Jane',\n",
    " 'Jocelyn', 'Bryson', 'Cayden', 'Nahla', 'Bryce', 'Aniya', 'Rhys', 'Logan', 'Khadijah', 'Zoey',\n",
    " 'Tess', 'Lucien', 'Alison', 'Winter', 'Ashley', 'Erin', 'Alfred', 'Brendan', 'Heaven', 'Carly',\n",
    " 'Aliza', 'Abel', 'Shea', 'Frank', 'Youssef', 'Marcos', 'Selina', 'Yahir', 'Austin', 'Dahlia',\n",
    " 'Ruth', 'Giuseppe', 'Sincere', 'Ali', 'David', 'August', 'Nora', 'Mia', 'Esther', 'Mohammed',\n",
    " 'Mckenzie', 'Imran', 'Lola', 'Walter', 'Janice', 'Ayesha', 'Tristan', 'Chana', 'Linda', 'Emilio',\n",
    " 'Pinchas', 'Aziza', 'Samira', 'Arisha', 'Goldie', 'Cynthia', 'Queenie', 'Juliana', 'Orlando',\n",
    " 'Phoebe', 'Emely', 'Abby', 'Karina', 'Edison', 'Hugo', 'Aurora', 'Eva', 'Makayla', 'Juan',\n",
    " 'Khadija', 'Zain', 'Nadia', 'Jeffrey', 'Avrohom', 'Morris', 'Colette', 'Gordon', 'Barbara', \n",
    " 'Avital', 'Camilla', 'Esteban', 'Chany', 'Victor', 'Alina', 'Reed', 'Liam', 'Menachem', 'Melody',\n",
    " 'Ahmad', 'Zissy', 'Jared', 'Declan', 'Kaylin', 'Pinchus', 'Maliyah', 'Edward', 'Jack', 'Imani', \n",
    " 'Esme', 'Keila', 'Giovanna', 'Ronan', 'Marilyn', 'Ian', 'Stanley', 'Shawn', 'Thiago', 'Yitzchak',\n",
    " 'Yehoshua', 'Marcel', 'Monica', 'Diana', 'Noel', 'Jesse', 'Cesar', 'Rifky', 'Miguel', 'Ayden',\n",
    " 'Wesley', 'Rafael', 'Yisroel', 'Alicia', 'Yasmin', 'Arvin', 'Mordechai', 'Brayan', 'Sofia', \n",
    " 'Sylvia', 'Athena', 'Skylah', 'Darren', 'Lauryn', 'Irene', 'Bryan', 'Luna', 'Roberto', 'Cassidy',\n",
    " 'Yitty', 'Hana', 'Alessandra', 'Filip', 'Gianna', 'Shaindy', 'Eloise', 'Jayson', 'Briana', 'Juliet',\n",
    " 'Jenna', 'Jessie', 'Sam', 'Jefferson', 'Israel', 'Oumar', 'Tenzin', 'Aicha', 'Moussa', 'Shulem', \n",
    " 'Ada', 'Francesca', 'Alexandria', 'Shmiel', 'Henry', 'Johnny', 'Joselyn', 'Hope', 'Luke', 'Joaquin',\n",
    " 'Max', 'Milana', 'Salma', 'Camila', 'Martin', 'Aarya', 'Jason', 'Gia', 'Brianna', 'Jade',\n",
    " 'Clementine', 'Hunter', 'Ayala', 'Junior', 'Leon', 'Fatima', 'Dov', 'Sarai', 'Jenny', 'Salome', \n",
    " 'Faigy', 'Brittany', 'Ibrahim', 'Kaden', 'Frady', 'Gavriel', 'Danna', 'Armaan', 'Alyssa', 'Jacob',\n",
    " 'Jazmin', 'Katherine', 'Yisrael', 'Mohammad', 'Devora', 'Jeremiah', 'Amirah', 'Diego', 'Paige',\n",
    " 'Anisa', 'Chaya', 'Felix', 'Avigail', 'Carolina', 'Vera', 'Alana', 'Theo', 'Lana', 'Azaan',\n",
    " 'Safa', 'Amara', 'Myles', 'Gregory', 'Bridget', 'Kayleen', 'Silas', 'Chavy', 'Shmuel', 'Noor',\n",
    " 'Bennett', 'Zev', 'Peter', 'Neymar', 'Quinn', 'Zuri', 'Shreya', 'Adele', 'Julia', 'Yaseen', 'Kamari',\n",
    " 'Amelie', 'Dennis', 'Jaxson', 'Mark', 'Brigitte', 'Sawyer', 'Everly', 'Rose', 'Mya', 'Savannah',\n",
    " 'Lilian', 'London', 'Nicole', 'Layan', 'Ayan', 'Trany', 'Anya', 'Molly', 'Angie', 'Kyrie', 'Jonas',\n",
    " 'Ahnaf', 'Adelina', 'Gabrielle', 'Julio', 'Savion', 'Kayla', 'Violeta', 'Yossi', 'Lipa', 'Arjun', \n",
    " 'Brady', 'Allyson', 'Marco', 'Dina', 'Fradel', 'Genesis', 'Shaya', 'Giselle', 'Jaiden', 'Akiva',\n",
    " 'Aisha', 'Beatrice', 'Pedro', 'Ariel', 'Daniella', 'Lindsay', 'Calvin', 'Chanel', 'Liana', 'Riley',\n",
    " 'Mikayla', 'Nicolas', 'Adan', 'Megan', 'Griffin', 'Layla', 'Lina', 'Dakota', 'Lea', 'Leora', 'Liba',\n",
    " 'Rosa', 'Jayden', 'Samuel', 'Brooke', 'Rachel', 'Liliana', 'Axel', 'Lia', 'Itzel', 'Shaindel',\n",
    " 'Amaya', 'Stella', 'Alexis', 'Valentina', 'Jalen', 'Hersh', 'Alejandro', 'Rory', 'Paris', 'Perl',\n",
    " 'Albert', 'Hershel', 'Nicholas', 'Trinity', 'Danielle', 'Harry', 'Mike', 'Kyla', 'June', 'Juliette',\n",
    " 'Michaela', 'Jariel', 'Allen', 'Dariel', 'Skyler', 'Darwin', 'Sienna', 'Mendel', 'Mariana', 'Mila',\n",
    " 'Adyan', 'Valerie', 'Zachary', 'Jace', 'Julie', 'Major', 'Caden', 'Carmen', 'Devorah', 'Marielle',\n",
    " 'Jaden', 'Kylie', 'Parker', 'Keira', 'Emilia', 'Elijah', 'Elias', 'Dylan', 'Yousef', 'Yasmina',\n",
    " 'Denis', 'Shimon', 'Alexandra', 'Meilech', 'Annabelle', 'Harmony', 'Scarlett', 'Roman', 'Menucha',\n",
    " 'Frances', 'Wendy', 'Suri', 'Emily', 'Naomi', 'Julian', 'Sarah', 'Leandro', 'Kimora', 'Selena',\n",
    " 'Lesly', 'Queena', 'Morgan', 'Veronica', 'Joyce', 'Enzo', 'Ariana', 'Cheskel', 'Alessia', 'George',\n",
    " 'Priscilla', 'Penelope', 'Kenny', 'Abdoul', 'Armani', 'Angelina', 'Mina', 'Kimberly', 'Everett',\n",
    " 'Summer', 'Jayce', 'Simone', 'Sylvie', 'Adam', 'Avi', 'Fatoumata', 'Caleb', 'Clara', 'Devin',\n",
    " 'Judah', 'Efraim', 'Maggie', 'Georgia', 'Preston', 'Sloane', 'Emerson', 'Maria', 'Ruby', 'Quincy',\n",
    " 'Destiny', 'Jada', 'Jean', 'Jael', 'Montserrat', 'Tony', 'Jeremy', 'Timothy', 'Amia', 'Rowan',\n",
    " 'Kingston', 'Aliyah', 'Esmeralda', 'Cristian', 'Andrew', 'Zariah', 'Shoshana', 'Bradley', 'Natalia',\n",
    " 'Lauren', 'Batsheva', 'Asher', 'Jonah', 'Pessy', 'Annie', 'Brian', 'Perry', 'Nova', 'John',\n",
    " 'Brielle', 'Sabrina', 'Josephine', 'Golda', 'Gitty', 'Amiyah', 'Adina', 'Eli', 'Valeria', 'Weston',\n",
    " 'Camille', 'Drew', 'Binyamin', 'Ruchel', 'Emmett', 'Charles', 'Chris', 'Blimy', 'Jake', 'Allan',\n",
    " 'Arya', 'Princeton', 'Paul', 'Carter', 'Olive', 'Peyton', 'Delilah', 'Reuben', 'Henny', 'Andy',\n",
    " 'Anabelle', 'Kaylee', 'Adeline', 'Kennedy', 'Lara', 'Ethan', 'Hershy', 'Mathias', 'Berish', 'Evie',\n",
    " 'Leila', 'Patrick', 'Cameron', 'Matthew', 'Sima', 'Lena', 'Eliza', 'Skye', 'Laila', 'Owen',\n",
    " 'Kevin', 'Donovan', 'Reizy', 'Aryeh', 'Madison', 'Avery', 'Tessa', 'Derek', 'Lizbeth', 'Evelyn',\n",
    " 'Adonis', 'Sebastian', 'Brucha', 'Cecelia', 'Elliot', 'Lyric', 'Levi', 'Ruben', 'Amari', 'Malachi',\n",
    " 'Charlie', 'Zaire', 'Ari', 'Dominic', 'Elle', 'Melina', 'Amber', 'Eason', 'Alexa', 'Conor',\n",
    " 'Jimena', 'Kaiden', 'Tori', 'Rochel', 'Noam', 'Leonardo', 'Alice', 'Hinda', 'Ester', 'Haylee',\n",
    " 'Anton', 'Angela', 'Vicky', 'Kristian', 'Alberto', 'Zendaya', 'Jaylin', 'Catalina', 'Antonia',\n",
    " 'Yaakov', 'Jaylah', 'Keyla', 'Maya', 'Rocco', 'Oliver', 'Shane', 'Mariah', 'Pearl', 'Livia',\n",
    " 'Seth', 'Angelo', 'Amina', 'Omar', 'Ezra', 'Raphael', 'Isaiah', 'Journee', 'Noa', 'Theodore', \n",
    " 'Pablo', 'Bryant', 'Ivy', 'Marvin', 'Lucy', 'Emiliano', 'Arabella', 'Roizy', 'Hudson', 'Lincoln',\n",
    " 'Juniper', 'Inaya', 'Wyatt', 'Elena', 'Xavier', 'Naftuli', 'Shiloh', 'Aayan', 'Ana', 'Nolan',\n",
    " 'Giovanni', 'Annalise', 'Harrison', 'Lailah', 'Malaysia', 'Aubrey', 'Hadassah', 'Lila', 'Fatou',\n",
    " 'Giuliana', 'Alexia', 'Hector', 'Ricky', 'Benzion', 'Boruch', 'Alondra', 'Hindy', 'Claire', 'Mushka',\n",
    " 'Syed', 'Angelique', 'Perel', 'Harlow', 'Ella', 'Moshe', 'Reid', 'Salvatore', 'Daniela', 'Alan',\n",
    " 'Zoe', 'Malky', 'Eve', 'Mae', 'Aria', 'Khalil', 'Toby', 'Meir', 'Ava', 'Messiah', 'Lyla',\n",
    " 'Francisco', 'Sadie', 'Maxim', 'Inaaya', 'Nikita', 'Kyle', 'Nathalie', 'Shira', 'Philip', 'Piper',\n",
    " 'Nasir', 'Milan', 'Zane', 'Adelaide', 'Luca', 'Angeline', 'Oumou', 'Jaylen', 'Raizel', 'Amiya',\n",
    " 'Marjorie', 'Aiden', 'Vincenzo', 'Dominick', 'Celia', 'Moises', 'Bruchy', 'Aissatou', 'Hamza', \n",
    " 'Lorenzo', 'Dovid', 'Beckett', 'Gary', 'Zelda', 'Micah', 'Mira', 'Adriel', 'Johan', 'Isabel',\n",
    " 'Samantha', 'Hannah', 'Usher', 'Etty', 'Kailey', 'Cataleya', 'Tzipora', 'Gavin', 'Kylee', 'Baruch',\n",
    " 'Valery', 'Aharon', 'Nylah', 'Leonidas', 'Aaliyah', 'Ivanna', 'Shifra', 'Anastasia', 'Maia',\n",
    " 'Maximus', 'Sasha', 'Brayden', 'Louisa', 'Leilani', 'Poppy', 'Monserrat', 'Melany', 'Sydney',\n",
    " 'Lilah', 'Christopher', 'Santino', 'Edwin', 'Aydin', 'Bella', 'Musa', 'Elaine', 'Danny', 'Fabian',\n",
    " 'Rehan', 'Madeleine', 'Aron', 'Elimelech', 'Phoenix', 'Derick', 'Adrian', 'Blake', 'Crystal',\n",
    " 'Serenity', 'Ezequiel', 'Shirley', 'Emmanuel', 'Lawrence', 'Amy', 'Helen', 'Zainab', 'Izabella',\n",
    " 'Sophie', 'Caroline', 'Maeve', 'Sara', 'Enrique', 'Vanessa', 'Yehuda', 'Milo', 'Cora', 'Kaylie',\n",
    " 'Eduardo', 'Katelyn', 'Marlon', 'Eddy', 'Raizy', 'Goldy', 'Michelle', 'Eliezer', 'Mackenzie',\n",
    " 'Gabriella', 'Litzy', 'Issac', 'Jasmina', 'Shia', 'Stephanie', 'Jadiel', 'Yidel', 'Kristen',\n",
    " 'Lazer', 'Rayan', 'Evangeline', 'Yaretzi', 'Moses', 'Stephen', 'Connor', 'Marcus', 'Mason',\n",
    " 'Ibrahima', 'Maximiliano', 'Menashe', 'Ben', 'Christina', 'Brooks', 'Shlomo', 'Arlo', 'Ahuva',\n",
    " 'Caiden', 'Sherlyn', 'Kelly', 'Erick', 'Samir', 'Hassan', 'Shayan', 'Shaina', 'Angelica',\n",
    " 'Luka', 'Aden', 'Justin', 'Joseph', 'Sury', 'Maisie', 'Annabella', 'Shlome', 'Celeste', 'Rodrigo',\n",
    " 'Hailey', 'Leibish', 'Leah', 'Faiga', 'Aryan', 'Margaret', 'Jude', 'Matias', 'Gerardo', 'Mayer',\n",
    " 'Nevaeh', 'Ximena', 'Jayda', 'Vivienne', 'Zahir', 'Elsa', 'Winston', 'Steven', 'Mariam', 'Emmeline',\n",
    " 'Karas', 'Yael', 'Tyler', 'Sharon', 'Yitzchok', 'Oscar', 'Maximilian', 'Kendrick', 'Lucas', 'Mekhi',\n",
    " 'Malia', 'Addison', 'Zayden', 'Nelson', 'Chance', 'Eitan', 'Mandy', 'Thea', 'Eliyahu', 'Hawa',\n",
    " 'Ellie', 'Blessing', 'Mirel', 'Janelle', 'Ruchy', 'Fraidy', 'Hanna', 'Yechiel', 'Reese', 'Kadiatou',\n",
    " 'Aidan', 'Julius', 'Muhammad', 'Colin', 'Audrey', 'Mary', 'Brenda', 'Dashiell', 'Andres', 'April',\n",
    " 'Joel', 'Yasmine', 'Karla', 'Chelsea', 'Lily', 'Elizabeth', 'Kali', 'Autumn', 'Arely', 'Blima',\n",
    " 'Moishe', 'Iris', 'Tiffany', 'Jorge', 'Neil', 'Lillian', 'Harper', 'Aahil', 'Elisheva', 'Lydia',\n",
    " 'Jana', 'Grayson', 'Edgar', 'Arham', 'Yoel', 'Ashton', 'Anderson', 'Genevieve', 'Mateo', 'Amadou',\n",
    " 'Carson', 'Omari'}\n",
    "\n",
    "\n",
    "records = [['2011', 'FEMALE', 'HISPANIC', 'Geraldine', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Gia', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Gianna', '49', '42'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Giselle', '38', '51'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Grace', '36', '53'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Guadalupe', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hailey', '126', '8'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Haley', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hannah', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Haylee', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hayley', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hazel', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Heaven', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Heidi', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Heidy', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Helen', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Imani', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Ingrid', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Irene', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Iris', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabel', '28', '60'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabela', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabella', '331', '1'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabelle', '18', '70'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isis', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Itzel', '27', '61'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Izabella', '23', '65'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jacqueline', '30', '58'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jada', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jade', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaelynn', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jamie', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Janelle', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaslene', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jasmin', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jasmine', '41', '48'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jayda', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jayla', '33', '55'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylah', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jayleen', '51', '40'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylene', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylin', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylyn', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jazlyn', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jazmin', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jazmine', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jennifer', '59', '34'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jessica', '46', '44'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jimena', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jocelyn', '46', '44'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Johanna', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Joselyn', '30', '58'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Julia', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Juliana', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Julianna', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Juliet', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Juliette', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Julissa', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaelyn', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kailey', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kailyn', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaitlyn', '31', '57'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kamila', '36', '53'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Karen', '41', '48'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Karla', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kate', '27', '61'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katelyn', '23', '65'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katelynn', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katherine', '60', '33'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katie', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kayla', '62', '31'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaylee', '83', '20'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kayleen', '19', '69'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kayleigh', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaylie', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaylin', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Keily', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kelly', '41', '48'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Keyla', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Khloe', '57', '35'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kiara', '36', '53'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kimberly', '103', '13'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Krystal', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kylee', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kylie', '37', '52'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Laila', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Laura', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lauren', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Layla', '52', '39'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lea', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leah', '123', '9'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leila', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leilani', '33', '55'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lesley', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leslie', '66', '27'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lesly', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leyla', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lia', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Liana', '18', '70'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Liliana', '28', '60'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lily', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lindsay', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lizbeth', '30', '58'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'London', '27', '61'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lucia', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Luna', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Luz', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Madeline', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Madelyn', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Madison', '122', '10'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Makayla', '32', '56'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Maria', '59', '34'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mariah', '33', '55'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mariana', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Marilyn', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Marisol', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Maya', '49', '42'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Megan', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melanie', '109', '12'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melany', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melissa', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melody', '23', '65'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mia', '229', '2'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miah', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Michelle', '51', '40'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mikaela', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mikayla', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mila', '19', '69'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miley', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miranda', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miriam', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mya', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nadia', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nancy', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Naomi', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Natalia', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Natalie', '46', '44'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nataly', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Natasha', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nathalia', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nathalie', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nathaly', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nayeli', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nevaeh', '32', '56'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nicole', '71', '25'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nina', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Noemi', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nyla', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Olivia', '86', '18'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Paola', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Penelope', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Perla', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rachel', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Raquel', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rebecca', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rihanna', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Riley', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rosa', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rose', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Roselyn', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Ruby', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sabrina', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sadie', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Samantha', '100', '14'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Samara', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sara', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sarah', '53', '38'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sarai', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sariah', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sasha', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Savanna', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Savannah', '38', '51'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Scarlet', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Scarlett', '65', '28'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Selena', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Serenity', '37', '52'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sherlyn', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Shirley', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sienna', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Skyla', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Skylar', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sofia', '165', '6'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sophia', '223', '3'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sophie', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stacy', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stella', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stephanie', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stephany', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Tatiana', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Taylor', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Tiana', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Tiffany', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Valentina', '85', '19'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Valeria', '45', '45'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Valerie', '53', '38'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Vanessa', '42', '47'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Veronica', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Victoria', '114', '11'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Violet', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Viviana', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Wendy', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Ximena', '18', '70'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Yamilet', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Yaretzi', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Zoe', '65', '28'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Zoey', '27', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Abigail', '103', '19'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Addison', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adele', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adeline', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adina', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adriana', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adrianna', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ahuva', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alessandra', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alessia', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alexa', '54', '40'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alexandra', '109', '17'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alexis', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alice', '50', '43'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alicia', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alina', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alisa', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Aliza', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Allison', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alyssa', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amanda', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amelia', '62', '37'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amelie', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amina', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amira', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amy', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Anastasia', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Angelica', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Angelina', '50', '43'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Anna', '85', '25'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Annabel', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Annabelle', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariana', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Arianna', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariel', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariela', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariella', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ashley', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Atara', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Aubrey', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Audrey', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Autumn', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ava', '162', '8'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Avery', '52', '41'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Avigail', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Aviva', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ayla', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Baila', '40', '51'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Barbara', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Batsheva', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Batya', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Beatrice', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bella', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bianca', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Blake', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Blima', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Blimy', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bracha', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Breindy', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brianna', '32', '59'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bridget', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brooke', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brooklyn', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brucha', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bruchy', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brynn', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Caitlin', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Cameron', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Caroline', '52', '41'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Casey', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Catherine', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Cecilia', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Celia', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chana', '132', '10'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chany', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Charlie', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Charlotte', '127', '12'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chava', '46', '46'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chavy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chaya', '178', '5'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chloe', '95', '22'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Christina', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Claire', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Clara', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Colette', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Cora', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dahlia', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Daisy', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dalia', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Daniela', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Daniella', '41', '50'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Danielle', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Devora', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Devorah', '44', '48'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Diana', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dina', '41', '50'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dylan', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eden', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eleanor', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elena', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eliana', '36', '55'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elise', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elisheva', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eliza', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elizabeth', '100', '21'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ella', '127', '12'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elle', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elliana', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ellie', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eloise', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emerson', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emilia', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emily', '119', '14'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emma', '213', '2'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Erin', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ester', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Esther', '224', '1'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Esty', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Etty', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eva', '64', '35'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eve', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Evelyn', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Faiga', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Faigy', '72', '30'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Finley', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Fiona', '30', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Fradel', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Fraidy', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Francesca', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Frimet', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gabriela', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gabriella', '102', '20'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gabrielle', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gemma', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Genevieve', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Georgia', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gia', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gianna', '68', '33'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Giovanna', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gittel', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gitty', '82', '27'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Giuliana', '41', '50'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Golda', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Goldy', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Grace', '93', '24'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Greta', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hadassa', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hadassah', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hailey', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hanna', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hannah', '71', '31'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Harper', '30', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hazel', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Henny', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hindy', '37', '54'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Idy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ilana', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Iris', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isabel', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isabella', '160', '9'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isabelle', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isla', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ivy', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Izabella', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jacqueline', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jane', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jasmine', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jenna', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jessica', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jordyn', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Josephine', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Joyce', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Julia', '116', '15'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Juliana', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Julianna', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Julie', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Juliet', '36', '55'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Juliette', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kaitlyn', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kate', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Katherine', '44', '48'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kathryn', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kayla', '45', '47'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kaylee', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kira', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kylie', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Laila', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lara', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Laura', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lauren', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Layla', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lea', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Leah', '183', '3'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Leila', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lena', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Leora', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lia', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Liana', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Liba', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Libby', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lila', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lilah', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Liliana', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lillian', '47', '45'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lilly', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lily', '81', '28'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lina', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lola', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'London', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lucia', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lucy', '47', '45'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lyla', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mackenzie', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madeleine', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madeline', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madelyn', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madison', '57', '39'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Maeve', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Makayla', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Malak', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Malka', '63', '36'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Malky', '59', '38'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Margaret', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Maria', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mariam', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mary', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Matilda', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Maya', '94', '23'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Melanie', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mia', '116', '15'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Michaela', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Michal', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Michelle', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mikayla', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mila', '31', '60'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Milena', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mindy', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Miri', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Miriam', '131', '11'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Molly', '36', '55'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Morgan', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Naomi', '46', '46'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Natalia', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Natalie', '40', '51'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nechama', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nicole', '45', '47'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nicolette', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nina', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Noa', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nora', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Olivia', '213', '2'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Paige', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Parker', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Pearl', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Penelope', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Perel', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Pessy', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Phoebe', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Piper', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Quinn', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rachel', '171', '7'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Raizel', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Raizy', '51', '42'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rebecca', '54', '40'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Reese', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rifka', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rifky', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Riley', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rivka', '113', '16'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rivky', '43', '49'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rochel', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Roizy', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rose', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ruby', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ruchy', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ruth', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ryan', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sabrina', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sadie', '32', '59'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Salma', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Samantha', '74', '29'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sara', '107', '18'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sarah', '177', '6'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sasha', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Savannah', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Scarlett', '31', '60'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Serena', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shaina', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shaindel', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shaindy', '48', '44'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shevy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shifra', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shira', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shoshana', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Siena', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sienna', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sima', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Simi', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Simone', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Skylar', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sloane', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sofia', '122', '13'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sophia', '180', '4'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sophie', '65', '34'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Stella', '69', '32'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Summer', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Suri', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sury', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sydney', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sylvia', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Talia', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tamar', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Taylor', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tessa', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Toby', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tzipora', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tziporah', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tzippy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tzivia', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Valentina', '31', '60'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Valerie', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vanessa', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vera', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Veronica', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Veronika', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Victoria', '84', '26'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Violet', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vivian', '30', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vivienne', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Willa', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yachet', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yael', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yasmine', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yehudis', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yides', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yitty', '32', '59'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yocheved', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Zissy', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Zoe', '81', '28'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Zoey', '21', '70'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aarav', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aaron', '51', '19'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Abdul', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Abdullah', '30', '36'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Adam', '28', '38'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aditya', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Adrian', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ahmed', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aidan', '32', '34'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aiden', '96', '7'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alan', '37', '30'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alex', '52', '18'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexander', '47', '22'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ali', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Allen', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alvin', '43', '25'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Andrew', '70', '14'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Andy', '43', '25'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Anson', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Anthony', '46', '23'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Arjun', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Armaan', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aryan', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Austin', '64', '16'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayaan', '31', '35'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayden', '24', '42'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Benjamin', '39', '28'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Benson', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Brandon', '29', '37'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Brian', '40', '27'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Bryan', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Caleb', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Calvin', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Carson', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Charles', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Christian', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Christopher', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Cody', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Connor', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Daniel', '84', '10'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Danny', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Darren', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'David', '28', '38'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Derek', '32', '34'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Devin', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Dylan', '53', '17'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Eason', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Edison', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Edwin', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Elijah', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Elvis', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Eric', '85', '9'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ethan', '177', '1'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Evan', '51', '19'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Farhan', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Felix', '22', '44'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Gabriel', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Gavin', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'George', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Harrison', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Hayden', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Henry', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ian', '22', '44'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ibrahim', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Isaac', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ishaan', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ivan', '66', '15'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jack', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jackson', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jacky', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jacob', '49', '21'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jaden', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jake', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'James', '36', '31'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jason', '98', '6'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jay', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jayden', '173', '2'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jeffrey', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jeremy', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jerry', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jia', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'John', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Johnny', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jonathan', '50', '20'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jordan', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Joseph', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Joshua', '36', '31'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Julian', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Justin', '110', '4'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kai', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kevin', '92', '8'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kingsley', '36', '31'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kyle', '45', '24'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Lawrence', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Leo', '40', '27'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Leon', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Liam', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Logan', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Louis', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Lucas', '103', '5'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Luke', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Marcus', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Martin', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mason', '34', '33'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Matthew', '74', '12'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Max', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Michael', '38', '29'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Miles', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mohamed', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mohammad', '46', '23'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mohammed', '41', '26'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Muhammad', '76', '11'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nathan', '41', '26'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nathaniel', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nelson', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nicholas', '26', '40'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Noah', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Oliver', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Oscar', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Owen', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Patrick', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Peter', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Rayan', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Raymond', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Richard', '26', '40'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ricky', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Rohan', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ryan', '150', '3'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Samuel', '30', '36'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Sean', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Sebastian', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Shawn', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Simon', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Stanley', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Steven', '35', '32'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Syed', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Tenzin', '24', '42'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Terry', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Thomas', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Timothy', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Tony', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Travis', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Tyler', '31', '35'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Victor', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Vincent', '71', '13'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'William', '66', '15'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Wilson', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Xavier', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Zachary', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Zain', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Zayan', '10', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aaron', '53', '28'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Abdoul', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Abdoulaye', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Adam', '28', '46'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aden', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Adonis', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Adrian', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ahmed', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aidan', '20', '54'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aiden', '167', '2'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Alexander', '39', '35'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ali', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Alijah', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Alvin', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amadou', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', \"Amar'E\", '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amare', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amari', '57', '27'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amir', '89', '12'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Andre', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Andrew', '36', '38'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Angel', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Anthony', '66', '22'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Antonio', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ashton', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Austin', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Avery', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ayden', '88', '13'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Benjamin', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Blake', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Brandon', '57', '27'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Brian', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Bryan', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Bryce', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Bryson', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Caleb', '44', '31'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Cameron', '47', '29'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Carmelo', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Carter', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Cayden', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chad', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chance', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Charles', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chase', '41', '33'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chris', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Christian', '91', '11'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Christopher', '80', '19'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Cody', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Corey', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Daniel', '81', '18'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Darius', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Darren', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'David', '84', '15'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Derrick', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Devin', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Devon', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Dominic', '17', '57'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Donovan', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Dwayne', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Dylan', '42', '32'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Edward', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Eli', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Elias', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Elijah', '156', '3'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Emmanuel', '46', '30'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Eric', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ethan', '124', '6'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Evan', '26', '48'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Gabriel', '36', '38'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Gavin', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'George', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Giovanni', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Hassan', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Hayden', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Hunter', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ian', '21', '53'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ibrahim', '25', '49'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ibrahima', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Isaac', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Isaiah', '105', '8'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ishmael', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Isiah', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jace', '21', '53'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jackson', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jacob', '82', '17'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaden', '75', '20'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaheim', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jahmir', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaiden', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jalen', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jamal', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jamel', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'James', '33', '41'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jamir', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jared', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jasiah', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jason', '44', '31'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jayden', '184', '1'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaylen', '38', '36'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jayson', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jelani', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jeremiah', '153', '4'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jeremy', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jermaine', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jesse', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Joel', '37', '37'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'John', '34', '40'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jonathan', '53', '28'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jordan', '53', '28'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Joseph', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Joshua', '128', '5'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Josiah', '121', '7'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Julian', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Justice', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Justin', '85', '14'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kaden', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kai', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kaiden', '28', '46'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kaleb', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kamari', '17', '57'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kameron', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kayden', '37', '37'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Keith', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kenneth', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kevin', '28', '46'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Khalil', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'King', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kiyan', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kyle', '40', '34'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kymani', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Lamar', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Landon', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Levi', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Liam', '61', '24'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Logan', '25', '49'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Lucas', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Makai', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Malachi', '72', '21'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Malcolm', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Malik', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mamadou', '27', '47'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Marc', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Marcus', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Marquis', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mason', '83', '16'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Matthew', '59', '25'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Maurice', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mekhi', '33', '41'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Messiah', '23', '51'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Micah', '29', '45'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Michael', '93', '10'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Miles', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mohamed', '37', '37'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mohammed', '23', '51'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Moussa', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Myles', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nana', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nasir', '33', '41'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nathan', '32', '42'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nathaniel', '64', '23'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nehemiah', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nicholas', '31', '43'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nicolas', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nigel', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Noah', '91', '11'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Omar', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Omari', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ousmane', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Patrick', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Preston', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Prince', '32', '42'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Quincy', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ricardo', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Richard', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Robert', '29', '45'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Rodney', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ryan', '41', '33'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Samuel', '30', '44'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sean', '29', '45'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sebastian', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sekou', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Seth', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Shawn', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sincere', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Stephen', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Steven', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Terrell', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Terrence', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Timothy', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Travis', '17', '57'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Tristan', '58', '26'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Troy', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Tyler', '101', '9'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Victor', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'William', '27', '47'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Xavier', '58', '26'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zachary', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zaire', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zion', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zyaire', '14', '60'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aaron', '102', '33'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Abdiel', '12', '92'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Abel', '14', '90'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Abraham', '22', '82'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adam', '32', '72'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aden', '13', '91'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adonis', '16', '88'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adrian', '157', '19'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adriel', '34', '70'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aidan', '36', '68'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aiden', '188', '12'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alan', '38', '66'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Albert', '13', '91'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alberto', '15', '89'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aldo', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alejandro', '68', '47'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alex', '68', '47'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alexander', '219', '9'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alexis', '55', '54'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alfredo', '15', '89'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Allan', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Allen', '22', '82'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alvin', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Amir', '21', '83'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Anderson', '33', '71'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andre', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andres', '51', '57'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andrew', '125', '25'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andy', '37', '67'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Angel', '253', '5'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Angelo', '28', '76'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Anthony', '207', '10'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Antonio', '32', '72'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Ariel', '26', '78'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Armando', '11', '93'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Armani', '20', '84'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Arturo', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Ashton', '17', '87'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Austin', '15', '89'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Avery', '13', '91'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Axel', '69', '46'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Ayden', '104', '31'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Benjamin', '51', '57'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bradley', '24', '80'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brandon', '184', '13'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brayan', '23', '81'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brayden', '16', '88'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brian', '59', '52'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bryan', '99', '35'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bryant', '17', '87'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bryce', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Byron', '19', '85'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Caleb', '42', '64'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Cameron', '19', '85'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Carlos', '109', '30'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Carmelo', '16', '88'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Cesar', '29', '75'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Abigail', '24', '24'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ada', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aisha', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aiza', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aleena', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexa', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexandra', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alice', '27', '21'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alina', '25', '23'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alisha', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aliyah', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Allison', '17', '31'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alyssa', '26', '22'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amanda', '22', '26'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amber', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amelia', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amy', '24', '24'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angel', '22', '26'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angela', '47', '10'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angelina', '26', '22'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angie', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anika', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anna', '30', '19'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Annabelle', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Annie', '20', '28'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aria', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ariana', '15', '33'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arianna', '11', '37'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arya', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ashley', '52', '8'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Audrey', '19', '29'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ava', '23', '25'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayesha', '17', '31'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Bella', '16', '32'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Bonnie', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Brianna', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Catherine', '27', '21'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cecilia', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Charlotte', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Chloe', '106', '2'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christina', '20', '28'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christine', '11', '37'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christy', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cindy', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Claire', '27', '21'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cynthia', '14', '34']]\n",
    "\n",
    "# Create the empty set: baby_names_2011\n",
    "baby_names_2011 = set()\n",
    "\n",
    "print(len(records))\n",
    "\n",
    "# Loop over records and add the names from 2011 to the baby_names_2011 set\n",
    "for row in records:\n",
    "    # Check if the first column is '2011'\n",
    "    if row[0] == '2011':\n",
    "        # Add the fourth column to the set\n",
    "        baby_names_2011.add(row[3])\n",
    "\n",
    "print(len(baby_names_2011))\n",
    "        \n",
    "# Find the difference between 2011 and 2014: differences\n",
    "differences = baby_names_2011.difference(baby_names_2014)\n",
    "\n",
    "# Print the differences\n",
    "print(differences)\n",
    "\n",
    "\n",
    "del records, baby_names_2011, baby_names_2014, differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51ada9-1130-40ba-83a1-e0de5b81282b",
   "metadata": {},
   "source": [
    "## Using dictionaries\n",
    "\n",
    "\n",
    "\n",
    "**Now we have familiar with the container sequence types, lets dive into dictionaries.  \n",
    "**People often joke that everything in Python is a dictionary. Many find using dictionary most\n",
    "\n",
    "## Creating and looping though dictionaries\n",
    "   __Hold data in key/value pairs__\n",
    "   __Nestable(use a dictionary as the value of a key within a dictionary)__\n",
    "   __Iterable__\n",
    "   __Create by dict() or{}__\n",
    "   \n",
    "\n",
    "**We can iterate over the keys and values of a dictionary.\n",
    "**We can also iterate over the items of a dictionary, which are tuples of the key/value pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180730d-2aad-41c9-a12d-9e108b57f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and looping though dictionaries\n",
    "\n",
    "art_galleries = {}\n",
    "\n",
    "# ***************************\n",
    "# Say we got a list of tuples containing the names and zip for NY Art Galleries\n",
    "for name, zip_code in galleries:\n",
    "    art_galleries[name] = zip_code\n",
    "    \n",
    "for name in art_galleries:\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a6085-cdf7-4540-a4c0-66db98c40cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely finding by key\n",
    "\n",
    "art_galleries[\"Louvre\"]\n",
    "   # if the key is not contained in the dictionary, code thrown error\n",
    "    \n",
    "# .get() method allows yu to safely access a key without error or exception handing\n",
    "art_galleries.get(\"Louver\", \"Louver not found\")\n",
    "art_galleries.get(\"Zarre Andre Gallery\")\n",
    "# return '10011'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c66e9b-d976-4099-807f-3a4c17b72f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with nested dictionaries\n",
    "art_galleries.keys()\n",
    "#returns:\n",
    "[\"10021\", \"10026\", \"10027\"]\n",
    "\n",
    "art_galleries[\"10027\"]\n",
    "#returns:\n",
    "{\"Paige's Art Gallery\": \"(212) 531-1577\",\n",
    "\"Triple Candie\": \"(212) 865-0783\"}\n",
    "\n",
    "\n",
    "# You can access nested values by providing multiple indices to the dictionary or using .get()\n",
    "art_galleries[\"10027\"][\"Paige's Art Gallery\"]\n",
    "#returns:\n",
    "\"(212) 531-1577\"\n",
    "\n",
    "\n",
    "# Nesting dictionaries is a very common way to deal with repeating data structries such as \n",
    "# yearly data, grouped, or hierarchical data such as orginization reporting structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97435890-85ba-4e6c-a7a2-ca7413d3e8fe",
   "metadata": {},
   "source": [
    "## Creating and looping through dictionaries\n",
    "\n",
    "You'll often encounter the need to loop over some array type data, like in Chapter 1, and provide it some structure so you can find the data you desire quickly.\n",
    "\n",
    "You start that by creating an empty dictionary and assigning part of your array data as the key and the rest as the value.\n",
    "\n",
    "Previously, you used sorted() to organize your data in a list. Dictionaries can also be sorted. By default, using sorted() on a dictionary will sort by the keys of the dictionary. You can also reverse the order by passing reverse=True as a keyword argument.\n",
    "\n",
    "Finally, since sorted returns a list, you can use slice notation to select only part of the list. For example, \n",
    "# [:10] will slice the first ten items off a list and return only those items.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create an empty dictionary called names_by_rank.\n",
    "    Loop over female_baby_names_2012.items(), unpacking it into the variables rank and name.\n",
    "    Inside the loop, add each name to the names_by_rank dictionary using the rank as the key.\n",
    "    Sort the names_by_rank dictionary keys in descending order, select the first ten items. Print each item.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e9b908-614c-4f2d-be7b-21d303ed9c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geraldine\n",
      "Gia\n",
      "Gianna\n",
      "Giselle\n",
      "Grace\n",
      "592\n",
      "481\n",
      "{'Geraldine': 1, 'Gia': 2, 'Gianna': 2, 'Giselle': 1, 'Grace': 2, 'Guadalupe': 1, 'Hailey': 2, 'Haley': 1, 'Hannah': 2, 'Haylee': 1, 'Hayley': 1, 'Hazel': 2, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Iris': 2, 'Isabel': 2, 'Isabela': 1, 'Isabella': 2, 'Isabelle': 2, 'Isis': 1, 'Itzel': 1, 'Izabella': 2, 'Jacqueline': 2, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jasmine': 2, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jessica': 2, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kaitlyn': 2, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Kate': 2, 'Katelyn': 1, 'Katelynn': 1, 'Katherine': 2, 'Katie': 1, 'Kayla': 2, 'Kaylee': 2, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'Lindsay': 1, 'Lizbeth': 1, 'London': 2, 'Lucia': 2, 'Luna': 1, 'Luz': 1, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Maya': 2, 'Megan': 1, 'Melanie': 2, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Mia': 2, 'Miah': 1, 'Michelle': 2, 'Mikaela': 1, 'Mikayla': 2, 'Mila': 2, 'Miley': 1, 'Miranda': 1, 'Miriam': 2, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Nicole': 2, 'Nina': 2, 'Noemi': 1, 'Nyla': 1, 'Olivia': 2, 'Paola': 1, 'Penelope': 2, 'Perla': 1, 'Rachel': 2, 'Raquel': 1, 'Rebecca': 2, 'Rihanna': 1, 'Riley': 2, 'Rosa': 1, 'Rose': 2, 'Roselyn': 1, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Samara': 1, 'Sara': 2, 'Sarah': 2, 'Sarai': 1, 'Sariah': 1, 'Sasha': 2, 'Savanna': 1, 'Savannah': 2, 'Scarlet': 1, 'Scarlett': 2, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Sienna': 2, 'Skyla': 1, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stacy': 1, 'Stella': 2, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Taylor': 2, 'Tiana': 1, 'Tiffany': 1, 'Valentina': 2, 'Valeria': 1, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexa': 2, 'Alexandra': 2, 'Alexis': 1, 'Alice': 2, 'Alicia': 1, 'Alina': 2, 'Alisa': 1, 'Aliza': 1, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Amy': 2, 'Anastasia': 1, 'Angelica': 1, 'Angelina': 2, 'Anna': 2, 'Annabel': 1, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Ashley': 2, 'Atara': 1, 'Aubrey': 1, 'Audrey': 2, 'Autumn': 1, 'Ava': 2, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bella': 2, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Brianna': 2, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Catherine': 2, 'Cecilia': 2, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Charlotte': 2, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1}\n"
     ]
    }
   ],
   "source": [
    "records = [['2011', 'FEMALE', 'HISPANIC', 'Geraldine', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Gia', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Gianna', '49', '42'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Giselle', '38', '51'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Grace', '36', '53'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Guadalupe', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hailey', '126', '8'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Haley', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hannah', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Haylee', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hayley', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Hazel', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Heaven', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Heidi', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Heidy', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Helen', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Imani', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Ingrid', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Irene', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Iris', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabel', '28', '60'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabela', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabella', '331', '1'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isabelle', '18', '70'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Isis', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Itzel', '27', '61'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Izabella', '23', '65'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jacqueline', '30', '58'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jada', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jade', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaelynn', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jamie', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Janelle', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaslene', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jasmin', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jasmine', '41', '48'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jayda', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jayla', '33', '55'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylah', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jayleen', '51', '40'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylene', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylin', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jaylyn', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jazlyn', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jazmin', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jazmine', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jennifer', '59', '34'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jessica', '46', '44'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jimena', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Jocelyn', '46', '44'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Johanna', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Joselyn', '30', '58'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Julia', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Juliana', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Julianna', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Juliet', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Juliette', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Julissa', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaelyn', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kailey', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kailyn', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaitlyn', '31', '57'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kamila', '36', '53'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Karen', '41', '48'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Karla', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kate', '27', '61'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katelyn', '23', '65'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katelynn', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katherine', '60', '33'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Katie', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kayla', '62', '31'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaylee', '83', '20'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kayleen', '19', '69'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kayleigh', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaylie', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kaylin', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Keily', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kelly', '41', '48'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Keyla', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Khloe', '57', '35'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kiara', '36', '53'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kimberly', '103', '13'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Krystal', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kylee', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Kylie', '37', '52'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Laila', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Laura', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lauren', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Layla', '52', '39'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lea', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leah', '123', '9'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leila', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leilani', '33', '55'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lesley', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leslie', '66', '27'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lesly', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Leyla', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lia', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Liana', '18', '70'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Liliana', '28', '60'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lily', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lindsay', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lizbeth', '30', '58'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'London', '27', '61'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Lucia', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Luna', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Luz', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Madeline', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Madelyn', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Madison', '122', '10'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Makayla', '32', '56'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Maria', '59', '34'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mariah', '33', '55'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mariana', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Marilyn', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Marisol', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Maya', '49', '42'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Megan', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melanie', '109', '12'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melany', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melissa', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Melody', '23', '65'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mia', '229', '2'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miah', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Michelle', '51', '40'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mikaela', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mikayla', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mila', '19', '69'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miley', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miranda', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Miriam', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Mya', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nadia', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nancy', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Naomi', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Natalia', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Natalie', '46', '44'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nataly', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Natasha', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nathalia', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nathalie', '26', '62'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nathaly', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nayeli', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nevaeh', '32', '56'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nicole', '71', '25'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nina', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Noemi', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Nyla', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Olivia', '86', '18'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Paola', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Penelope', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Perla', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rachel', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Raquel', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rebecca', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rihanna', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Riley', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rosa', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Rose', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Roselyn', '16', '72'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Ruby', '20', '68'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sabrina', '25', '63'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sadie', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Samantha', '100', '14'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Samara', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sara', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sarah', '53', '38'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sarai', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sariah', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sasha', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Savanna', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Savannah', '38', '51'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Scarlet', '29', '59'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Scarlett', '65', '28'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Selena', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Serenity', '37', '52'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sherlyn', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Shirley', '15', '73'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sienna', '17', '71'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Skyla', '21', '67'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Skylar', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sofia', '165', '6'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sophia', '223', '3'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Sophie', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stacy', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stella', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stephanie', '50', '41'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Stephany', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Tatiana', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Taylor', '13', '75'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Tiana', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Tiffany', '24', '64'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Valentina', '85', '19'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Valeria', '45', '45'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Valerie', '53', '38'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Vanessa', '42', '47'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Veronica', '10', '78'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Victoria', '114', '11'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Violet', '14', '74'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Viviana', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Wendy', '12', '76'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Ximena', '18', '70'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Yamilet', '11', '77'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Yaretzi', '22', '66'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Zoe', '65', '28'],\n",
    " ['2011', 'FEMALE', 'HISPANIC', 'Zoey', '27', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Abigail', '103', '19'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Addison', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adele', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adeline', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adina', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adriana', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Adrianna', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ahuva', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alessandra', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alessia', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alexa', '54', '40'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alexandra', '109', '17'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alexis', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alice', '50', '43'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alicia', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alina', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alisa', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Aliza', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Allison', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Alyssa', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amanda', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amelia', '62', '37'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amelie', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amina', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amira', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Amy', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Anastasia', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Angelica', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Angelina', '50', '43'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Anna', '85', '25'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Annabel', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Annabelle', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariana', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Arianna', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariel', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariela', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ariella', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ashley', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Atara', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Aubrey', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Audrey', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Autumn', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ava', '162', '8'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Avery', '52', '41'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Avigail', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Aviva', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ayla', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Baila', '40', '51'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Barbara', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Batsheva', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Batya', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Beatrice', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bella', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bianca', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Blake', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Blima', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Blimy', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bracha', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Breindy', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brianna', '32', '59'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bridget', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brooke', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brooklyn', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brucha', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Bruchy', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Brynn', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Caitlin', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Cameron', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Caroline', '52', '41'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Casey', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Catherine', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Cecilia', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Celia', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chana', '132', '10'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chany', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Charlie', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Charlotte', '127', '12'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chava', '46', '46'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chavy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chaya', '178', '5'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Chloe', '95', '22'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Christina', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Claire', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Clara', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Colette', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Cora', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dahlia', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Daisy', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dalia', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Daniela', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Daniella', '41', '50'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Danielle', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Devora', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Devorah', '44', '48'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Diana', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dina', '41', '50'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Dylan', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eden', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eleanor', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elena', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eliana', '36', '55'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elise', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elisheva', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eliza', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elizabeth', '100', '21'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ella', '127', '12'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elle', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Elliana', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ellie', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eloise', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emerson', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emilia', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emily', '119', '14'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Emma', '213', '2'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Erin', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ester', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Esther', '224', '1'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Esty', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Etty', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eva', '64', '35'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Eve', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Evelyn', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Faiga', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Faigy', '72', '30'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Finley', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Fiona', '30', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Fradel', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Fraidy', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Francesca', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Frimet', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gabriela', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gabriella', '102', '20'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gabrielle', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gemma', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Genevieve', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Georgia', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gia', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gianna', '68', '33'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Giovanna', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gittel', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Gitty', '82', '27'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Giuliana', '41', '50'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Golda', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Goldy', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Grace', '93', '24'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Greta', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hadassa', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hadassah', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hailey', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hanna', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hannah', '71', '31'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Harper', '30', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hazel', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Henny', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Hindy', '37', '54'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Idy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ilana', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Iris', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isabel', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isabella', '160', '9'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isabelle', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Isla', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ivy', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Izabella', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jacqueline', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jane', '27', '64'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jasmine', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jenna', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jessica', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Jordyn', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Josephine', '33', '58'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Joyce', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Julia', '116', '15'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Juliana', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Julianna', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Julie', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Juliet', '36', '55'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Juliette', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kaitlyn', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kate', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Katherine', '44', '48'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kathryn', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kayla', '45', '47'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kaylee', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kira', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Kylie', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Laila', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lara', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Laura', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lauren', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Layla', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lea', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Leah', '183', '3'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Leila', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lena', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Leora', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lia', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Liana', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Liba', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Libby', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lila', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lilah', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Liliana', '29', '62'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lillian', '47', '45'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lilly', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lily', '81', '28'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lina', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lola', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'London', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lucia', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lucy', '47', '45'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Lyla', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mackenzie', '18', '73'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madeleine', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madeline', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madelyn', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Madison', '57', '39'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Maeve', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Makayla', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Malak', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Malka', '63', '36'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Malky', '59', '38'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Margaret', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Maria', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mariam', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mary', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Matilda', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Maya', '94', '23'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Melanie', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mia', '116', '15'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Michaela', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Michal', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Michelle', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mikayla', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mila', '31', '60'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Milena', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Mindy', '24', '67'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Miri', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Miriam', '131', '11'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Molly', '36', '55'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Morgan', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Naomi', '46', '46'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Natalia', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Natalie', '40', '51'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nechama', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nicole', '45', '47'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nicolette', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nina', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Noa', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Nora', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Olivia', '213', '2'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Paige', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Parker', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Pearl', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Penelope', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Perel', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Pessy', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Phoebe', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Piper', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Quinn', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rachel', '171', '7'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Raizel', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Raizy', '51', '42'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rebecca', '54', '40'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Reese', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rifka', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rifky', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Riley', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rivka', '113', '16'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rivky', '43', '49'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rochel', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Roizy', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Rose', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ruby', '28', '63'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ruchy', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ruth', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Ryan', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sabrina', '34', '57'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sadie', '32', '59'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Salma', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Samantha', '74', '29'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sara', '107', '18'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sarah', '177', '6'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sasha', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Savannah', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Scarlett', '31', '60'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Serena', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shaina', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shaindel', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shaindy', '48', '44'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shevy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shifra', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shira', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Shoshana', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Siena', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sienna', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sima', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Simi', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Simone', '15', '76'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Skylar', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sloane', '17', '74'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sofia', '122', '13'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sophia', '180', '4'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sophie', '65', '34'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Stella', '69', '32'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Summer', '16', '75'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Suri', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sury', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sydney', '35', '56'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Sylvia', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Talia', '26', '65'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tamar', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Taylor', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tessa', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Toby', '39', '52'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tzipora', '12', '79'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tziporah', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tzippy', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Tzivia', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Valentina', '31', '60'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Valerie', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vanessa', '14', '77'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vera', '19', '72'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Veronica', '22', '69'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Veronika', '11', '80'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Victoria', '84', '26'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Violet', '38', '53'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vivian', '30', '61'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Vivienne', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Willa', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yachet', '13', '78'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yael', '21', '70'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yasmine', '10', '81'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yehudis', '23', '68'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yides', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yitty', '32', '59'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Yocheved', '20', '71'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Zissy', '25', '66'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Zoe', '81', '28'],\n",
    " ['2011', 'FEMALE', 'WHITE NON HISPANIC', 'Zoey', '21', '70'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aarav', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aaron', '51', '19'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Abdul', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Abdullah', '30', '36'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Adam', '28', '38'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aditya', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Adrian', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ahmed', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aidan', '32', '34'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aiden', '96', '7'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alan', '37', '30'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alex', '52', '18'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexander', '47', '22'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ali', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Allen', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Alvin', '43', '25'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Andrew', '70', '14'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Andy', '43', '25'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Anson', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Anthony', '46', '23'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Arjun', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Armaan', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Aryan', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Austin', '64', '16'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayaan', '31', '35'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayden', '24', '42'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Benjamin', '39', '28'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Benson', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Brandon', '29', '37'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Brian', '40', '27'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Bryan', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Caleb', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Calvin', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Carson', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Charles', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Christian', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Christopher', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Cody', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Connor', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Daniel', '84', '10'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Danny', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Darren', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'David', '28', '38'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Derek', '32', '34'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Devin', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Dylan', '53', '17'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Eason', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Edison', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Edwin', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Elijah', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Elvis', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Eric', '85', '9'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ethan', '177', '1'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Evan', '51', '19'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Farhan', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Felix', '22', '44'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Gabriel', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Gavin', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'George', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Harrison', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Hayden', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Henry', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ian', '22', '44'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ibrahim', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Isaac', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ishaan', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ivan', '66', '15'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jack', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jackson', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jacky', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jacob', '49', '21'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jaden', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jake', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'James', '36', '31'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jason', '98', '6'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jay', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jayden', '173', '2'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jeffrey', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jeremy', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jerry', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jia', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'John', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Johnny', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jonathan', '50', '20'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Jordan', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Joseph', '21', '45'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Joshua', '36', '31'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Julian', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Justin', '110', '4'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kai', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kevin', '92', '8'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kingsley', '36', '31'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Kyle', '45', '24'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Lawrence', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Leo', '40', '27'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Leon', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Liam', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Logan', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Louis', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Lucas', '103', '5'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Luke', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Marcus', '20', '46'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Martin', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mason', '34', '33'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Matthew', '74', '12'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Max', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Michael', '38', '29'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Miles', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mohamed', '13', '53'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mohammad', '46', '23'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Mohammed', '41', '26'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Muhammad', '76', '11'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nathan', '41', '26'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nathaniel', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nelson', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Nicholas', '26', '40'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Noah', '19', '47'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Oliver', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Oscar', '25', '41'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Owen', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Patrick', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Peter', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Rayan', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Raymond', '27', '39'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Richard', '26', '40'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ricky', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Rohan', '15', '51'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Ryan', '150', '3'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Samuel', '30', '36'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Sean', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Sebastian', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Shawn', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Simon', '14', '52'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Stanley', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Steven', '35', '32'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Syed', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Tenzin', '24', '42'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Terry', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Thomas', '18', '48'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Timothy', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Tony', '16', '50'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Travis', '11', '55'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Tyler', '31', '35'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Victor', '17', '49'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Vincent', '71', '13'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'William', '66', '15'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Wilson', '23', '43'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Xavier', '10', '56'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Zachary', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Zain', '12', '54'],\n",
    " ['2011', 'MALE', 'ASIAN AND PACIFIC ISLANDER', 'Zayan', '10', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aaron', '53', '28'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Abdoul', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Abdoulaye', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Adam', '28', '46'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aden', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Adonis', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Adrian', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ahmed', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aidan', '20', '54'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Aiden', '167', '2'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Alexander', '39', '35'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ali', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Alijah', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Alvin', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amadou', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', \"Amar'E\", '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amare', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amari', '57', '27'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Amir', '89', '12'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Andre', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Andrew', '36', '38'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Angel', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Anthony', '66', '22'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Antonio', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ashton', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Austin', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Avery', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ayden', '88', '13'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Benjamin', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Blake', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Brandon', '57', '27'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Brian', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Bryan', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Bryce', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Bryson', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Caleb', '44', '31'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Cameron', '47', '29'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Carmelo', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Carter', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Cayden', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chad', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chance', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Charles', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chase', '41', '33'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Chris', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Christian', '91', '11'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Christopher', '80', '19'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Cody', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Corey', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Daniel', '81', '18'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Darius', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Darren', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'David', '84', '15'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Derrick', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Devin', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Devon', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Dominic', '17', '57'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Donovan', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Dwayne', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Dylan', '42', '32'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Edward', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Eli', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Elias', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Elijah', '156', '3'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Emmanuel', '46', '30'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Eric', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ethan', '124', '6'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Evan', '26', '48'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Gabriel', '36', '38'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Gavin', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'George', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Giovanni', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Hassan', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Hayden', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Hunter', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ian', '21', '53'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ibrahim', '25', '49'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ibrahima', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Isaac', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Isaiah', '105', '8'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ishmael', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Isiah', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jace', '21', '53'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jackson', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jacob', '82', '17'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaden', '75', '20'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaheim', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jahmir', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaiden', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jalen', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jamal', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jamel', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'James', '33', '41'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jamir', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jared', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jasiah', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jason', '44', '31'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jayden', '184', '1'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jaylen', '38', '36'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jayson', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jelani', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jeremiah', '153', '4'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jeremy', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jermaine', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jesse', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Joel', '37', '37'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'John', '34', '40'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jonathan', '53', '28'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Jordan', '53', '28'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Joseph', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Joshua', '128', '5'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Josiah', '121', '7'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Julian', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Justice', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Justin', '85', '14'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kaden', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kai', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kaiden', '28', '46'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kaleb', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kamari', '17', '57'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kameron', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kayden', '37', '37'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Keith', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kenneth', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kevin', '28', '46'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Khalil', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'King', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kiyan', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kyle', '40', '34'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Kymani', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Lamar', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Landon', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Levi', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Liam', '61', '24'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Logan', '25', '49'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Lucas', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Makai', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Malachi', '72', '21'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Malcolm', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Malik', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mamadou', '27', '47'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Marc', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Marcus', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Marquis', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mason', '83', '16'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Matthew', '59', '25'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Maurice', '16', '58'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mekhi', '33', '41'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Messiah', '23', '51'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Micah', '29', '45'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Michael', '93', '10'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Miles', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mohamed', '37', '37'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Mohammed', '23', '51'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Moussa', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Myles', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nana', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nasir', '33', '41'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nathan', '32', '42'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nathaniel', '64', '23'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nehemiah', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nicholas', '31', '43'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nicolas', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Nigel', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Noah', '91', '11'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Omar', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Omari', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ousmane', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Patrick', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Preston', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Prince', '32', '42'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Quincy', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ricardo', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Richard', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Robert', '29', '45'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Rodney', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Ryan', '41', '33'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Samuel', '30', '44'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sean', '29', '45'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sebastian', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sekou', '15', '59'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Seth', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Shawn', '18', '56'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Sincere', '24', '50'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Stephen', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Steven', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Terrell', '10', '64'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Terrence', '11', '63'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Timothy', '14', '60'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Travis', '17', '57'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Tristan', '58', '26'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Troy', '13', '61'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Tyler', '101', '9'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Victor', '12', '62'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'William', '27', '47'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Xavier', '58', '26'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zachary', '22', '52'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zaire', '19', '55'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zion', '35', '39'],\n",
    " ['2011', 'MALE', 'BLACK NON HISPANIC', 'Zyaire', '14', '60'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aaron', '102', '33'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Abdiel', '12', '92'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Abel', '14', '90'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Abraham', '22', '82'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adam', '32', '72'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aden', '13', '91'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adonis', '16', '88'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adrian', '157', '19'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Adriel', '34', '70'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aidan', '36', '68'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aiden', '188', '12'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alan', '38', '66'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Albert', '13', '91'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alberto', '15', '89'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Aldo', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alejandro', '68', '47'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alex', '68', '47'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alexander', '219', '9'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alexis', '55', '54'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alfredo', '15', '89'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Allan', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Allen', '22', '82'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Alvin', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Amir', '21', '83'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Anderson', '33', '71'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andre', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andres', '51', '57'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andrew', '125', '25'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Andy', '37', '67'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Angel', '253', '5'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Angelo', '28', '76'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Anthony', '207', '10'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Antonio', '32', '72'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Ariel', '26', '78'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Armando', '11', '93'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Armani', '20', '84'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Arturo', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Ashton', '17', '87'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Austin', '15', '89'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Avery', '13', '91'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Axel', '69', '46'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Ayden', '104', '31'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Benjamin', '51', '57'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bradley', '24', '80'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brandon', '184', '13'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brayan', '23', '81'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brayden', '16', '88'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Brian', '59', '52'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bryan', '99', '35'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bryant', '17', '87'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Bryce', '10', '94'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Byron', '19', '85'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Caleb', '42', '64'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Cameron', '19', '85'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Carlos', '109', '30'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Carmelo', '16', '88'],\n",
    " ['2011', 'MALE', 'HISPANIC', 'Cesar', '29', '75'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Abigail', '24', '24'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ada', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aisha', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aiza', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aleena', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexa', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alexandra', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alice', '27', '21'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alina', '25', '23'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alisha', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aliyah', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Allison', '17', '31'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Alyssa', '26', '22'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amanda', '22', '26'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amber', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amelia', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Amy', '24', '24'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angel', '22', '26'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angela', '47', '10'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angelina', '26', '22'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Angie', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anika', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Anna', '30', '19'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Annabelle', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Annie', '20', '28'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Aria', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ariana', '15', '33'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arianna', '11', '37'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Arya', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ashley', '52', '8'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Audrey', '19', '29'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ava', '23', '25'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Ayesha', '17', '31'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Bella', '16', '32'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Bonnie', '10', '38'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Brianna', '13', '35'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Catherine', '27', '21'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cecilia', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Charlotte', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Chloe', '106', '2'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christina', '20', '28'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christine', '11', '37'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Christy', '12', '36'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cindy', '14', '34'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Claire', '27', '21'],\n",
    " ['2011', 'FEMALE', 'ASIAN AND PACIFIC ISLANDER', 'Cynthia', '14', '34']]\n",
    "\n",
    "\n",
    "# Create the empty set: baby_names_2011\n",
    "f_name_2011 = []\n",
    "\n",
    "for row in records:\n",
    "    if row[0] == \"2011\" and row[1] == \"FEMALE\":\n",
    "        f_name_2011.append(row[3])\n",
    "#print(f_name_2011)\n",
    "# ['Geraldine', 'Gia', 'Gianna', \n",
    "\n",
    "    \n",
    "female_baby_names_2011_1 = {i: f_name_2011[i] for i in range(len(f_name_2011))}\n",
    "#print(female_baby_names_2011_1)\n",
    "# {0: 'Geraldine', 1: 'Gia', 2: 'Gianna',       # dict() from list f_name_2011\n",
    "\n",
    "\n",
    "# Create an empty dictionary: names_by_rank\n",
    "names_by_rank = {}\n",
    "\n",
    "# Loop over the girl names\n",
    "for rank, names in female_baby_names_2011_1.items():\n",
    "    # Add each name to the names_by_rank dictionary using rank as the key\n",
    "    names_by_rank[rank] = names\n",
    "    \n",
    "#print(names_by_rank)\n",
    "# {0: 'Geraldine', 1: 'Gia', 2: 'Gianna',       # dict() from list f_name_2011\n",
    "\n",
    "# Sort the names_by_rank dict by rank in descending order and slice the first 10 items\n",
    "for i in range(0, 5):\n",
    "    # Print each item\n",
    "    print(names_by_rank[i])\n",
    "    \n",
    "print(len(names_by_rank))\n",
    "    \n",
    "\n",
    "\n",
    "# ******************************************************************************* Problem ***** #\n",
    "names_count = {}\n",
    "\n",
    "for i in f_name_2011:\n",
    "#    names_count[i] = 1\n",
    "    if i in names_count:\n",
    "        names_count[i] += 1\n",
    "    else:\n",
    "        names_count[i] = 1\n",
    "    \n",
    "print(len(names_count))\n",
    "print(names_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbdd836c-8d65-4a74-aadc-c939f31dba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gia': 2, 'Gianna': 2, 'Grace': 2, 'Hailey': 2, 'Hannah': 2, 'Hazel': 2, 'Iris': 2, 'Isabel': 2, 'Isabella': 2, 'Isabelle': 2, 'Izabella': 2, 'Jacqueline': 2, 'Jasmine': 2, 'Jessica': 2, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Kaitlyn': 2, 'Kate': 2, 'Katherine': 2, 'Kayla': 2, 'Kaylee': 2, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'London': 2, 'Lucia': 2, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Maya': 2, 'Melanie': 2, 'Mia': 2, 'Michelle': 2, 'Mikayla': 2, 'Mila': 2, 'Miriam': 2, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nicole': 2, 'Nina': 2, 'Olivia': 2, 'Penelope': 2, 'Rachel': 2, 'Rebecca': 2, 'Riley': 2, 'Rose': 2, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Sara': 2, 'Sarah': 2, 'Sasha': 2, 'Savannah': 2, 'Scarlett': 2, 'Sienna': 2, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stella': 2, 'Taylor': 2, 'Valentina': 2, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Alexa': 2, 'Alexandra': 2, 'Alice': 2, 'Alina': 2, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amy': 2, 'Angelina': 2, 'Anna': 2, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ashley': 2, 'Audrey': 2, 'Ava': 2, 'Bella': 2, 'Brianna': 2, 'Catherine': 2, 'Cecilia': 2, 'Charlotte': 2, 'Chloe': 2, 'Christina': 2, 'Claire': 2}\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "# Create the empty set: baby_names_2011\n",
    "f_name_2011 = []\n",
    "\n",
    "for row in records:\n",
    "    if row[0] == \"2011\" and row[1] == \"FEMALE\":\n",
    "        f_name_2011.append(row[3])\n",
    "        \n",
    "        \n",
    "# ******************************************************************************* Problem ***** #\n",
    "new_dict = {}\n",
    "n = len(f_name_2011)\n",
    "count = [1 for i in range(n)]\n",
    "#print(count)\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        #count=1\n",
    "        if f_name_2011[i] == f_name_2011[j]: \n",
    "            count[i] = count[i] + 1  \n",
    "            new_dict[f_name_2011[i]]=count[i]\n",
    "            #if f_name_2011[i] != f_name_2011[j]:\n",
    "            #new_dict[f_name_2011[i]]=count[i]\n",
    "        #new_dict[f_name_2011[i]]=count[i]\n",
    "print(new_dict)\n",
    "print(len(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36494d81-7323-4286-a81a-49c882602513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Geraldine': 1, 'Gia': 2, 'Gianna': 2, 'Giselle': 1, 'Grace': 2, 'Guadalupe': 1, 'Hailey': 2, 'Haley': 1, 'Hannah': 2, 'Haylee': 1, 'Hayley': 1, 'Hazel': 2, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Iris': 2, 'Isabel': 2, 'Isabela': 1, 'Isabella': 2, 'Isabelle': 2, 'Isis': 1, 'Itzel': 1, 'Izabella': 2, 'Jacqueline': 2, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jasmine': 2, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jessica': 2, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kaitlyn': 2, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Kate': 2, 'Katelyn': 1, 'Katelynn': 1, 'Katherine': 2, 'Katie': 1, 'Kayla': 2, 'Kaylee': 2, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'Lindsay': 1, 'Lizbeth': 1, 'London': 2, 'Lucia': 2, 'Luna': 1, 'Luz': 1, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Maya': 2, 'Megan': 1, 'Melanie': 2, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Mia': 2, 'Miah': 1, 'Michelle': 2, 'Mikaela': 1, 'Mikayla': 2, 'Mila': 2, 'Miley': 1, 'Miranda': 1, 'Miriam': 2, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Nicole': 2, 'Nina': 2, 'Noemi': 1, 'Nyla': 1, 'Olivia': 2, 'Paola': 1, 'Penelope': 2, 'Perla': 1, 'Rachel': 2, 'Raquel': 1, 'Rebecca': 2, 'Rihanna': 1, 'Riley': 2, 'Rosa': 1, 'Rose': 2, 'Roselyn': 1, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Samara': 1, 'Sara': 2, 'Sarah': 2, 'Sarai': 1, 'Sariah': 1, 'Sasha': 2, 'Savanna': 1, 'Savannah': 2, 'Scarlet': 1, 'Scarlett': 2, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Sienna': 2, 'Skyla': 1, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stacy': 1, 'Stella': 2, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Taylor': 2, 'Tiana': 1, 'Tiffany': 1, 'Valentina': 2, 'Valeria': 1, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexa': 2, 'Alexandra': 2, 'Alexis': 1, 'Alice': 2, 'Alicia': 1, 'Alina': 2, 'Alisa': 1, 'Aliza': 1, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Amy': 2, 'Anastasia': 1, 'Angelica': 1, 'Angelina': 2, 'Anna': 2, 'Annabel': 1, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Ashley': 2, 'Atara': 1, 'Aubrey': 1, 'Audrey': 2, 'Autumn': 1, 'Ava': 2, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bella': 2, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Brianna': 2, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Catherine': 2, 'Cecilia': 2, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Charlotte': 2, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1}\n"
     ]
    }
   ],
   "source": [
    "counter = {}\n",
    "for name in f_name_2011:\n",
    "    if name not in counter:\n",
    "        counter[name] = 0\n",
    "    counter[name] += 1\n",
    "    \n",
    "print(counter)\n",
    "\n",
    "# ************************************************************************************************** #\n",
    "# ************************************************************************************************** #\n",
    "# ************************************************************************************************** #\n",
    "\n",
    "#>>> word = \"mississippi\"\n",
    "#>>> counter = {}\n",
    "#\n",
    "#>>> for letter in word:\n",
    "#...     if letter not in counter:\n",
    "#...         counter[letter] = 0\n",
    "#...     counter[letter] += 1\n",
    "#>>> counter\n",
    "#{'m': 1, 'i': 4, 's': 4, 'p': 2}\n",
    "\n",
    "\n",
    "#>>> word = \"mississippi\"\n",
    "#>>> counter = {}\n",
    "#\n",
    "#>>> for letter in word:\n",
    "#...     counter[letter] = counter.get(letter, 0) + 1\n",
    "#...\n",
    "#\n",
    "#>>> counter\n",
    "#{'m': 1, 'i': 4, 's': 4, 'p': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b752d411-e3fa-4fbb-8de3-8dea54f16a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Geraldine': 1, 'Gia': 2, 'Gianna': 2, 'Giselle': 1, 'Grace': 2, 'Guadalupe': 1, 'Hailey': 2, 'Haley': 1, 'Hannah': 2, 'Haylee': 1, 'Hayley': 1, 'Hazel': 2, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Iris': 2, 'Isabel': 2, 'Isabela': 1, 'Isabella': 2, 'Isabelle': 2, 'Isis': 1, 'Itzel': 1, 'Izabella': 2, 'Jacqueline': 2, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jasmine': 2, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jessica': 2, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kaitlyn': 2, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Kate': 2, 'Katelyn': 1, 'Katelynn': 1, 'Katherine': 2, 'Katie': 1, 'Kayla': 2, 'Kaylee': 2, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'Lindsay': 1, 'Lizbeth': 1, 'London': 2, 'Lucia': 2, 'Luna': 1, 'Luz': 1, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Maya': 2, 'Megan': 1, 'Melanie': 2, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Mia': 2, 'Miah': 1, 'Michelle': 2, 'Mikaela': 1, 'Mikayla': 2, 'Mila': 2, 'Miley': 1, 'Miranda': 1, 'Miriam': 2, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Nicole': 2, 'Nina': 2, 'Noemi': 1, 'Nyla': 1, 'Olivia': 2, 'Paola': 1, 'Penelope': 2, 'Perla': 1, 'Rachel': 2, 'Raquel': 1, 'Rebecca': 2, 'Rihanna': 1, 'Riley': 2, 'Rosa': 1, 'Rose': 2, 'Roselyn': 1, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Samara': 1, 'Sara': 2, 'Sarah': 2, 'Sarai': 1, 'Sariah': 1, 'Sasha': 2, 'Savanna': 1, 'Savannah': 2, 'Scarlet': 1, 'Scarlett': 2, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Sienna': 2, 'Skyla': 1, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stacy': 1, 'Stella': 2, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Taylor': 2, 'Tiana': 1, 'Tiffany': 1, 'Valentina': 2, 'Valeria': 1, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexa': 2, 'Alexandra': 2, 'Alexis': 1, 'Alice': 2, 'Alicia': 1, 'Alina': 2, 'Alisa': 1, 'Aliza': 1, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Amy': 2, 'Anastasia': 1, 'Angelica': 1, 'Angelina': 2, 'Anna': 2, 'Annabel': 1, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Ashley': 2, 'Atara': 1, 'Aubrey': 1, 'Audrey': 2, 'Autumn': 1, 'Ava': 2, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bella': 2, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Brianna': 2, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Catherine': 2, 'Cecilia': 2, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Charlotte': 2, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1}\n"
     ]
    }
   ],
   "source": [
    "n_counter = {}\n",
    "\n",
    "# ************************************************************************************************* #\n",
    "# ************************************************************************************************* #\n",
    "# Beautiful, after understanding of previous RealPython code, I write this code successful in 1 shot\n",
    "\n",
    "\n",
    "for name in f_name_2011:\n",
    "    if name in n_counter:\n",
    "        n_counter[name] += 1\n",
    "    else:\n",
    "        n_counter[name] = 1\n",
    "        \n",
    "print(n_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36ecbba-3be7-4566-8882-10a0c3e83595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'Geraldine': 1, 'Gia': 2, 'Gianna': 2, 'Giselle': 1, 'Grace': 2, 'Guadalupe': 1, 'Hailey': 2, 'Haley': 1, 'Hannah': 2, 'Haylee': 1, 'Hayley': 1, 'Hazel': 2, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Iris': 2, 'Isabel': 2, 'Isabela': 1, 'Isabella': 2, 'Isabelle': 2, 'Isis': 1, 'Itzel': 1, 'Izabella': 2, 'Jacqueline': 2, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jasmine': 2, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jessica': 2, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kaitlyn': 2, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Kate': 2, 'Katelyn': 1, 'Katelynn': 1, 'Katherine': 2, 'Katie': 1, 'Kayla': 2, 'Kaylee': 2, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'Lindsay': 1, 'Lizbeth': 1, 'London': 2, 'Lucia': 2, 'Luna': 1, 'Luz': 1, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Maya': 2, 'Megan': 1, 'Melanie': 2, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Mia': 2, 'Miah': 1, 'Michelle': 2, 'Mikaela': 1, 'Mikayla': 2, 'Mila': 2, 'Miley': 1, 'Miranda': 1, 'Miriam': 2, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Nicole': 2, 'Nina': 2, 'Noemi': 1, 'Nyla': 1, 'Olivia': 2, 'Paola': 1, 'Penelope': 2, 'Perla': 1, 'Rachel': 2, 'Raquel': 1, 'Rebecca': 2, 'Rihanna': 1, 'Riley': 2, 'Rosa': 1, 'Rose': 2, 'Roselyn': 1, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Samara': 1, 'Sara': 2, 'Sarah': 2, 'Sarai': 1, 'Sariah': 1, 'Sasha': 2, 'Savanna': 1, 'Savannah': 2, 'Scarlet': 1, 'Scarlett': 2, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Sienna': 2, 'Skyla': 1, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stacy': 1, 'Stella': 2, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Taylor': 2, 'Tiana': 1, 'Tiffany': 1, 'Valentina': 2, 'Valeria': 1, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexa': 2, 'Alexandra': 2, 'Alexis': 1, 'Alice': 2, 'Alicia': 1, 'Alina': 2, 'Alisa': 1, 'Aliza': 1, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Amy': 2, 'Anastasia': 1, 'Angelica': 1, 'Angelina': 2, 'Anna': 2, 'Annabel': 1, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Ashley': 2, 'Atara': 1, 'Aubrey': 1, 'Audrey': 2, 'Autumn': 1, 'Ava': 2, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bella': 2, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Brianna': 2, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Catherine': 2, 'Cecilia': 2, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Charlotte': 2, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create the empty set: baby_names_2011\n",
    "f_name_2011 = []\n",
    "\n",
    "for row in records:\n",
    "    if row[0] == \"2011\" and row[1] == \"FEMALE\":\n",
    "        f_name_2011.append(row[3])\n",
    "        \n",
    "\n",
    "# ******************************************************************************************** #\n",
    "# ******************************************************************************************** #\n",
    "my_dict = {i:f_name_2011.count(i) for i in f_name_2011}\n",
    "\n",
    "print(my_dict.get('Alice'))\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8c7c4-2520-4b85-9561-6c1edb076709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary: names_by_rank\n",
    "names_by_rank = {}\n",
    "\n",
    "# Loop over the girl names\n",
    "for rank, name in female_baby_names_2012.items():\n",
    "    # Add each name to the names_by_rank dictionary using rank as the key\n",
    "    names_by_rank[rank] = name\n",
    "    \n",
    "# Sort the names_by_rank dict by rank in descending order and slice the first 10 items\n",
    "for rank in sorted(names_by_rank, reverse=True)[:10]:\n",
    "    # Print each item\n",
    "    print(names_by_rank[rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1e02c512-9db5-4915-9fa5-aa23e46efe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (3, 4), (5, 6)]\n"
     ]
    }
   ],
   "source": [
    "lst = [[1, 2], [3, 4], [5, 6]]\n",
    "tuples = [tuple(x) for x in lst]\n",
    "\n",
    "print(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89442f3-d117-4e92-a935-37a62982930c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cynthia\n",
      "Claire\n",
      "Cindy\n",
      "Christy\n",
      "Christine\n",
      "Christina\n",
      "Chloe\n",
      "Charlotte\n",
      "Cecilia\n",
      "Catherine\n"
     ]
    }
   ],
   "source": [
    "# *********************************************************************************************** #\n",
    "# *********************************************************************************************** #\n",
    "# *********************************************************************************************** #\n",
    "\n",
    "\n",
    "# Create the empty set: baby_names_2011\n",
    "f_name_2011 = []\n",
    "for row in records:\n",
    "    if row[0] == \"2011\" and row[1] == \"FEMALE\":\n",
    "        f_name_2011.append(row[3])\n",
    "\n",
    "f_name_2011_idx = [i for i in range(len(records))]\n",
    "\n",
    "pairs = list(zip(f_name_2011_idx, f_name_2011))\n",
    "#print(pairs)\n",
    "# [(0, 'Geraldine'), (1, 'Gia'), (2, 'Gianna'),\n",
    "\n",
    "\n",
    "f_name_2011_n = {}\n",
    "for idx, names in enumerate(f_name_2011):\n",
    "    f_name_2011_n[idx] = names\n",
    "#print(f_name_2011_n)\n",
    "# {0: 'Geraldine', 1: 'Gia', 2: 'Gianna',\n",
    "\n",
    "\n",
    "# Creating and looping through dictionaries\n",
    "# ***************************************************************************************** #\n",
    "# Create an empty dictionary: names_by_rank\n",
    "names_by_rank = {}\n",
    "\n",
    "# Loop over the girl names      # missing .items(), TypeError: cannot unpack non-iterable int object\n",
    "for rank, names in pairs:\n",
    "    # Add each name to the names_by_rank dictionary using rank as the key\n",
    "    names_by_rank[rank] = names\n",
    "    \n",
    "# Sort the names_by_rank dict by rank in descending order and slice the first 10 items\n",
    "for rank in sorted(names_by_rank, reverse=True)[:10]:\n",
    "    # Print each item\n",
    "    print(names_by_rank[rank])\n",
    "# ***************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53296a51-91f7-490c-9b49-03705184a970",
   "metadata": {},
   "source": [
    "## Safely finding by key\n",
    "\n",
    "As demonstrated in the video, if you attempt to access a key that isn't present in a dictionary, you'll get a KeyError. One option to handle this type of error is to use a try: except: block. You can learn more about error handling in Python Data Science Toolbox (Part 1), specifically in this video.\n",
    "\n",
    "Python provides a faster, more versatile tool to help with this problem in the form of the .get() method. The .get() method allows you to supply the name of a key, and optionally, what you'd like to have returned if the key is not found.\n",
    "\n",
    "You'll be using same names dictionary from the previous exercise and will gain practice using the .get() method.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Safely print rank 7 from the names dictionary.\n",
    "    Safely print the type of rank 100 from the names dictionary.\n",
    "    Safely print rank 105 from the names dictionary or 'Not Found' if 105 is not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e37f403-5d3d-4327-a902-7a53a893983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haley\n",
      "<class 'str'>\n",
      "Luna\n"
     ]
    }
   ],
   "source": [
    "names = names_by_rank\n",
    "\n",
    "\n",
    "# Safely print rank 7 from the names dictionary\n",
    "print(names.get(7))\n",
    "\n",
    "# Safely print the type of rank 100 from the names dictionary\n",
    "type(names.get(100))\n",
    "print(type(names.get(100)))\n",
    "\n",
    "# Safely print rank 105 from the names dictionary or 'Not Found'\n",
    "print(names.get(105, \"Not Found\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c2e4e-4e95-4b1a-bce6-1d2ff9b77336",
   "metadata": {},
   "source": [
    "## Dealing with nested data\n",
    "\n",
    "A dictionary can contain another dictionary as the value of a key, and this is a very common way to deal with repeating data structures such as yearly, monthly or weekly data. All the same rules apply when creating or accessing the dictionary.\n",
    "\n",
    "For example, if you had a dictionary that had a ranking of my cookie consumption by year and type of cookie. It might look like cookies = {'2017': {'chocolate chip': 483, 'peanut butter': 115}, '2016': {'chocolate chip': 9513, 'peanut butter': 6792}}. I could access how many chocolate chip cookies I ate in 2016 using cookies['2016']['chocolate chip'].\n",
    "\n",
    "When exploring a new dictionary, it can be helpful to use the .keys() method to get an idea of what data might be available within the dictionary. You can also iterate over a dictionary and it will return each key in the dictionary for you to use inside the loop. Here, a dictionary called boy_names has been loaded into your workspace. It consists of all male names in 2013 and 2014.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Print the keys of the boy_names dictionary.\n",
    "    Print the keys of the boy_names dictionary for the year 2013.\n",
    "    Loop over the boy_names dictionary.\n",
    "        Inside the loop, safely print the year and the third ranked name. Print 'Unknown' if the third ranked name is not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4135caba-ea63-4a54-b477-33c679877ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'JOSIAH', 1: 'ETHAN', 2: 'David', 3: 'Jayden', 4: 'MASON', 5: 'RYAN', 6: 'CHRISTIAN', 7: 'ISAIAH', 8: 'JAYDEN', 9: 'Michael', 10: 'NOAH', 11: 'SAMUEL', 12: 'SEBASTIAN', 13: 'Noah', 14: 'Dylan', 15: 'LUCAS', 16: 'JOSHUA', 17: 'ANGEL', 18: 'Jacob', 19: 'Matthew', 20: 'Josiah', 21: 'JACOB', 22: 'Muhammad', 23: 'ALEXANDER', 24: 'Jason', 25: 'Ethan', 26: 'DANIEL', 27: 'Joseph', 28: 'AIDEN', 29: 'Moshe', 30: 'Jeremiah', 31: 'William', 32: 'Alexander', 33: 'Sebastian', 34: 'ERIC', 35: 'MOSHE', 36: 'Jack', 37: 'Eric', 38: 'MUHAMMAD', 39: 'Lucas', 40: 'BENJAMIN', 41: 'Aiden', 42: 'Ryan', 43: 'Liam', 44: 'JASON', 45: 'KEVIN', 46: 'Elijah', 47: 'Angel', 48: 'JAMES', 49: 'Daniel', 50: 'Samuel', 51: 'Amir', 52: 'Mason', 53: 'Joshua', 54: 'ANTHONY', 55: 'JOSEPH', 56: 'Benjamin', 57: 'JUSTIN', 58: 'JEREMIAH', 59: 'MATTHEW', 60: 'Carter', 61: 'James', 62: 'TYLER', 63: 'DAVID', 64: 'JACK', 65: 'ELIJAH', 66: 'MICHAEL', 67: 'CHRISTOPHER'}\n",
      "\n",
      "[dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67])]\n",
      "dict_keys(['John'])\n",
      "2012 Unknow\n",
      "2013 15\n"
     ]
    }
   ],
   "source": [
    "cookies = {'2017': {'chocolate chip': 483, 'peanut butter': 115}, \n",
    "           '2016': {'chocolate chip': 9513, 'peanut butter': 6792}}\n",
    "\n",
    "cookies['2016']['chocolate chip']\n",
    "\n",
    "\n",
    "boy_names = ['JOSIAH', 'ETHAN', 'David', 'Jayden', 'MASON', 'RYAN', 'CHRISTIAN', 'ISAIAH', \n",
    " 'JAYDEN', 'Michael', 'NOAH', 'SAMUEL', 'SEBASTIAN', 'Noah', 'Dylan', 'LUCAS', 'JOSHUA', \n",
    " 'ANGEL', 'Jacob', 'Matthew', 'Josiah', 'JACOB', 'Muhammad', 'ALEXANDER', 'Jason', 'Ethan', \n",
    " 'DANIEL', 'Joseph', 'AIDEN', 'Moshe', 'Jeremiah', 'William', 'Alexander', 'Sebastian', \n",
    " 'ERIC', 'MOSHE', 'Jack', 'Eric', 'MUHAMMAD', 'Lucas', 'BENJAMIN', 'Aiden', 'Ryan', 'Liam', \n",
    " 'JASON', 'KEVIN', 'Elijah', 'Angel', 'JAMES', 'Daniel', 'Samuel', 'Amir', 'Mason', 'Joshua', \n",
    " 'ANTHONY', 'JOSEPH', 'Benjamin', 'JUSTIN', 'JEREMIAH', 'MATTHEW', 'Carter', 'James', 'TYLER', \n",
    " 'DAVID', 'JACK', 'ELIJAH', 'MICHAEL', 'CHRISTOPHER']\n",
    "\n",
    "# ************************************************************************************** #\n",
    "# ************************************************************************************** #\n",
    "n_boy_names = {i: boy_names[i] for i in range(len(boy_names))}\n",
    "print(n_boy_names)\n",
    "print()\n",
    "\n",
    "# Print a list of keys from the boy_names dictionary\n",
    "print([n_boy_names.keys()])\n",
    "\n",
    "\n",
    "# ************************************************************************************** #\n",
    "# So this should be a nested dictionary, maybe looks like this {year: {name: jhhu}}\n",
    "n_boy_names = {2012: {\"Jhu\": 13}, \n",
    "              2013: {\"John\": 15}}\n",
    "\n",
    "# Print a list of keys from the boy_names dictionary for the year 2013\n",
    "print(n_boy_names[2013].keys())\n",
    "\n",
    "# Loop over the dictionary\n",
    "for year in n_boy_names:\n",
    "    # Safely print the year and the third ranked name or 'Unknown'\n",
    "    print(year, n_boy_names[year].get(\"John\", \"Unknow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087cecd-c69d-4fe6-b0d1-cb495a5d2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a list of keys from the boy_names dictionary\n",
    "print(boy_names.keys())\n",
    "\n",
    "# Print a list of keys from the boy_names dictionary for the year 2013\n",
    "print(boy_names[2013].keys())\n",
    "\n",
    "# Loop over the dictionary\n",
    "for year in boy_names:\n",
    "    # Safely print the year and the third ranked name or 'Unknown'\n",
    "    print(year, boy_names[year].get(3, 'Unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e3af5-b4d8-467a-abee-5a619303de35",
   "metadata": {},
   "source": [
    "## Altering dictionaries\n",
    "\n",
    "\n",
    "# *****************************************************************************************************\n",
    "**In the prior video we learned that dictionaries are mutable, so we can alter them in a number of ways, Let start by adding data to them which is you, as a data scientist, will need to do all the time.\n",
    "\n",
    "  **Adding and extanding dictionaries\n",
    "    __Assignment to add a new key/value to a dictionary__\n",
    "#    __.update() method to update a dictionary from another dictionary, tuples or keywords__\n",
    "\n",
    "\n",
    "# you can add data to a dictionary just by using a new key as index and assigning it a value.  \n",
    "Its also supply a dictionary, list of tuples or a set of keywords arguments to the .update() method to add values into a dictionary. \n",
    "\n",
    "   ## We can also create a list of tuples, and supply them to the update() method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "013e447c-e5a2-4869-a050-98e1d026fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336', 'Africa Afericam History': '(313) 767-2234'}\n",
      "\n",
      "[('A J ARTS LTD', '(718) 763-5473'), ('Doug Meyer Fine Art', '(718) 375-8006'), ('Portrait Gallery', '(718) 377-8762')]\n",
      "\n",
      "{'1234': {'P J ARTS LTD': '(738) 364-6193', 'A J ARTS LTD': '(718) 763-5473', 'Doug Meyer Fine Art': '(718) 375-8006', 'Portrait Gallery': '(718) 377-8762'}}\n"
     ]
    }
   ],
   "source": [
    "art_gallery = {\"Nyabinghi Africian Gift Shop\": \"(212) 566-3336\"}\n",
    "\n",
    "art_gallery[\"Africa Afericam History\"] = \"(313) 767-2234\"\n",
    "print(art_gallery)\n",
    "print()\n",
    "\n",
    "\n",
    "# We can also create a list of tuples, and supply them to the update() method\n",
    "# ***************************************************************************************** #\n",
    "# ***************************************************************************************** #\n",
    "cookies = {'2017': {'chocolate chip': 483, 'peanut butter': 115}, \n",
    "           '2016': {'chocolate chip': 9513, 'peanut butter': 6792}}\n",
    "\n",
    "\n",
    "galleries = [(\"A J ARTS LTD\", \"(718) 763-5473\"), \n",
    "            (\"Doug Meyer Fine Art\", \"(718) 375-8006\"), \n",
    "            (\"Portrait Gallery\", \"(718) 377-8762\")]\n",
    "\n",
    "n_galleries = {\"1234\": {\"P J ARTS LTD\": \"(738) 364-6193\"}}\n",
    "# ***************************************************************************************** #\n",
    "# ***************************************************************************************** #\n",
    "n_galleries[\"1234\"].update(galleries)\n",
    "print(galleries)\n",
    "print()\n",
    "print(n_galleries)\n",
    "\n",
    "art_gallery.update(galleries)\n",
    "#print(art_gallery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b105d6-efc0-4ef1-8242-7f0fe5c3a234",
   "metadata": {},
   "source": [
    "## Popping and deleting from dictionaries\n",
    "\n",
    "\n",
    "\n",
    "   __del insruction delete a key/value__\n",
    "#   __.pop() method safely remove key/value__\n",
    "# Python dictionary pop() method removes and returns the specified element from the dictionary.\n",
    "# *************************************************************************************************\n",
    "\n",
    "\n",
    "   **You can use the del python instructionon a dictionary key to remove data from a dictionary, will throw KeyError if key not exist. .pop() method will not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e39aaa6f-233f-419d-b7d4-55f233feed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336', 'Africa Afericam History': '(313) 767-2234'}\n",
      "[('A J ARTS LTD', '(718) 763-5473'), ('Doug Meyer Fine Art', '(718) 375-8006'), ('Portrait Gallery', '(718) 377-8762')]\n",
      "\n",
      "{'1234': {'P J ARTS LTD': '(738) 364-6193', 'A J ARTS LTD': '(718) 763-5473', 'Doug Meyer Fine Art': '(718) 375-8006', 'Portrait Gallery': '(718) 377-8762'}, '1111': {'Dog Wang Wang': '(111) 234-7890'}, '3838': {'Brave Africian Art': '(314) 586-3123'}}\n",
      "\n",
      "{'1111': {'Dog Wang Wang': '(111) 234-7890'}, '3838': {'Brave Africian Art': '(314) 586-3123'}}\n",
      "\n",
      "{'3838': {'Brave Africian Art': '(314) 586-3123'}}\n"
     ]
    }
   ],
   "source": [
    "art_gallery = {\"Nyabinghi Africian Gift Shop\": \"(212) 566-3336\"}\n",
    "\n",
    "art_gallery[\"Africa Afericam History\"] = \"(313) 767-2234\"\n",
    "print(art_gallery)\n",
    "\n",
    "\n",
    "# We can also create a list of tuples, and supply them to the update() method\n",
    "# ***************************************************************************************** #\n",
    "# ***************************************************************************************** #\n",
    "cookies = {'2017': {'chocolate chip': 483, 'peanut butter': 115}, \n",
    "           '2016': {'chocolate chip': 9513, 'peanut butter': 6792}}\n",
    "\n",
    "\n",
    "galleries = [(\"A J ARTS LTD\", \"(718) 763-5473\"), \n",
    "            (\"Doug Meyer Fine Art\", \"(718) 375-8006\"), \n",
    "            (\"Portrait Gallery\", \"(718) 377-8762\")]\n",
    "\n",
    "n_galleries = {\"1234\": {\"P J ARTS LTD\": \"(738) 364-6193\"}, \n",
    "              \"1111\": {\"Dog Wang Wang\": \"(111) 234-7890\"},\n",
    "              \"3838\": {\"Brave Africian Art\": \"(314) 586-3123\"}}\n",
    "# ***************************************************************************************** #\n",
    "# ***************************************************************************************** #\n",
    "n_galleries[\"1234\"].update(galleries)\n",
    "print(galleries)\n",
    "print()\n",
    "print(n_galleries)\n",
    "\n",
    "print()\n",
    "del n_galleries[\"1234\"]\n",
    "print(n_galleries)\n",
    "\n",
    "print()\n",
    "n_galleries.pop(\"1111\")\n",
    "print(n_galleries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e3916-5d0b-499e-ada1-f4b757c12a3f",
   "metadata": {},
   "source": [
    "## Adding and extending dictionaries\n",
    "\n",
    "If you have a dictionary and you want to add data to it, you can simply create a new key and assign the data you desire to it. It's important to remember that if it's a nested dictionary, then all the keys in the data path must exist, and each key in the path must be assigned individually.\n",
    "\n",
    "You can also use the .update() method to update a dictionary with keys and values from another dictionary, tuples or keyword arguments.\n",
    "\n",
    "Here, you'll combine several techniques used in prior exercises to setup your dictionary in a way that makes it easy to find the least popular baby name for each year.\n",
    "\n",
    "Your job is to add data for the year 2011 to your dictionary by assignment, 2012 by update, and then find the least popular baby name for each year.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Assign the names_2011 dictionary as the value to the 2011 key of the boy_names dictionary.\n",
    "    Update the 2012 key in the boy_names dictionary with the following data in a list of tuples: (1, 'Casey'), (2, 'Aiden').\n",
    "    Loop over the boy_names dictionary.\n",
    "        Inside the for loop, sort the data for each year of boy_names by descending rank and take the first result which will be the lowest ranked name.\n",
    "        Safely print the year and least popular name or 'Not Available' if it is not found. Take advantage of the .get() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df6d903c-5e90-4a0e-9b77-f77b8a7a621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2011': {1: 'Khloe', 2: 'Ahmed', 3: 'Byron', 4: 'Kira', 5: 'John'}, '2012': {1: 'Tiffany', 2: 'Jo', 3: 'Casey', 4: 'Aiden'}}\n",
      "5\n",
      "2011 John\n",
      "4\n",
      "2012 Aiden\n"
     ]
    }
   ],
   "source": [
    "# Assign the names_2011 dictionary as the value to the 2011 key of boy_names\n",
    "names_2011 = {1:'Khloe', 2:'Ahmed', 3:'Byron', 4:'Kira', 5:'John'}\n",
    "\n",
    "boy_names = {\"2011\": {1: \"John\"}, \"2012\": {1: \"Tiffany\", 2: \"Jo\"}}\n",
    "boy_names[\"2011\"] = names_2011\n",
    "\n",
    "# Update the 2012 key in the boy_names dictionary\n",
    "boy_names[\"2012\"].update([(3, \"Casey\"), (4, \"Aiden\")])\n",
    "print(boy_names)\n",
    "\n",
    "\n",
    "# Loop over the years in the boy_names dictionary \n",
    "for year in boy_names:\n",
    "    # Sort the data for each year by descending rank and get the lowest one\n",
    "    lowest_ranked =  sorted(boy_names[year], reverse=True)[0]\n",
    "    print(lowest_ranked)\n",
    "    # Safely print the year and the least popular name or 'Not Available'\n",
    "    print(year, boy_names[year].get(lowest_ranked, \"Not Available\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6f992-e960-4ab7-8d21-37f3c47f5d75",
   "metadata": {},
   "source": [
    "## Popping and deleting from dictionaries\n",
    "\n",
    "Often, you will want to remove keys and value from a dictionary. You can do so using the del Python instruction. It's important to remember that del will throw a KeyError if the key you are trying to delete does not exist. You can not use it with the .get() method to safely delete items; however, it can be used with try: catch:.\n",
    "\n",
    "# If you want to save that deleted data into another variable for further processing, the .pop() dictionary method will do just that. You can supply a default value for .pop() much like you did for .get() to safely deal with missing keys. It's also typical to use .pop() instead of del since it is a safe method.\n",
    "\n",
    "Here, you'll remove 2011 and 2015 to save them for later, and then delete 2012 from the dictionary.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Remove 2011 from female_names and store it as female_names_2011.\n",
    "    Safely remove 2015 from female_names with a empty dictionary as the default and store it as female_names_2015. To do this, pass in an empty dictionary {} as a second argument to .pop().\n",
    "    Delete 2012 from female_names.\n",
    "    Print female_names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71228f-b279-46d3-84fb-fd5eecce3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 2011 from female_names and store it: female_names_2011\n",
    "female_names_2011 = del female_names[\"2011\"]\n",
    "\n",
    "# Safely remove 2015 from female_names with an empty dictionary as the default: female_names_2015\n",
    "female_names_2015 = female_names.pop(\"2015\", {})\n",
    "\n",
    "# Delete 2012 from female_names\n",
    "del female_names[\"2012\"]\n",
    "\n",
    "# Print female_names\n",
    "print(female_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40362f40-83f4-4108-aac4-38bc4fc1dafa",
   "metadata": {},
   "source": [
    "## Pythonically using dictionaries\n",
    "\n",
    "   __.items() method returns an object we can iterate over__\n",
    "\n",
    "\n",
    "**So far, we've been working with dictionaries in a straight forward manner, but Python has more efficient ways to work with them.  We refer to these manners of interacting as being Pythonic. \n",
    "\n",
    "**Previously, we looped though dictionary keys then used the key to get the value wedesired. \n",
    "\n",
    "\n",
    "## Python provids .items() method which returns a dict items object, but we can iterate over it as a list of key/value tuples.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bdcdb3f-f08b-4de5-9d0e-f1226a0e4968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336'}\n",
      "\n",
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336', 'A J ARTS LTD': '(718) 763-5473', 'Doug Meyer Fine Art': '(718) 375-8006', 'Portrait Gallery': '(718) 377-8762'}\n",
      "\n",
      "Nyabinghi Africian Gift Shop\n",
      "(212) 566-3336\n",
      "A J ARTS LTD\n",
      "(718) 763-5473\n",
      "Doug Meyer Fine Art\n",
      "(718) 375-8006\n",
      "Portrait Gallery\n",
      "(718) 377-8762\n",
      "dict_items([('Nyabinghi Africian Gift Shop', '(212) 566-3336'), ('A J ARTS LTD', '(718) 763-5473'), ('Doug Meyer Fine Art', '(718) 375-8006'), ('Portrait Gallery', '(718) 377-8762')])\n"
     ]
    }
   ],
   "source": [
    "art_gallery = {\"Nyabinghi Africian Gift Shop\": \"(212) 566-3336\"}\n",
    "\n",
    "art_gallery[\"Africa Afericam History\"] = \"(313) 767-2234\"\n",
    "art_gallery.pop(\"Africa Afericam History\", {})\n",
    "print(art_gallery)\n",
    "print()\n",
    "\n",
    "# We can also create a list of tuples, and supply them to the update() method\n",
    "# ***************************************************************************************** #\n",
    "\n",
    "galleries = [(\"A J ARTS LTD\", \"(718) 763-5473\"), \n",
    "            (\"Doug Meyer Fine Art\", \"(718) 375-8006\"), \n",
    "            (\"Portrait Gallery\", \"(718) 377-8762\")]\n",
    "\n",
    "# ***************************************************************************************** #\n",
    "# ***************************************************************************************** #\n",
    "art_gallery.update(galleries)\n",
    "print(art_gallery)\n",
    "print()\n",
    "\n",
    "for name, phone in art_gallery.items():\n",
    "    print(name)\n",
    "    print(phone)\n",
    "    \n",
    "print(art_gallery.items())\n",
    "# dict.items() method converted dict key/value pairs into two elements tuples contained in a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75932b7-24a1-4288-b0c7-57d5cc64e5d7",
   "metadata": {},
   "source": [
    "## Checking dictionaries for data\n",
    "\n",
    "\n",
    "   __.get() does a lot of work to check for a key__\n",
    "#   __in operator is much more efficient and clearner__\n",
    "   \n",
    "\n",
    "\n",
    "   **Now lets look at a more Pythonic method for checking if data is present in a dictionary.  \n",
    "    \n",
    "   **Earlier, we used .get() method to safely look for keys, and we can use it to check if a key is in a dictionary\n",
    "   \n",
    "   **However, Python provides the in operator to see if a key is in a dictionary. It returns a boolean\n",
    "     **Since it returns a boolean, it is often used in conditionals statements like an if/else statement\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ef863ea6-dc0f-491f-a5a0-452a89ef85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336', 'Africa Afericam History': '(313) 767-2234'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_gallery = {\"Nyabinghi Africian Gift Shop\": \"(212) 566-3336\"}\n",
    "\n",
    "art_gallery[\"Africa Afericam History\"] = \"(313) 767-2234\"\n",
    "print(art_gallery)\n",
    "print()\n",
    "\n",
    "\"Africa Afericam History\" in art_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23de6a95-4b13-425c-a937-db3f4b21abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336', 'Africa Afericam History': '(313) 767-2234'}\n",
      "\n",
      "New key/value inserted\n",
      "\n",
      "{'Nyabinghi Africian Gift Shop': '(212) 566-3336', 'Africa Afericam History': '(313) 767-2234', 'A J ARTS LTD': '(718) 763-5473', 'Doug Meyer Fine Art': '(718) 375-8006', 'Portrait Gallery': '(718) 377-8762'}\n"
     ]
    }
   ],
   "source": [
    "art_gallery = {\"Nyabinghi Africian Gift Shop\": \"(212) 566-3336\"}\n",
    "\n",
    "art_gallery[\"Africa Afericam History\"] = \"(313) 767-2234\"\n",
    "print(art_gallery)\n",
    "print()\n",
    "\n",
    "\n",
    "galleries = [(\"A J ARTS LTD\", \"(718) 763-5473\"), \n",
    "            (\"Doug Meyer Fine Art\", \"(718) 375-8006\"), \n",
    "            (\"Portrait Gallery\", \"(718) 377-8762\")]\n",
    "\n",
    "\n",
    "if \"Africa Afericam History\" in art_gallery:\n",
    "    print(\"New key/value inserted\\n\")\n",
    "else: \n",
    "    art_gallery.update(galleries)\n",
    "\n",
    "art_gallery.update(galleries)\n",
    "print(art_gallery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83a627-3678-4fd5-ad61-7b734449a266",
   "metadata": {},
   "source": [
    "## Working with dictionaries more pythonically\n",
    "\n",
    "So far, you've worked a lot with the keys of a dictionary to access data, but in Python, the preferred manner for iterating over items in a dictionary is with the __.items() method.\n",
    "\n",
    "## This returns each key and value from the dictionary as a tuple\n",
    "This returns each key and value from the dictionary as a tuple, which you can unpack in a for loop. You'll now get practice doing this.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Iterate over baby_names[2014], unpacking it into rank and name.\n",
    "        Print each rank and name.\n",
    "    Repeat the process for baby_names[2012].\n",
    "\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the .items() method on baby_names[2014] and baby_names[2012] in a for loop, and unpack the resulting tuple in each case into rank and name. Then, use a print() function to print both rank and name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7e840-74fc-4b61-b098-4acd79dedc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the 2014 nested dictionary\n",
    "for rank, name in baby_names[\"2014\"].items():\n",
    "    # Print rank and name\n",
    "    print(rank, name)\n",
    "    \n",
    "# Iterate over the 2012 nested dictionary\n",
    "\n",
    "    # Print rank and name\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc28182-c1bc-44e7-80d6-d882ce066f06",
   "metadata": {},
   "source": [
    "## Checking dictionaries for data\n",
    "\n",
    "You can check to see if a key exists in a dictionary by using the in expression.\n",
    "\n",
    "For example, you can check to see if 'cookies' is a key in the dictionary by using if 'cookies' in recipes_dict: this allows you to safely react to data being present in the dictionary.\n",
    "\n",
    "You can also use the in expression so see if data is in the value of a dictionary such as if 'cookies' in recipes_dict.values(). Remember you have to handle nested dictionaries differently as illustrated in the video and previous exercises, and use the in expression on each nested dictionary.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Check to see if 2011 is in the baby_names dictionary.\n",
    "        Print 'Found 2011' if it is present.\n",
    "    Check to see if 1 is in baby_names[2012].\n",
    "        Print 'Found Rank 1 in 2012' if found and 'Rank 1 missing from 2012' if not found.\n",
    "    Check to see if rank 5 is in baby_names[2013].\n",
    "        Print 'Found Rank 5' if it is present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f49a66-8f87-44e7-bf86-5dd9ebc8a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if 2011 is in baby_names\n",
    "if \"2011\" in baby_names:\n",
    "    # Print 'Found 2011'\n",
    "    print('Found 2011')\n",
    "    \n",
    "# Check to see if rank 1 is in 2012\n",
    "if \"1\" in baby_names[\"2012\"]\n",
    "    # Print 'Found Rank 1 in 2012' if found\n",
    "    print('Found Rank 1 in 2012')\n",
    "else:\n",
    "    # Print 'Rank 1 missing from 2012' if not found\n",
    "    print('Rank 1 missing from 2012')\n",
    "    \n",
    "# Check to see if Rank 5 is in 2013\n",
    "if \"5\" in baby_names[\"2013\"]\n",
    "    # Print 'Found Rank 5'\n",
    "    print('Found Rank 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e226fa-c488-4ab0-952a-2ca2a5de8845",
   "metadata": {},
   "source": [
    "## Working with CSV files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Python methods and function (methods are associated with its class):\n",
    "   __a method: math.ceil(15.25)\n",
    "   __a function: max(15, 6)\n",
    "\n",
    "\n",
    "**Reading from a file using CSV reader\n",
    "\n",
    "   __Python CSV module__\n",
    "   __open() function provides a variable that represents a file, takes a path and a mode__\n",
    "#   __csv.reader() reades a file objectand returns the lines from the file as tuples__\n",
    "#   __.close() method close a file object__\n",
    "   \n",
    "**To create a Python file object, you use the open() function, which accepts a file name and a mode.  \n",
    "\n",
    "\n",
    "\n",
    "Reading from a CSV file is done using the reader object. The CSV file is opened as a text file with Pythons built-in open() function, which returns a file object. This is then passed to the reader, which does the heavy lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "914ba601-309b-4450-985a-90678565d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'the_geom', 'TEL', 'URL', 'Address']\n",
      "['Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csvfile = open(\"art_gallery.csv\")\n",
    "\n",
    "# ************************************************************************************************ #\n",
    "# ************************************************************************************************ #\n",
    "# its a list of strings or a list of tuples? \n",
    "for row in csv.reader(csvfile):\n",
    "    print(row)\n",
    "    #print(type(row[0]))\n",
    "    \n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "eff464a9-30cd-4405-bbec-2fc2e807929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 'the_geom', 'TEL', 'URL', 'Address'), ('Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST')]\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('art_gallery.csv', 'r') as read_obj:\n",
    "    #print(read_obj)     # <_io.TextIOWrapper name='art_gallery.csv' mode='r' encoding='UTF-8'>\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    #print(csv_reader)   # <_csv.reader object at 0x7f8bdd69a3c0>\n",
    "    \n",
    "    # Get all rows of csv from csv_reader object as list of tuples\n",
    "    list_of_tuples = list(map(tuple, csv_reader))\n",
    "    print(list_of_tuples)\n",
    "    \n",
    "    for row in list_of_tuples:\n",
    "        print(type(row))\n",
    "\n",
    "# with context manager, file was automatically closed after with open() as read_obj: statements\n",
    "read_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ebd2e14b-4b86-40e0-a2e1-6264c8d4e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 'the_geom', 'TEL', 'URL', 'Address'), ('Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST')]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csvfile = open(\"art_gallery.csv\", 'r')\n",
    "art_gallery = [tuple(row) for row in csv.reader(csvfile)]\n",
    "print(art_gallery)\n",
    "\n",
    "csvfile.close()\n",
    "\n",
    "# ********************************************************************************************* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f6ef0-05c5-482c-ae08-24881e07af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = open(\"path_to_art_gallery.csv\", \"r\")\n",
    "\n",
    "for row in csv.reader(csv_file):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430bf99-6319-4fa8-98af-d0ff28bfe0dc",
   "metadata": {},
   "source": [
    "## Creating a dictionary from a file\n",
    "\n",
    "#   __Ofen we want to go from CSV file to dictionary__\n",
    "   __DictReader does just that__\n",
    "   __If data doesnt have a header row, you can pass in the column names list__\n",
    "\n",
    "\n",
    "  **The Python CSV module also provides a way to directly create a dictionary from a csv file with the DictReader class\n",
    "  \n",
    "  **If the file has a header row, that row will automatically be used as the keys for the dictionary; however, if not you can supply a list of keys to be used\n",
    "  \n",
    "#  **Each row from the file is returned as an orded dictionary\n",
    "  \n",
    "  **Its all the same untill line there we use the DictReader instead of csvreader\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39f51971-707c-4467-8faf-388d35b51655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Odyssia Gallery', 'the_geom': 'POINT(-73.96 40.76)', 'TEL': '(212) 486-7338', 'URL': 'https://www.livevillage.com/netword/art/odyssia-gallery.html', 'Address': '305 E 61st ST'}\n",
      "{'name': 'Africa America History Art', 'the_geom': 'POINT(-78.78 45.78)', 'TEL': '(878) 234-1234', 'URL': 'https://www.africa-america-art.com', 'Address': 'No. 234 5st TT'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrestkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrestval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'excel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/lib/python3.9/csv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csvfile = open(\"art_gallery.csv\")\n",
    "\n",
    "for row in csv.DictReader(csvfile):\n",
    "    print(row)\n",
    "    \n",
    "csvfile.close()\n",
    "\n",
    "?csv.DictReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2dbca9b0-c3c5-4d3e-aa1a-bc179e5b6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name', 'the_geom', 'TEL', 'URL', 'Address'), ('Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST')]\n",
      "('name', 'the_geom', 'TEL', 'URL', 'Address')\n",
      "<class 'tuple'>\n",
      "('Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST')\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('art_gallery.csv', 'r') as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    \n",
    "    list_of_tuples = list(map(tuple, csv_reader))  # do not use [map(tuple, csv_reader)]\n",
    "    print(list_of_tuples)                          # [<map object at 0x7f8bfc030760>]\n",
    "    \n",
    "    for row in list_of_tuples:\n",
    "        print(row)\n",
    "        print(type(row))\n",
    "\n",
    "#\n",
    "#art_galleries = {\"name\": \"Africa America History Art\", \"the_geom\": \"POINT(-78.78 45.78)\", \n",
    "#                 \"TEL\": \"(878) 234-1234\", \"URL\": \"https://www.africa-america-art.com\", \n",
    "#                 \"Address\": \"No. 234 5st TT\"}\n",
    "#art_galleries.update(list_of_tuples)   # .update() can be applied on list of two elements tuples\n",
    "#print(art_galleries)                   # can we read .csv into that data structure for later use?\n",
    "\n",
    "\n",
    "# ********************************************************************************************* #\n",
    "''' The former just wraps the entire item in square brackets [], making it a one-item list:\n",
    "\n",
    "    >>> [{'foo': 1, 'bar': 2}]\n",
    "    [{'foo': 1, 'bar': 2}]\n",
    "\n",
    "    The latter iterates over the dictionary (getting keys) and produces a list out of them:\n",
    "\n",
    "    >>> list({'foo': 1, 'bar': 2})\n",
    "    ['foo', 'bar']      '''\n",
    "# ********************************************************************************************* #\n",
    "\n",
    "        \n",
    "read_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "afdec1da-1fa0-48a7-a1f4-2938e8108203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['name', 'the_geom', 'TEL', 'URL', 'Address'], ['Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST'])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"art_gallery.csv\", \"r\") as f_obj:\n",
    "    dict_reader = csv.reader(f_obj)         # csv.reader() returns a sequence of row in each list\n",
    "    \n",
    "    dict_of_tuples = tuple(dict_reader)     # we save those sequence of lists into a tuple\n",
    "    #dict_gallery[dict_reader[0]]=dict_reader[2]\n",
    "    print(dict_of_tuples)\n",
    "    #print(dict_gallery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c927e846-29bd-4cfb-89a5-21bc8e436ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7f9bdc0474a0>\n",
      "['Odyssia Gallery', 'POINT(-73.96 40.76)', '(212) 486-7338', 'https://www.livevillage.com/netword/art/odyssia-gallery.html', '305 E 61st ST']\n",
      "{'Odyssia Gallery': '(212) 486-7338'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "f_obj = open(\"art_gallery.csv\", \"r\")\n",
    "\n",
    "csvread = csv.reader(f_obj)\n",
    "print(csvread)\n",
    "\n",
    "# ****************************************************************************** #\n",
    "# # This skips the first row of the CSV file.\n",
    "next(csvread)\n",
    "\n",
    "new_dict = {}\n",
    "for row in csvread:\n",
    "    #print(row)\n",
    "    print(row)\n",
    "    new_dict[row[0]]=row[2]\n",
    "    \n",
    "print(new_dict)\n",
    "\n",
    "# ****************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57864d-1a87-46e8-9e11-85a9bb1741c5",
   "metadata": {},
   "source": [
    "## Reading from a file using CSV reader\n",
    "\n",
    "Python provides a wonderful module called csv to work with CSV files. You can pass the .reader() method of csv a Python file object and use it as you would any other iterable. To create a Python file object, you use the open() function, which accepts a file name and a mode. The mode is typically 'r' for read or 'w' for write.\n",
    "\n",
    "Though you won't use it for this exercise, often CSV files will have a header row with field names, and \n",
    "# you will need to use slice notation such as [1:] to skip the header row.\n",
    "\n",
    "You'll now use the csv module to read the baby_names.csv file and fill the baby_names dictionary with data. This baby_names dictionary has already been created for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the python csv module.\n",
    "    Create a Python file object in read mode for baby_names.csv called csvfile with the open function.\n",
    "    Use the reader method from the csv module on the file object in a for loop. Inside the loop:\n",
    "        Print each row and add the rank (the 6th element of row) as the key and name (the 4th element of row) as the value to the existing dictionary (baby_names).\n",
    "    Print the keys of baby_names.\n",
    "\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "    To create csvfile, use the open() function, passing in the name of the CSV file as well as the argument 'r'.\n",
    "    To loop over csvfile, pass it in as an argument to csv.reader().\n",
    "    Inside the loop, use row[5] as the key of baby_names, and assign row[3] as its value.\n",
    "    Use the .keys() method on baby_names to print its keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb515dee-e5df-4079-a091-42000e3d942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Abigail'\", \"'0'\", \"'24'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Ada'\", \"'1'\", \"'35'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Aisha'\", \"'2'\", \"'35'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Aiza'\", \"'3'\", \"'38'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Aleena'\", \"'4'\", \"'36'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Alexa'\", \"'5'\", \"'38'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Alexandra'\", \"'6'\", \"'35'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Alice'\", \"'7'\", \"'21'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Alina'\", \"'8'\", \"'23'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Alisha'\", \"'9'\", \"'38'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Aliyah'\", \"'10'\", \"'36'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Allison'\", \"'11'\", \"'31'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Alyssa'\", \"'12'\", \"'22'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Amanda'\", \"'13'\", \"'26'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Amber'\", \"'14'\", \"'35'\", '']\n",
      "[\"'2011'\", \"'FEMALE'\", \"'ASIAN AND PACIFIC ISLANDER'\", \"'Amelia'\", \"'15'\", \"'34'\", '']\n",
      "dict_keys([\"'0'\", \"'1'\", \"'2'\", \"'3'\", \"'4'\", \"'5'\", \"'6'\", \"'7'\", \"'8'\", \"'9'\", \"'10'\", \"'11'\", \"'12'\", \"'13'\", \"'14'\", \"'15'\"])\n",
      "{\"'0'\": \"'Abigail'\", \"'1'\": \"'Ada'\", \"'2'\": \"'Aisha'\", \"'3'\": \"'Aiza'\", \"'4'\": \"'Aleena'\", \"'5'\": \"'Alexa'\", \"'6'\": \"'Alexandra'\", \"'7'\": \"'Alice'\", \"'8'\": \"'Alina'\", \"'9'\": \"'Alisha'\", \"'10'\": \"'Aliyah'\", \"'11'\": \"'Allison'\", \"'12'\": \"'Alyssa'\", \"'13'\": \"'Amanda'\", \"'14'\": \"'Amber'\", \"'15'\": \"'Amelia'\"}\n"
     ]
    }
   ],
   "source": [
    "# Import the python CSV module\n",
    "import csv\n",
    "\n",
    "# Create a python file object in read mode for the baby_names.csv file: csvfile\n",
    "csvfile = open(\"baby_names.csv\", \"r\")\n",
    "\n",
    "# Loop over a csv reader on the file object\n",
    "baby_names = {}\n",
    "\n",
    "csvread = csv.reader(csvfile)\n",
    "# *********************************************************************************************** #\n",
    "# *********************************************************************************************** #\n",
    "# This skips the first row of the CSV file.\n",
    "next(csvread)\n",
    "\n",
    "for row in csvread:\n",
    "    # Print each row \n",
    "    print(row)\n",
    "    # Add the rank and name to the dictionary\n",
    "    baby_names[row[4]] = row[3]\n",
    "\n",
    "# Print the dictionary keys\n",
    "print(baby_names.keys())\n",
    "print(baby_names)\n",
    "\n",
    "csvfile.close()\n",
    "\n",
    "# ************************************************************************************************ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7da07b-9504-4474-83f3-bbd9a3a876cc",
   "metadata": {},
   "source": [
    "## Creating a dictionary from a file\n",
    "\n",
    "The csv module also provides a way to directly create a dictionary from a CSV file with the DictReader class. If the file has a header row, that row will automatically be used as the keys for the dictionary. However, if not, you can supply a list of keys to be used. Each row from the file is returned as a dictionary. Using DictReader can make it much easier to read your code and understand what data is being used, especially when compared to the numbered indexes you used in the prior exercise.\n",
    "\n",
    "Your job in this exercise is to create a dictionary directly from the data file using DictReader. NOTE: The misspellings are from the original data, and this is a very common issue. Again, the baby_names dictionary has already been created for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the Python csv module.\n",
    "    Create a Python file object in read mode for the baby_names.csv called csvfile.\n",
    "    Loop over a csv DictReader on csvfile. Inside the loop:\n",
    "        Print each row.\n",
    "        Add the 'RANK' of each row as the key and 'NAME' of each row as the value to the existing dictionary.\n",
    "    Print the dictionary keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "542424cf-f4d0-4fc9-99c7-d5fc0c4b68ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Abigail'\", 'Rank': \"'0'\", 'Height': \"'24'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Ada'\", 'Rank': \"'1'\", 'Height': \"'35'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Aisha'\", 'Rank': \"'2'\", 'Height': \"'35'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Aiza'\", 'Rank': \"'3'\", 'Height': \"'38'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Aleena'\", 'Rank': \"'4'\", 'Height': \"'36'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Alexa'\", 'Rank': \"'5'\", 'Height': \"'38'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Alexandra'\", 'Rank': \"'6'\", 'Height': \"'35'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Alice'\", 'Rank': \"'7'\", 'Height': \"'21'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Alina'\", 'Rank': \"'8'\", 'Height': \"'23'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Alisha'\", 'Rank': \"'9'\", 'Height': \"'38'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Aliyah'\", 'Rank': \"'10'\", 'Height': \"'36'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Allison'\", 'Rank': \"'11'\", 'Height': \"'31'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Alyssa'\", 'Rank': \"'12'\", 'Height': \"'22'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Amanda'\", 'Rank': \"'13'\", 'Height': \"'26'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Amber'\", 'Rank': \"'14'\", 'Height': \"'35'\"}\n",
      "{'Year': \"'2011'\", 'Sex': \"'FEMALE'\", 'Address': \"'ASIAN AND PACIFIC ISLANDER'\", 'Name': \"'Amelia'\", 'Rank': \"'15'\", 'Height': \"'34'\"}\n",
      "dict_keys([\"'0'\", \"'1'\", \"'2'\", \"'3'\", \"'4'\", \"'5'\", \"'6'\", \"'7'\", \"'8'\", \"'9'\", \"'10'\", \"'11'\", \"'12'\", \"'13'\", \"'14'\", \"'15'\"])\n",
      "{\"'0'\": \"'Abigail'\", \"'1'\": \"'Ada'\", \"'2'\": \"'Aisha'\", \"'3'\": \"'Aiza'\", \"'4'\": \"'Aleena'\", \"'5'\": \"'Alexa'\", \"'6'\": \"'Alexandra'\", \"'7'\": \"'Alice'\", \"'8'\": \"'Alina'\", \"'9'\": \"'Alisha'\", \"'10'\": \"'Aliyah'\", \"'11'\": \"'Allison'\", \"'12'\": \"'Alyssa'\", \"'13'\": \"'Amanda'\", \"'14'\": \"'Amber'\", \"'15'\": \"'Amelia'\"}\n"
     ]
    }
   ],
   "source": [
    "# Import the python CSV module\n",
    "import csv\n",
    "\n",
    "# Create a python file object in read mode for the baby_names.csv file: csvfile\n",
    "csvfile = open(\"baby_names.csv\", \"r\")\n",
    "\n",
    "# Loop over a csv reader on the file object\n",
    "baby_names = {}\n",
    "\n",
    "csvread = csv.DictReader(csvfile)\n",
    "\n",
    "for row in csvread:\n",
    "    # Print each row \n",
    "    print(row)\n",
    "    # Add the rank and name to the dictionary\n",
    "    baby_names[row[\"Rank\"]] = row[\"Name\"]\n",
    "\n",
    "# Print the dictionary keys\n",
    "print(baby_names.keys())\n",
    "print(baby_names)\n",
    "\n",
    "csvfile.close()\n",
    "\n",
    "# ************************************************************************************************ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e7fb8-64d1-437a-a816-fdb2f37a04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the python CSV module\n",
    "import csv\n",
    "\n",
    "# Create a python file object in read mode for the `baby_names.csv` file: csvfile\n",
    "csvfile = open(\"baby_names.csv\", \"r\")\n",
    "\n",
    "# Loop over a DictReader on the file\n",
    "for row in csv.DictReader(csvfile):\n",
    "    # Print each row \n",
    "    print(row)\n",
    "    # Add the rank and name to the dictionary: baby_names\n",
    "    baby_names[row[\"RANK\"]] = row[\"NAME\"]\n",
    "\n",
    "# Print the dictionary keys\n",
    "print(baby_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb0676-27ee-4cfd-8b54-50190042ab18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667878ae-2087-49fd-b1e1-b1b3f8a53d32",
   "metadata": {},
   "source": [
    "## Counting made easy\n",
    "\n",
    "\n",
    "\n",
    "**As a data scientist, we're often going to need to count items, create dictionaries values before we know keys to store them in, or maintain order in a dictionary. \n",
    "\n",
    "**The collections module is a part of Python standard library and holds several more advanced data containers which solve these problems and more.  \n",
    "\n",
    "\n",
    "## Lets start our tour of the collection module by learning about Counter. \n",
    "\n",
    "**Counter is a powerful Python object based on the dictionary object that accepts a list and counts the number of times a value is found within the elements of that list\n",
    "\n",
    "**Since its based on a dictionary, you can use all the normal dictionary features. \n",
    "\n",
    "   **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce8bea-808d-4885-8f7f-03064d35c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "nyc_eatery_types, it contains one column of data called type from a table about eateries in nyc parks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "nyc_eatery_count_by_types = Counter(nyc_eatery_types)\n",
    "print(nyc_eatery_count_by_types)\n",
    "\n",
    "# Return\n",
    "Counter({\"Mobile Food Truck\": 114, \"Food Cart\": 74, \"Snack Bar\": 24, \"Specialty Cart\": 18, \n",
    "         \"Restaurant\": 15, \"Fruit & Vegetable Cart\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1183d13-c5c7-4ff4-95bb-cbbcb2928900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geraldine', 'Gia', 'Gianna', 'Giselle']\n",
      "Counter({'Gia': 2, 'Gianna': 2, 'Grace': 2, 'Hailey': 2, 'Hannah': 2, 'Hazel': 2, 'Iris': 2, 'Isabel': 2, 'Isabella': 2, 'Isabelle': 2, 'Izabella': 2, 'Jacqueline': 2, 'Jasmine': 2, 'Jessica': 2, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Kaitlyn': 2, 'Kate': 2, 'Katherine': 2, 'Kayla': 2, 'Kaylee': 2, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'London': 2, 'Lucia': 2, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Maya': 2, 'Melanie': 2, 'Mia': 2, 'Michelle': 2, 'Mikayla': 2, 'Mila': 2, 'Miriam': 2, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nicole': 2, 'Nina': 2, 'Olivia': 2, 'Penelope': 2, 'Rachel': 2, 'Rebecca': 2, 'Riley': 2, 'Rose': 2, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Sara': 2, 'Sarah': 2, 'Sasha': 2, 'Savannah': 2, 'Scarlett': 2, 'Sienna': 2, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stella': 2, 'Taylor': 2, 'Valentina': 2, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Alexa': 2, 'Alexandra': 2, 'Alice': 2, 'Alina': 2, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amy': 2, 'Angelina': 2, 'Anna': 2, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ashley': 2, 'Audrey': 2, 'Ava': 2, 'Bella': 2, 'Brianna': 2, 'Catherine': 2, 'Cecilia': 2, 'Charlotte': 2, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Geraldine': 1, 'Giselle': 1, 'Guadalupe': 1, 'Haley': 1, 'Haylee': 1, 'Hayley': 1, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Isabela': 1, 'Isis': 1, 'Itzel': 1, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Katelyn': 1, 'Katelynn': 1, 'Katie': 1, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lindsay': 1, 'Lizbeth': 1, 'Luna': 1, 'Luz': 1, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Megan': 1, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Miah': 1, 'Mikaela': 1, 'Miley': 1, 'Miranda': 1, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Noemi': 1, 'Nyla': 1, 'Paola': 1, 'Perla': 1, 'Raquel': 1, 'Rihanna': 1, 'Rosa': 1, 'Roselyn': 1, 'Samara': 1, 'Sarai': 1, 'Sariah': 1, 'Savanna': 1, 'Scarlet': 1, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Skyla': 1, 'Stacy': 1, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Tiana': 1, 'Tiffany': 1, 'Valeria': 1, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexis': 1, 'Alicia': 1, 'Alisa': 1, 'Aliza': 1, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Anastasia': 1, 'Angelica': 1, 'Annabel': 1, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Atara': 1, 'Aubrey': 1, 'Autumn': 1, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "f_name_2011 = []\n",
    "\n",
    "for row in records:\n",
    "    if row[0] == \"2011\" and row[1] == \"FEMALE\":\n",
    "        f_name_2011.append(row[3])\n",
    "        # ******************************************************************************************* #\n",
    "        #f_name_2011.append((row[3], row[4]))  # Counter tuple pairs in a list, every one Counter as 1\n",
    "        # Thank and truly understand it\n",
    "        \n",
    "print(f_name_2011[:4])\n",
    "        \n",
    "f_name_2011_count = Counter(f_name_2011)\n",
    "print(f_name_2011_count)\n",
    "#print(Counter.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe02f6cb-f54a-470a-8bfb-a8badb49afbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'Geraldine': 1, 'Gia': 2, 'Gianna': 2, 'Giselle': 1, 'Grace': 2, 'Guadalupe': 1, 'Hailey': 2, 'Haley': 1, 'Hannah': 2, 'Haylee': 1, 'Hayley': 1, 'Hazel': 2, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Iris': 2, 'Isabel': 2, 'Isabela': 1, 'Isabella': 2, 'Isabelle': 2, 'Isis': 1, 'Itzel': 1, 'Izabella': 2, 'Jacqueline': 2, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jasmine': 2, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jessica': 2, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kaitlyn': 2, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Kate': 2, 'Katelyn': 1, 'Katelynn': 1, 'Katherine': 2, 'Katie': 1, 'Kayla': 2, 'Kaylee': 2, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'Lindsay': 1, 'Lizbeth': 1, 'London': 2, 'Lucia': 2, 'Luna': 1, 'Luz': 1, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Maya': 2, 'Megan': 1, 'Melanie': 2, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Mia': 2, 'Miah': 1, 'Michelle': 2, 'Mikaela': 1, 'Mikayla': 2, 'Mila': 2, 'Miley': 1, 'Miranda': 1, 'Miriam': 2, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Nicole': 2, 'Nina': 2, 'Noemi': 1, 'Nyla': 1, 'Olivia': 2, 'Paola': 1, 'Penelope': 2, 'Perla': 1, 'Rachel': 2, 'Raquel': 1, 'Rebecca': 2, 'Rihanna': 1, 'Riley': 2, 'Rosa': 1, 'Rose': 2, 'Roselyn': 1, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Samara': 1, 'Sara': 2, 'Sarah': 2, 'Sarai': 1, 'Sariah': 1, 'Sasha': 2, 'Savanna': 1, 'Savannah': 2, 'Scarlet': 1, 'Scarlett': 2, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Sienna': 2, 'Skyla': 1, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stacy': 1, 'Stella': 2, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Taylor': 2, 'Tiana': 1, 'Tiffany': 1, 'Valentina': 2, 'Valeria': 1, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexa': 2, 'Alexandra': 2, 'Alexis': 1, 'Alice': 2, 'Alicia': 1, 'Alina': 2, 'Alisa': 1, 'Aliza': 1, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Amy': 2, 'Anastasia': 1, 'Angelica': 1, 'Angelina': 2, 'Anna': 2, 'Annabel': 1, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Ashley': 2, 'Atara': 1, 'Aubrey': 1, 'Audrey': 2, 'Autumn': 1, 'Ava': 2, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bella': 2, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Brianna': 2, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Catherine': 2, 'Cecilia': 2, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Charlotte': 2, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1}\n"
     ]
    }
   ],
   "source": [
    "fruits = ['apple', 'banana', 'cherry']\n",
    "x = fruits.count(\"cherry\")\n",
    "print(x)\n",
    "\n",
    "my_dict = {i:f_name_2011.count(i) for i in f_name_2011}\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "212a78fe-6c7c-46cd-96ef-3c574de3da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Geraldine': 1, 'Gia': 2, 'Gianna': 2, 'Giselle': 1, 'Grace': 2, 'Guadalupe': 1, 'Hailey': 2, 'Haley': 1, 'Hannah': 2, 'Haylee': 1, 'Hayley': 1, 'Hazel': 2, 'Heaven': 1, 'Heidi': 1, 'Heidy': 1, 'Helen': 1, 'Imani': 1, 'Ingrid': 1, 'Irene': 1, 'Iris': 2, 'Isabel': 2, 'Isabela': 1, 'Isabella': 2, 'Isabelle': 2, 'Isis': 1, 'Itzel': 1, 'Izabella': 2, 'Jacqueline': 2, 'Jada': 1, 'Jade': 1, 'Jaelynn': 1, 'Jamie': 1, 'Janelle': 1, 'Jaslene': 1, 'Jasmin': 1, 'Jasmine': 2, 'Jayda': 1, 'Jayla': 1, 'Jaylah': 1, 'Jayleen': 1, 'Jaylene': 1, 'Jaylin': 1, 'Jaylyn': 1, 'Jazlyn': 1, 'Jazmin': 1, 'Jazmine': 1, 'Jennifer': 1, 'Jessica': 2, 'Jimena': 1, 'Jocelyn': 1, 'Johanna': 1, 'Joselyn': 1, 'Julia': 2, 'Juliana': 2, 'Julianna': 2, 'Juliet': 2, 'Juliette': 2, 'Julissa': 1, 'Kaelyn': 1, 'Kailey': 1, 'Kailyn': 1, 'Kaitlyn': 2, 'Kamila': 1, 'Karen': 1, 'Karla': 1, 'Kate': 2, 'Katelyn': 1, 'Katelynn': 1, 'Katherine': 2, 'Katie': 1, 'Kayla': 2, 'Kaylee': 2, 'Kayleen': 1, 'Kayleigh': 1, 'Kaylie': 1, 'Kaylin': 1, 'Keily': 1, 'Kelly': 1, 'Keyla': 1, 'Khloe': 1, 'Kiara': 1, 'Kimberly': 1, 'Krystal': 1, 'Kylee': 1, 'Kylie': 2, 'Laila': 2, 'Laura': 2, 'Lauren': 2, 'Layla': 2, 'Lea': 2, 'Leah': 2, 'Leila': 2, 'Leilani': 1, 'Lesley': 1, 'Leslie': 1, 'Lesly': 1, 'Leyla': 1, 'Lia': 2, 'Liana': 2, 'Liliana': 2, 'Lily': 2, 'Lindsay': 1, 'Lizbeth': 1, 'London': 2, 'Lucia': 2, 'Luna': 1, 'Luz': 1, 'Madeline': 2, 'Madelyn': 2, 'Madison': 2, 'Makayla': 2, 'Maria': 2, 'Mariah': 1, 'Mariana': 1, 'Marilyn': 1, 'Marisol': 1, 'Maya': 2, 'Megan': 1, 'Melanie': 2, 'Melany': 1, 'Melissa': 1, 'Melody': 1, 'Mia': 2, 'Miah': 1, 'Michelle': 2, 'Mikaela': 1, 'Mikayla': 2, 'Mila': 2, 'Miley': 1, 'Miranda': 1, 'Miriam': 2, 'Mya': 1, 'Nadia': 1, 'Nancy': 1, 'Naomi': 2, 'Natalia': 2, 'Natalie': 2, 'Nataly': 1, 'Natasha': 1, 'Nathalia': 1, 'Nathalie': 1, 'Nathaly': 1, 'Nayeli': 1, 'Nevaeh': 1, 'Nicole': 2, 'Nina': 2, 'Noemi': 1, 'Nyla': 1, 'Olivia': 2, 'Paola': 1, 'Penelope': 2, 'Perla': 1, 'Rachel': 2, 'Raquel': 1, 'Rebecca': 2, 'Rihanna': 1, 'Riley': 2, 'Rosa': 1, 'Rose': 2, 'Roselyn': 1, 'Ruby': 2, 'Sabrina': 2, 'Sadie': 2, 'Samantha': 2, 'Samara': 1, 'Sara': 2, 'Sarah': 2, 'Sarai': 1, 'Sariah': 1, 'Sasha': 2, 'Savanna': 1, 'Savannah': 2, 'Scarlet': 1, 'Scarlett': 2, 'Selena': 1, 'Serenity': 1, 'Sherlyn': 1, 'Shirley': 1, 'Sienna': 2, 'Skyla': 1, 'Skylar': 2, 'Sofia': 2, 'Sophia': 2, 'Sophie': 2, 'Stacy': 1, 'Stella': 2, 'Stephanie': 1, 'Stephany': 1, 'Tatiana': 1, 'Taylor': 2, 'Tiana': 1, 'Tiffany': 1, 'Valentina': 2, 'Valeria': 1, 'Valerie': 2, 'Vanessa': 2, 'Veronica': 2, 'Victoria': 2, 'Violet': 2, 'Viviana': 1, 'Wendy': 1, 'Ximena': 1, 'Yamilet': 1, 'Yaretzi': 1, 'Zoe': 2, 'Zoey': 2, 'Abigail': 2, 'Addison': 1, 'Adele': 1, 'Adeline': 1, 'Adina': 1, 'Adriana': 1, 'Adrianna': 1, 'Ahuva': 1, 'Alessandra': 1, 'Alessia': 1, 'Alexa': 2, 'Alexandra': 2, 'Alexis': 1, 'Alice': 2, 'Alicia': 1, 'Alina': 2, 'Alisa': 1, 'Aliza': 1, 'Allison': 2, 'Alyssa': 2, 'Amanda': 2, 'Amelia': 2, 'Amelie': 1, 'Amina': 1, 'Amira': 1, 'Amy': 2, 'Anastasia': 1, 'Angelica': 1, 'Angelina': 2, 'Anna': 2, 'Annabel': 1, 'Annabelle': 2, 'Ariana': 2, 'Arianna': 2, 'Ariel': 1, 'Ariela': 1, 'Ariella': 1, 'Ashley': 2, 'Atara': 1, 'Aubrey': 1, 'Audrey': 2, 'Autumn': 1, 'Ava': 2, 'Avery': 1, 'Avigail': 1, 'Aviva': 1, 'Ayla': 1, 'Baila': 1, 'Barbara': 1, 'Batsheva': 1, 'Batya': 1, 'Beatrice': 1, 'Bella': 2, 'Bianca': 1, 'Blake': 1, 'Blima': 1, 'Blimy': 1, 'Bracha': 1, 'Breindy': 1, 'Brianna': 2, 'Bridget': 1, 'Brooke': 1, 'Brooklyn': 1, 'Brucha': 1, 'Bruchy': 1, 'Brynn': 1, 'Caitlin': 1, 'Cameron': 1, 'Caroline': 1, 'Casey': 1, 'Catherine': 2, 'Cecilia': 2, 'Celia': 1, 'Chana': 1, 'Chany': 1, 'Charlie': 1, 'Charlotte': 2, 'Chava': 1, 'Chavy': 1, 'Chaya': 1, 'Chloe': 2, 'Christina': 2, 'Claire': 2, 'Clara': 1, 'Colette': 1, 'Cora': 1, 'Dahlia': 1, 'Daisy': 1, 'Dalia': 1, 'Daniela': 1, 'Daniella': 1, 'Danielle': 1, 'Devora': 1, 'Devorah': 1, 'Diana': 1, 'Dina': 1, 'Dylan': 1, 'Eden': 1, 'Eleanor': 1, 'Elena': 1, 'Eliana': 1, 'Elise': 1, 'Elisheva': 1, 'Eliza': 1, 'Elizabeth': 1, 'Ella': 1, 'Elle': 1, 'Elliana': 1, 'Ellie': 1, 'Eloise': 1, 'Emerson': 1, 'Emilia': 1, 'Emily': 1, 'Emma': 1, 'Erin': 1, 'Ester': 1, 'Esther': 1, 'Esty': 1, 'Etty': 1, 'Eva': 1, 'Eve': 1, 'Evelyn': 1, 'Faiga': 1, 'Faigy': 1, 'Finley': 1, 'Fiona': 1, 'Fradel': 1, 'Fraidy': 1, 'Francesca': 1, 'Frimet': 1, 'Gabriela': 1, 'Gabriella': 1, 'Gabrielle': 1, 'Gemma': 1, 'Genevieve': 1, 'Georgia': 1, 'Giovanna': 1, 'Gittel': 1, 'Gitty': 1, 'Giuliana': 1, 'Golda': 1, 'Goldy': 1, 'Greta': 1, 'Hadassa': 1, 'Hadassah': 1, 'Hanna': 1, 'Harper': 1, 'Henny': 1, 'Hindy': 1, 'Idy': 1, 'Ilana': 1, 'Isla': 1, 'Ivy': 1, 'Jane': 1, 'Jenna': 1, 'Jordyn': 1, 'Josephine': 1, 'Joyce': 1, 'Julie': 1, 'Kathryn': 1, 'Kira': 1, 'Lara': 1, 'Lena': 1, 'Leora': 1, 'Liba': 1, 'Libby': 1, 'Lila': 1, 'Lilah': 1, 'Lillian': 1, 'Lilly': 1, 'Lina': 1, 'Lola': 1, 'Lucy': 1, 'Lyla': 1, 'Mackenzie': 1, 'Madeleine': 1, 'Maeve': 1, 'Malak': 1, 'Malka': 1, 'Malky': 1, 'Margaret': 1, 'Mariam': 1, 'Mary': 1, 'Matilda': 1, 'Michaela': 1, 'Michal': 1, 'Milena': 1, 'Mindy': 1, 'Miri': 1, 'Molly': 1, 'Morgan': 1, 'Nechama': 1, 'Nicolette': 1, 'Noa': 1, 'Nora': 1, 'Paige': 1, 'Parker': 1, 'Pearl': 1, 'Perel': 1, 'Pessy': 1, 'Phoebe': 1, 'Piper': 1, 'Quinn': 1, 'Raizel': 1, 'Raizy': 1, 'Reese': 1, 'Rifka': 1, 'Rifky': 1, 'Rivka': 1, 'Rivky': 1, 'Rochel': 1, 'Roizy': 1, 'Ruchy': 1, 'Ruth': 1, 'Ryan': 1, 'Salma': 1, 'Serena': 1, 'Shaina': 1, 'Shaindel': 1, 'Shaindy': 1, 'Shevy': 1, 'Shifra': 1, 'Shira': 1, 'Shoshana': 1, 'Siena': 1, 'Sima': 1, 'Simi': 1, 'Simone': 1, 'Sloane': 1, 'Summer': 1, 'Suri': 1, 'Sury': 1, 'Sydney': 1, 'Sylvia': 1, 'Talia': 1, 'Tamar': 1, 'Tessa': 1, 'Toby': 1, 'Tzipora': 1, 'Tziporah': 1, 'Tzippy': 1, 'Tzivia': 1, 'Vera': 1, 'Veronika': 1, 'Vivian': 1, 'Vivienne': 1, 'Willa': 1, 'Yachet': 1, 'Yael': 1, 'Yasmine': 1, 'Yehudis': 1, 'Yides': 1, 'Yitty': 1, 'Yocheved': 1, 'Zissy': 1, 'Ada': 1, 'Aisha': 1, 'Aiza': 1, 'Aleena': 1, 'Alisha': 1, 'Aliyah': 1, 'Amber': 1, 'Angel': 1, 'Angela': 1, 'Angie': 1, 'Anika': 1, 'Annie': 1, 'Aria': 1, 'Arya': 1, 'Ayesha': 1, 'Bonnie': 1, 'Christine': 1, 'Christy': 1, 'Cindy': 1, 'Cynthia': 1}\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************************************** #\n",
    "# ********************************************************************************************** #\n",
    "# ********************************************************************************************** #\n",
    "\n",
    "new_dic = {}\n",
    "for i in range(len(f_name_2011)):\n",
    "    count = 1\n",
    "    for k in range(0, i):\n",
    "        if f_name_2011[i] == f_name_2011[k]:\n",
    "            count = count + 1\n",
    "            new_dic[f_name_2011[i]] = count\n",
    "            #break\n",
    "    new_dic[f_name_2011[i]] = count\n",
    "\n",
    "print(new_dic)\n",
    "\n",
    "#\n",
    "#my_list = [1,1,3,4,5,5,5,6]\n",
    "#count = 0\n",
    "#for i in range(len(my_list)):\n",
    "#    for k in range(0, i):\n",
    "#        if my_list[i] == my_list[k]:\n",
    "#            my_list[i] = 0  # If you want the result to be [1, 0, 3, 4, 5, 0, 0, 6]\n",
    "#            # my_list[k] = 0  # If you want the result to be [0, 1, 3, 4, 0, 0, 5, 6]\n",
    "#            count = count + 1\n",
    "#            break\n",
    "#\n",
    "#print(count)\n",
    "#print(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2541b-bd2b-4a7a-a80f-cd1cdd9129d9",
   "metadata": {},
   "source": [
    "## Counter to find the most common\n",
    "\n",
    "#   __.most_common() method returns the counter values in descending order__\n",
    "\n",
    "\n",
    "   **Counters also provides a wonderful way to find most common value they contain, \n",
    "  \n",
    "#   **The .most_common() method on a Counter returns a list of tuples containing the items and their count in descending order.  \n",
    "   \n",
    "   **most.common() is great for frequency analysis, how often something occurs. The problem encounter often working on data science problems. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328af8b-ede5-4f43-a100-92fc40bf0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "nyc_eatery_types, it contains one column of data called type from a table about eateries in nyc parks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "nyc_eatery_count_by_types = Counter(nyc_eatery_types)\n",
    "print(nyc_eatery_count_by_types)\n",
    "# Return\n",
    "Counter({\"Mobile Food Truck\": 114, \"Food Cart\": 74, \"Snack Bar\": 24, \"Specialty Cart\": 18, \n",
    "         \"Restaurant\": 15, \"Fruit & Vegetable Cart\": 4})\n",
    "\n",
    "\n",
    "print(nyc_eatery_count_by_types.most_common(3))\n",
    "# Return\n",
    "[(\"Mobile Food Truck\": 114), (\"Food Cart\": 74), (\"Snack Bar\": 24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5998d33-52e4-4dc7-9d7e-ae1e23ce5718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__and__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__init__', '__init_subclass__', '__ior__', '__isub__', '__iter__', '__le__', '__len__', '__lt__', '__missing__', '__module__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__weakref__', '_keep_positive', 'clear', 'copy', 'elements', 'fromkeys', 'get', 'items', 'keys', 'most_common', 'pop', 'popitem', 'setdefault', 'subtract', 'update', 'values']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "List the n most common elements and their counts from the most\n",
       "common to the least.  If n is None, then list all element counts.\n",
       "\n",
       ">>> Counter('abracadabra').most_common(3)\n",
       "[('a', 5), ('b', 2), ('r', 2)]\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/lib/python3.9/collections/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "print(dir(Counter))\n",
    "\n",
    "?Counter.most_common\n",
    "\n",
    "# *************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d6b37b-cb67-4f13-a85e-c464b6bf2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gia', 2), ('Gianna', 2), ('Grace', 2), ('Hailey', 2), ('Hannah', 2), ('Hazel', 2), ('Iris', 2), ('Isabel', 2), ('Isabella', 2), ('Isabelle', 2), ('Izabella', 2), ('Jacqueline', 2), ('Jasmine', 2), ('Jessica', 2), ('Julia', 2), ('Juliana', 2), ('Julianna', 2), ('Juliet', 2), ('Juliette', 2), ('Kaitlyn', 2), ('Kate', 2), ('Katherine', 2), ('Kayla', 2), ('Kaylee', 2), ('Kylie', 2), ('Laila', 2), ('Laura', 2), ('Lauren', 2), ('Layla', 2), ('Lea', 2), ('Leah', 2), ('Leila', 2), ('Lia', 2), ('Liana', 2), ('Liliana', 2), ('Lily', 2), ('London', 2), ('Lucia', 2), ('Madeline', 2), ('Madelyn', 2), ('Madison', 2), ('Makayla', 2), ('Maria', 2), ('Maya', 2), ('Melanie', 2), ('Mia', 2), ('Michelle', 2), ('Mikayla', 2), ('Mila', 2), ('Miriam', 2), ('Naomi', 2), ('Natalia', 2), ('Natalie', 2), ('Nicole', 2), ('Nina', 2), ('Olivia', 2), ('Penelope', 2), ('Rachel', 2), ('Rebecca', 2), ('Riley', 2), ('Rose', 2), ('Ruby', 2), ('Sabrina', 2), ('Sadie', 2), ('Samantha', 2), ('Sara', 2), ('Sarah', 2), ('Sasha', 2), ('Savannah', 2), ('Scarlett', 2), ('Sienna', 2), ('Skylar', 2), ('Sofia', 2), ('Sophia', 2), ('Sophie', 2), ('Stella', 2), ('Taylor', 2), ('Valentina', 2), ('Valerie', 2), ('Vanessa', 2), ('Veronica', 2), ('Victoria', 2), ('Violet', 2), ('Zoe', 2), ('Zoey', 2), ('Abigail', 2), ('Alexa', 2), ('Alexandra', 2), ('Alice', 2), ('Alina', 2), ('Allison', 2), ('Alyssa', 2), ('Amanda', 2), ('Amelia', 2), ('Amy', 2), ('Angelina', 2), ('Anna', 2), ('Annabelle', 2), ('Ariana', 2), ('Arianna', 2), ('Ashley', 2), ('Audrey', 2), ('Ava', 2), ('Bella', 2), ('Brianna', 2), ('Catherine', 2), ('Cecilia', 2), ('Charlotte', 2), ('Chloe', 2), ('Christina', 2), ('Claire', 2), ('Geraldine', 1), ('Giselle', 1), ('Guadalupe', 1), ('Haley', 1), ('Haylee', 1), ('Hayley', 1), ('Heaven', 1), ('Heidi', 1), ('Heidy', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "f_name_2011 = []\n",
    "\n",
    "for row in records:\n",
    "    if row[0] == \"2011\" and row[1] == \"FEMALE\":\n",
    "        f_name_2011.append(row[3])\n",
    "        \n",
    "f_name_2011_count = Counter(f_name_2011)\n",
    "\n",
    "print(f_name_2011_count.most_common(120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a4291-8c90-4711-94d0-cab263cea153",
   "metadata": {},
   "source": [
    "## Using Counter on lists\n",
    "\n",
    "Counter is a powerful tool for counting, validating, and learning more about the elements within a dataset that is found in the collections module. You pass an iterable (list, set, tuple) or a dictionary to the Counter. You can also use the Counter object similarly to a dictionary with key/value assignment, for example counter[key] = value.\n",
    "\n",
    "A common usage for Counter is checking data for consistency prior to using it, so let's do just that. In this exercise, you'll be using data from the Chicago Transit Authority on ridership.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the Counter object from collections.\n",
    "    Print the first ten items from the stations list.\n",
    "    Create a Counter of the stations list called station_count.\n",
    "    Print the station_count.\n",
    "\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    Use list slicing to select the first 10 items of stations. Be sure to place it inside a print() function.\n",
    "    Use Counter() with stations as an argument to create station_count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3adb9b-e182-4520-99d4-421875036a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counter object\n",
    "from collections import Counter\n",
    "\n",
    "# Print the first ten items from the stations list\n",
    "print(stations[:10])\n",
    "\n",
    "# Create a Counter of the stations list: station_count\n",
    "station_count = Counter(stations)\n",
    "\n",
    "# Print the station_count\n",
    "print(station_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093313f-d3e8-4ecb-b2af-a15fad8a3db6",
   "metadata": {},
   "source": [
    "## Finding most common elements\n",
    "\n",
    "Another powerful usage of Counter is finding the most common elements in a list. This can be done with the .most_common() method.\n",
    "\n",
    "Practice using this now to find the most common stations in a stations list.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the Counter object from collections.\n",
    "    Create a Counter of the stations list called station_count.\n",
    "    Print the 5 most common elements.\n",
    "\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    Use Counter() with stations as an argument to create station_count.\n",
    "#    Use the .most_common() method on the Counter object with the number of most common elements you want to find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25a3dc-4b89-4509-bcd0-2bad84bb763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counter object\n",
    "from collections import Counter\n",
    "\n",
    "# Create a Counter of the stations list: station_count\n",
    "station_count = Counter(stations)\n",
    "\n",
    "# Find the 5 most common elements\n",
    "print(station_count.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8714f1-d46c-4c01-9956-a4b71b3fce5d",
   "metadata": {},
   "source": [
    "## Dictionaries of unknown structure - Defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# **Often, we'll be working with data where we dont know all the key that will be used, but we want to store a complex structure under those keys.  \n",
    "\n",
    "#   **A good example is I want every key to have a list of values. I'll have to initialize every key with an empty list than add values to the list.  \n",
    "   \n",
    "   **Example: we startby looping over a list of tuples with park id and name of the eatery in park, then I check to see if I have a list for that park already in my dictionary.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986410a-704d-40bc-8bbf-3b3795a67eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for park_id, name in nyc_eateries_parks:\n",
    "    if park_id not in eateries_by_park:\n",
    "        eateries_by_park[park_id] = []\n",
    "    eateries_by_park[park_id].append(name)\n",
    "    \n",
    "print(eateries_by_park[\"M010\"])\n",
    "# Returns:\n",
    "{\"MOHAMMAD MATIN\", \"PRODUCTS CORP.\", \"Loeb Boathouse Restaurant\", \"Nandita Inc.\", \n",
    " \"SALIM AHAMED\", \"THE MY PICNIC COMPANY\", \"THE NEW YORK PICNIC COMPANY, INC.\", \n",
    " \"NANDITA, INC.\", \"JANANI FOOD SERVICE, INC.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1234a5b-16c9-41aa-b796-75718645dda8",
   "metadata": {},
   "source": [
    "## Using defaultdict\n",
    "\n",
    "  __Pass it a default type that every key will have even if it doesnt currently exist__\n",
    "  __Works exactly like a dictionary__\n",
    "\n",
    "\n",
    "\n",
    "# **Defaultdict accepts a type that every value will default to if the key is not present in the dictionary.  \n",
    "\n",
    "**You can overwrite that type by setting the key manually to a value of different type. \n",
    "\n",
    "\n",
    "    Say I have a list of tuples that contain the park id and the name of an eatery, I want to create a list of eateries by park, I import defaultdict from collections module, then I create d defaultdict that defaults to a list, next I iterate over my data and unpark it into the park_id and name, append each eatery name into list for each park_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a37b3b1-cc7c-4fea-9f78-6858624ff72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"John's Food Truck\", \"Ms. Banana's Cafe\", 'James Food Services']\n",
      "\n",
      "defaultdict(<class 'list'>, {'M010': [\"John's Food Truck\", \"Ms. Banana's Cafe\", 'James Food Services'], 'F150': ['Cool Mexio Food Truck']})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# **Defaultdict accepts a type that every value will default to if the key \n",
    "# **is not present in the dictionary.  \n",
    "eateries_by_park = defaultdict(list)\n",
    "\n",
    "nyc_eateries_parks = [(\"M010\", \"John's Food Truck\"), (\"M010\", \"Ms. Banana's Cafe\"), \n",
    "                      (\"F150\", \"Cool Mexio Food Truck\"), (\"M010\", \"James Food Services\")]\n",
    "\n",
    "for park_id, name in nyc_eateries_parks:\n",
    "    eateries_by_park[park_id].append(name)\n",
    "    \n",
    "print(eateries_by_park[\"M010\"])\n",
    "print()\n",
    "print(eateries_by_park)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da0641-e4ce-4c14-8778-5493e3fa8a5f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## It is also common to use a defaultdict as a type of counter for a list of dictionaries where we counting multiple keys from those dictionaries. \n",
    "\n",
    "\n",
    "  **Say in our nyc park eateries, I was curious how many had a publiched phone number or a website. \n",
    "    **This time when creating a defaultdict, we tell it we want  it to be an int\n",
    "    **Then we add 1 to the website key if it has a website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c999cc-0c7b-4ddd-a83c-f0cc603ee2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "eatery_contact_types = defaultdict(int)\n",
    "for eatery in nyc_eateries:\n",
    "    if eatery.get[\"Phone\"]:\n",
    "        eatery_contact_types[\"Phone\"] += 1\n",
    "    if eatery.get[\"Website\"]:\n",
    "        eatery_contact_types[\"Website\"] += 1\n",
    "print(eatery_contact_types)\n",
    "\n",
    "# Return\n",
    "#defaultdict(<class 'int'>, {\"Phone\": 28, \"Website\": 31}\n",
    "\n",
    "\n",
    "nyc_eateries = [{\"ParkId\": \"M010\", \"EateryName\": \"Hello Kitty Co\", \"Phone\": \"(313) 345-0976\"}, \n",
    "                {\"ParkId\": \"M008\", \"EateryName\": \"Hello World Co\", \"Website\": \"www.helloworld.com\"}, \n",
    "                {\"ParkID\": \"P013\", \"EateryName\": \"Banana Repablic\"}, \n",
    "                {\"ParkID\": \"M010\", \"EateryName\": \"Uninated Banana Kingdom\", \"Phone\": \"(212) 093-1234\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c96d69b-9f88-4354-b46f-fd5e7631aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Phone': 2, 'Website': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "nyc_eateries = [{\"ParkId\": \"M010\", \"EateryName\": \"Hello Kitty Co\", \"Phone\": \"(313) 345-0976\"}, \n",
    "                {\"ParkId\": \"M008\", \"EateryName\": \"Hello World Co\", \"Website\": \"www.helloworld.com\"}, \n",
    "                {\"ParkID\": \"P013\", \"EateryName\": \"Banana Repablic\"}, \n",
    "                {\"ParkID\": \"M010\", \"EateryName\": \"Uninated Banana Kingdom\", \"Phone\": \"(212) 093-1234\"}]\n",
    "\n",
    "\n",
    "eatery_contact_types = defaultdict(int)\n",
    "for eatery in nyc_eateries:\n",
    "    if eatery.get(\"Phone\"):\n",
    "        eatery_contact_types[\"Phone\"] += 1\n",
    "    if eatery.get(\"Website\"):\n",
    "        eatery_contact_types[\"Website\"] += 1\n",
    "print(eatery_contact_types)\n",
    "\n",
    "# Return\n",
    "#defaultdict(<class 'int'>, {\"Phone\": 28, \"Website\": 31}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de85c6d-48a2-492a-a7b8-5b7e8ab62d84",
   "metadata": {},
   "source": [
    "## ating dictionaries of an unknown structure\n",
    "\n",
    "Occasionally, you'll need a structure to hold nested data, and you may not be certain that the keys will all actually exist. This can be an issue if you're trying to append items to a list for that key. You might remember the NYC data that we explored in the video. In order to solve the problem with a regular dictionary, you'll need to test that the key exists in the dictionary, and if not, add it with an empty list.\n",
    "\n",
    "You'll be working with a list of entries that contains ridership details on the Chicago transit system. You're going to solve this same type of problem with a much easier solution in the next exercise.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create an empty dictionary called ridership.\n",
    "    Iterate over entries, unpacking it into the variables date, stop, and riders.\n",
    "    Check to see if the date already exists in the ridership dictionary. If it does not exist, create an empty list for the date key.\n",
    "    Append a tuple consisting of stop and riders to the date key of the ridership dictionary.\n",
    "    Print the ridership for '03/09/2016'.\n",
    "\n",
    "Hint\n",
    "\n",
    "    You can create an empty dictionary using either dict() or {}.\n",
    "    entries is a tuple consisting of 3 elements. If you had a tuple consisting of two elements, you could unpack it into a and b using a for loop like so: for a, b in tuple.\n",
    "    You need to create an empty list if date is not in ridership.\n",
    "    Use the .append() method on ridership[date] with the a tuple consisting of stop and riders.\n",
    "    You should pass in the date as a key to the ridership dictionary to find out the ridership on that date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef0a10-e605-4162-a6e2-7cfd3d716f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary: ridership\n",
    "ridership = {}\n",
    "\n",
    "# Iterate over the entries\n",
    "for date, stop, riders in entries:\n",
    "    # Check to see if date is already in the ridership dictionary\n",
    "    if date not in ridership:\n",
    "        # Create an empty list for any missing date\n",
    "        ridership[date] = []    # ******************************* #\n",
    "    # Append the stop and riders as a tuple to the date keys list\n",
    "    ridership[date].append((stop, riders))  # .append((one, two))\n",
    "    \n",
    "# Print the ridership for '03/09/2016'\n",
    "print(ridership[\"03/09/2016\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ee3b8-6163-4c18-8dbb-02ccb0912bf7",
   "metadata": {},
   "source": [
    "## Safely appending to a key's value list\n",
    "\n",
    "Often when working with dictionaries, you will need to initialize a data type before you can use it. A prime example of this is a list, which has to be initialized on each key before you can append to that list.\n",
    "\n",
    "# A defaultdict allows you to define what each uninitialized key will contain. \n",
    "When establishing a defaultdict, you pass it the type you want it to be, such as a list, tuple, set, int, string, dictionary or any other valid type object.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import defaultdict from collections.\n",
    "    Create a defaultdict with a default type of list called ridership.\n",
    "    Iterate over the list entries, unpacking it into the variables date, stop, and riders, exactly as you did in the previous exercise.\n",
    "        Use stop as the key of the ridership dictionary and append riders to its value.\n",
    "#    Print the first 10 items of the ridership dictionary. You can use the .items() method for this. Remember, you have to convert ridership.items() to a list before slicing.\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "    You can use the command from y import x to import x from y.\n",
    "    You can use defaultdict() with the desired default type (in this case, list) as an argument to create ridership.\n",
    "    Iterate over entries exactly as you did in the previous exercise, by unpacking it into 3 variables. Inside the loop, use the .append() method on ridership[stop] with riders as the argument.\n",
    "    Use the .items() method on ridership to access its items, and then convert it into a list using list(). Be sure to use list slicing to select only the first 10 items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ad1d4-7b57-4fe8-b757-ce80d2d571b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a defaultdict with a default type of list: ridership\n",
    "ridership = defaultdict(list)\n",
    "\n",
    "# Iterate over the entries\n",
    "for date, stop, riders in entries:\n",
    "    # Use the stop as the key of ridership and append the riders to its value\n",
    "    ridership[stop].append(riders)\n",
    "    \n",
    "# Print the first 10 items of the ridership dictionary\n",
    "print(list(ridership.items())[:10])\n",
    "\n",
    "\n",
    "# entries should be a list of tuple pairs, \n",
    "# like this: [(\"2021-06-23\", \"King's Cross Station\", \"Mr. John Banana\"), \n",
    "#             (\"2021-06-24\", \"ST Peter Church Station\", \"Mr. Peater Banana\"), \n",
    "#             (\"2021-06-24\", \"New York Central Park East Gate\", \"Mr. Jack Banana\"), \n",
    "#             (\"2021-06-25\", \"Washington Square Park North Gate\", \"Ms. Tiffany Banana\"), \n",
    "#             (\"2021-06-25\", \"King's Cross Station\", \"Ms. Ivan Banana\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ce2fc8-eb50-4d6c-bc3f-ce9cc18ea9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"King's Cross Station\", ['Mr. John Banana', 'Ms. Ivan Banana']), ('ST Peter Church Station', ['Mr. Peater Banana']), ('New York Central Park East Gate', ['Mr. Jack Banana'])]\n"
     ]
    }
   ],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "entries = [(\"2021-06-23\", \"King's Cross Station\", \"Mr. John Banana\"), \n",
    "           (\"2021-06-24\", \"ST Peter Church Station\", \"Mr. Peater Banana\"), \n",
    "           (\"2021-06-24\", \"New York Central Park East Gate\", \"Mr. Jack Banana\"), \n",
    "           (\"2021-06-25\", \"Washington Square Park North Gate\", \"Ms. Tiffany Banana\"), \n",
    "           (\"2021-06-25\", \"King's Cross Station\", \"Ms. Ivan Banana\")]\n",
    "\n",
    "# Create a defaultdict with a default type of list: ridership\n",
    "ridership = defaultdict(list)\n",
    "\n",
    "# Iterate over the entries\n",
    "for date, stop, riders in entries:\n",
    "    # Use the stop as the key of ridership and append the riders to its value\n",
    "    ridership[stop].append(riders)\n",
    "    \n",
    "# Print the first 10 items of the ridership dictionary\n",
    "print(list(ridership.items())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b668fcee-e95d-4c9c-8cb9-8dd3caa79a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2021-06-25', 'stop': \"King's Cross Station\", 'riders': 'Ms. Ivan Banana'}\n"
     ]
    }
   ],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "entries = [(\"2021-06-23\", \"King's Cross Station\", \"Mr. John Banana\"), \n",
    "           (\"2021-06-24\", \"ST Peter Church Station\", \"Mr. Peater Banana\"), \n",
    "           (\"2021-06-24\", \"New York Central Park East Gate\", \"Mr. Jack Banana\"), \n",
    "           (\"2021-06-25\", \"Washington Square Park North Gate\", \"Ms. Tiffany Banana\"), \n",
    "           (\"2021-06-25\", \"King's Cross Station\", \"Ms. Ivan Banana\")]\n",
    "\n",
    "# Create a defaultdict with a default type of list: ridership\n",
    "ridership = {}\n",
    "\n",
    "# Iterate over the entries\n",
    "for date, stop, riders in entries:\n",
    "    # Use the stop as the key of ridership and append the riders to its value\n",
    "    ridership[\"date\"] = date\n",
    "    ridership[\"stop\"] = stop\n",
    "    ridership[\"riders\"] = riders\n",
    "    \n",
    "# Print the first 10 items of the ridership dictionary\n",
    "print(ridership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b893ac68-1d5d-48e5-ae60-6662b95dab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Date': '2021-06-23', 'Stop': \"King's Cross Station\", 'Riders': 'Mr. John Banana'}, {'Date': '2021-06-24', 'Stop': 'ST Peter Church Station', 'Riders': 'Mr. Peater Banana'}, {'Date': '2021-06-24', 'Stop': 'New York Central Park East Gate', 'Riders': 'Mr. Jack Banana'}, {'Date': '2021-06-25', 'Stop': 'Washington Square Park North Gate', 'Riders': 'Ms. Tiffany Banana'}, {'Date': '2021-06-25', 'Stop': \"King's Cross Station\", 'Riders': 'Ms. Ivan Banana'}]\n"
     ]
    }
   ],
   "source": [
    "nyc_eateries = [{\"ParkId\": \"M010\", \"EateryName\": \"Hello Kitty Co\", \"Phone\": \"(313) 345-0976\"}, \n",
    "                {\"ParkId\": \"M008\", \"EateryName\": \"Hello World Co\", \"Website\": \"www.helloworld.com\"}, \n",
    "                {\"ParkID\": \"P013\", \"EateryName\": \"Banana Repablic\"}, \n",
    "                {\"ParkID\": \"M010\", \"EateryName\": \"Uninated Banana Kingdom\", \"Phone\": \"(212) 093-1234\"}]\n",
    "\n",
    "\n",
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "entries = [(\"2021-06-23\", \"King's Cross Station\", \"Mr. John Banana\"), \n",
    "           (\"2021-06-24\", \"ST Peter Church Station\", \"Mr. Peater Banana\"), \n",
    "           (\"2021-06-24\", \"New York Central Park East Gate\", \"Mr. Jack Banana\"), \n",
    "           (\"2021-06-25\", \"Washington Square Park North Gate\", \"Ms. Tiffany Banana\"), \n",
    "           (\"2021-06-25\", \"King's Cross Station\", \"Ms. Ivan Banana\")]\n",
    "\n",
    "# Create a defaultdict with a default type of list: ridership\n",
    "ridership = []\n",
    "\n",
    "# Iterate over the entries\n",
    "for row in entries:\n",
    "    # Use the stop as the key of ridership and append the riders to its value\n",
    "    ridership.append({\"Date\": row[0], \"Stop\": row[1], \"Riders\": row[2]})\n",
    "\n",
    "    \n",
    "# Print the first 10 items of the ridership dictionary\n",
    "print(ridership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7227e1a-5776-4bb2-bdd9-1a1dc6953953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2021-06-25', 'stop': \"King's Cross Station\", 'riders': 'Ms. Ivan Banana'}\n",
      "\n",
      "{'date': '2021-06-25', 'stop': \"King's Cross Station\", 'riders': 'Ms. Ivan Banana'}\n"
     ]
    }
   ],
   "source": [
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "entries = [(\"2021-06-23\", \"King's Cross Station\", \"Mr. John Banana\"), \n",
    "           (\"2021-06-24\", \"ST Peter Church Station\", \"Mr. Peater Banana\"), \n",
    "           (\"2021-06-24\", \"New York Central Park East Gate\", \"Mr. Jack Banana\"), \n",
    "           (\"2021-06-25\", \"Washington Square Park North Gate\", \"Ms. Tiffany Banana\"), \n",
    "           (\"2021-06-25\", \"King's Cross Station\", \"Ms. Ivan Banana\")]\n",
    "\n",
    "# Create a defaultdict with a default type of list: ridership\n",
    "#ridership = {}\n",
    "\n",
    "# Iterate over the entries\n",
    "for row in entries:\n",
    "    # Use the stop as the key of ridership and append the riders to its value\n",
    "    ridership = {\"date\": row[0], \"stop\": row[1], \"riders\": row[2]}\n",
    "    \n",
    "# Print the first 10 items of the ridership dictionary\n",
    "print(ridership)\n",
    "print()\n",
    "\n",
    "#ridership.update(entries)\n",
    "print(ridership)\n",
    "\n",
    "# If we want to create a dictionary from a list of tuples, it must be the tuple pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc92dd4-8354-47d2-9e22-9866fadad551",
   "metadata": {},
   "source": [
    "## Maintaining Dictionary Order with OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Often we want to store data in the dictionary in an ordered fashion.  For example I might want to store the data in order by date or by ranking.  \n",
    "\n",
    "**Normal dictionaries dont maintain order of the keys that you insert into them in versions of Python below 3.6, in Python 3.6 they start storing dictionary order. \n",
    "\n",
    "# **Howevery, the collections module provides an OrderedDict that maintains the order that keys and values as they were added to the dictionary without regard for the Python version.  \n",
    "\n",
    "**You might remenber this from our video using CSV dictreaer which returns each row as an OrdedDict.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec22fba-e7cb-4af3-9545-fe8ed4507a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collectioons import OrderedDict\n",
    "\n",
    "#*************************************************************************************************** #\n",
    "\n",
    "nyc_eatery_permits = OrderedDict()\n",
    "for eatery in nyc_eateries:\n",
    "    nyc_eatery_permits[eatery[\"End_date\"]] = eatery\n",
    "    # how about we say: lisy(nyc_eatery_permits[eatery[\"End_date\"] = eatery])\n",
    "    # using iterate to contain all the outcome into one list, instead of \n",
    "\n",
    "# The tutor made mistaks here, please check code box below\n",
    "print(list(nyc_eatery_permits())[:3])\n",
    "# Returns:\n",
    "(\"2029-04-28\", {\"Name\": \"Union Square Seasonal Cafe\", \"Location\": \"Union Square Park\", \n",
    "                \"Park_id\": \"M089\", \"Start_date\": \"2014-04-29\", \"End_date\": \"2029-04-28\", \n",
    "                \"Description\": None, \"Permit_number\": \"M089-SB-R\", \"Others\": None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b037b-0e3f-43a2-9cd9-465d4cec3fb6",
   "metadata": {},
   "source": [
    "## OrderedDict power feature\n",
    "\n",
    "#   __.popitem() method returns items in reverse insertion order__\n",
    "   \n",
    "   **Just like next(csvread) helps you skips the first row of the CSV file\n",
    "   **Run.popitem again returns you second latest expiration. \n",
    "   \n",
    "   __You can use the last=False keyword argument to return the items in insertion order__\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12135f65-9c26-4a5c-b3d8-c3bb363cad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1964, {'brand': 'Ford', 'model': 'Mustang', 'year': 1964}), (2017, {'brand': 'Toyota', 'model': 'Vios', 'year': 2017}), (2009, {'brand': 'Honda', 'model': 'Civic', 'year': 2009})])\n",
      "\n",
      "odict_items([(1964, {'brand': 'Ford', 'model': 'Mustang', 'year': 1964}), (2017, {'brand': 'Toyota', 'model': 'Vios', 'year': 2017}), (2009, {'brand': 'Honda', 'model': 'Civic', 'year': 2009})])\n",
      "\n",
      "[1964, 2017]\n"
     ]
    }
   ],
   "source": [
    "car = ({\"brand\": \"Ford\", \"model\": \"Mustang\", \"year\": 1964}, \n",
    "       {\"brand\": \"Toyota\", \"model\": \"Vios\", \"year\": 2017}, \n",
    "       {\"brand\": \"Honda\", \"model\": \"Civic\", \"year\": 2009})\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_car = OrderedDict()\n",
    "for info in car:\n",
    "    new_car[info[\"year\"]] = info\n",
    "\n",
    "print(new_car)\n",
    "print()\n",
    "print(new_car.items())\n",
    "# ************************************************************************************************** #\n",
    "print()\n",
    "print(list(new_car)[:2])\n",
    "#xx = new_car.popitem()\n",
    "\n",
    "#print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30403d26-0402-43e7-a559-3ce6d0a684a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1964, {'brand': 'Ford', 'model': 'Mustang', 'year': 1964}), (2017, {'brand': 'Toyota', 'model': 'Vios', 'year': 2017}), (2009, {'brand': 'Honda', 'model': 'Civic', 'year': 2009})])\n",
      "\n",
      "(2009, {'brand': 'Honda', 'model': 'Civic', 'year': 2009})\n",
      "\n",
      "[(2017, {'brand': 'Toyota', 'model': 'Vios', 'year': 2017})]\n",
      "\n",
      "[(1964, {'brand': 'Ford', 'model': 'Mustang', 'year': 1964})]\n",
      "\n",
      "[1964]\n",
      "\n",
      "[{'brand': 'Ford', 'model': 'Mustang', 'year': 1964}]\n"
     ]
    }
   ],
   "source": [
    "car = ({\"brand\": \"Ford\", \"model\": \"Mustang\", \"year\": 1964}, \n",
    "       {\"brand\": \"Toyota\", \"model\": \"Vios\", \"year\": 2017}, \n",
    "       {\"brand\": \"Honda\", \"model\": \"Civic\", \"year\": 2009})\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_car = OrderedDict()\n",
    "for info in car:\n",
    "    new_car[info[\"year\"]] = info\n",
    "\n",
    "print(new_car)\n",
    "print()\n",
    "# ************************************************************************************************** #\n",
    "# ************************************************************************************************** #\n",
    "print(new_car.popitem())     # the outcome data stored in a tuple by default\n",
    "print()\n",
    "print([new_car.popitem()])\n",
    "print()\n",
    "print(list(new_car.items())[:2])\n",
    "print()\n",
    "print(list(new_car)[:2])\n",
    "print()\n",
    "print(list(new_car.values())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126a693-a6ba-4331-9720-4a9af7d645b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nyc_eatery_permits.popitem())\n",
    "# Returns:\n",
    "(\"2029-04-28\", {\"Name\": \"Union Square Seasonal Cafe\", \"Location\": \"Union Square Park\", \n",
    "                \"Park_id\": \"M089\", \"Start_date\": \"2014-04-29\", \"End_date\": \"2029-04-28\", \n",
    "                \"Description\": None, \"Permit_number\": \"M089-SB-R\", \"Others\": None})\n",
    "\n",
    "print(nyc_eatery_permits.popitem())\n",
    "# Returns:\n",
    "(\"2027-03-31\", {\"Name\": \"Dyckman Marina Restaurant\", \"Location\": \"Dyckman Marina Restaurant\", \n",
    "                \"Park_id\": \"M028\", \"Start_date\": \"2012-04-01\", \"End_date\": \"2027-03-31\", \n",
    "                \"Description\": None, \"Permit_number\": \"M028-SB-R\", \"Others\": None})\n",
    "\n",
    "print(nyc_eatery_permits.popitem(last=False))\n",
    "# Returns:\n",
    "(\"2012-12-07\", {\"Name\": \"Mapes Avenue Ballfields Mobile Food Truck\", \"Location\": \"Prospect Avenue, E. 181st Street\", \n",
    "                \"Park_id\": \"X289\", \"Start_date\": \"2009-07-01\", \"End_date\": \"2012-12-07\", \n",
    "                \"Description\": None, \"Permit_number\": \"X289-MT\", \"Phone\": None, \"Website\": None, \n",
    "                \"Type_name\": \"Mobile Food Truck\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7cfb02-50ad-4bfa-8254-c4084bc8af70",
   "metadata": {},
   "source": [
    "## Working with OrderedDictionaries\n",
    "\n",
    "Recently in Python 3.6, dictionaries were made to maintain the order in which the keys were inserted; however, in all versions prior to that you need to use an OrderedDict to maintain insertion order.\n",
    "\n",
    "Let's create a dictionary of all the stop times by route and rider, then use it to find the ridership throughout the day.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import OrderedDict from collections.\n",
    "    Create an OrderedDict called ridership_date.\n",
    "    Iterate over the list entries, unpacking it into date and riders.\n",
    "    If a key does not exist in ridership_date for the date, set it equal to 0 (if only you could use defaultdict here!)\n",
    "    Add riders to the date key of ridership_date.\n",
    "    Print the first 31 records. Remember to convert the items into a list.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    You can use OrderedDict() to create an OrderedDict.\n",
    "    In the previous 2 exercises, you unpacked entries into 3 variables. Here, the entries you're given is a list containing tuples that have two elements, so you can unpack it into only 2 variables.\n",
    "    If date is not in ridership_date, set ridership_date[date] to 0.\n",
    "    Increment the date key of ridership_date by riders.\n",
    "    Use .items() to access the items of ridership_date. As you did in the previous exercise, use list() to convert the items into a list and then use list slicing to select the desired number of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec132650-02b3-412f-a892-1cec31acd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OrderedDict from collections\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Create an OrderedDict called: ridership_date\n",
    "ridership_date = OrderedDict()\n",
    "\n",
    "# Iterate over the entries\n",
    "for date, riders in entries:\n",
    "    # If a key does not exist in ridership_date, set it to 0\n",
    "    if  date not in ridership_date:\n",
    "        ridership_date[date] = 0\n",
    "        \n",
    "    # Add riders to the date key in ridership_date\n",
    "    ridership_date[date] += riders\n",
    "    \n",
    "# Print the first 31 records\n",
    "print(list(ridership_date.items())[:10])\n",
    "                        # *********************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d816a4-63a2-467a-84d6-3d535bb30d03",
   "metadata": {},
   "source": [
    "## Powerful Ordered popping\n",
    "\n",
    "Where OrderedDicts really shine is when you need to access the data in the dictionary in the order you added it. OrderedDict has a .popitem() method that will return items in reverse of which they were inserted. You can also pass .popitem() the last=False keyword argument and go through the items in the order of how they were added.\n",
    "\n",
    "Here, you'll use the ridership_date OrderedDict you created in the previous exercise.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Print the first key in ridership_date (Remember to make keys a list before slicing).\n",
    "    Pop the first item from ridership_date and print it.\n",
    "    Print the last key in ridership_date.\n",
    "    Pop the last item from ridership_date and print it.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the .keys() method to access the keys of ridership_date. Then, convert it into a list using list() and use [] to select the first key. Be sure to place this inside a print() function.\n",
    "    To pop the first item from ridership_date, use the .popitem() method with the keyword argument last=False.\n",
    "    To print the last key, use the same process as you did to print the first key, substituting -1 in place of 0.\n",
    "    Use the .popitem() method on ridership_date without any keyword arguments to print the last item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9b2c1-585f-4bb6-a38e-3b94575ef348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********************************************************************************************** #\n",
    "# *********************************************************************************************** #\n",
    "\n",
    "# Print the first key in ridership_date\n",
    "print(list(ridership_date.keys())[0])\n",
    "\n",
    "# Pop the first item from ridership_date and print it\n",
    "print(ridership_date.popitem(last=False))\n",
    "\n",
    "# Print the last key in ridership_date\n",
    "print(list(ridership_date.keys())[-1])\n",
    "\n",
    "# Pop the last item from ridership_date and print it\n",
    "print(ridership_date.popitem())\n",
    "\n",
    "# .popitem() can use a  list() to container outcome data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64846cd9-346b-4a03-8d38-52030d2a0cc3",
   "metadata": {},
   "source": [
    "## What do you mean I don't have any class? Namedtuple\n",
    "\n",
    "\n",
    "\n",
    "**Often time when working with data, we will use a dictionary just so we can use key names to make reading the code and accessing the data easier to understand.  Python has another container called Namedtuple which is a tuple, but has names for each position of the tuple.  \n",
    "\n",
    "**This works well when you dont need a nested structure of a dictionary or desire each item to look identical, and dont want to add the overhead of a Pandas DataFrame. \n",
    "\n",
    "**You create a namedtuple by passing a name for the tuple type and a list of field names. \n",
    "   **Its common practice to use Pascalecase(Capitalizing each word) when naming namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4133789-61e3-4de8-914b-a26a0589048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Eatery = namedtuple(\"Eatery\", [\"name\", \"location\", \"park_id\", \"start_date\", \"end_date\", \n",
    "                               \"description\", \"permit_number\", \"others\"])\n",
    "\n",
    "eateries = []\n",
    "\n",
    "for eatery in nyc_eateries:\n",
    "    details = Eatery(eatery[\"name\"], \n",
    "                     eatery[\"location\"], \n",
    "                     eatery[\"park_id\"], \n",
    "                     eatery[\"start_date\"], \n",
    "                     eatery[\"end_date\"], \n",
    "                     eatery[\"description\"], \n",
    "                     entery[\"permit_number\"], \n",
    "                     eatery[\"others\"])\n",
    "    eateries.append(details)\n",
    "    \n",
    "print(eateries[0])\n",
    "# Return\n",
    "Eatery(name=\"Mapes Avenue Ballfields Mobile Food Truck\", \n",
    "       location=\"Prospect Avenue, E. 181st Street\", \n",
    "       park_id=\"X289\", \n",
    "       type_name=\"Mobile Food Truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262f0cf2-d798-45bc-81c2-67021dfec0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Student'>\n",
      "Student(name='Nandini', age='19', DOB='2541997')\n",
      "[Student(name='Nandini', age='19', DOB='2541997'), Student(name='John', age='28', DOB='2631996')]\n",
      "The Student age using index is : 19\n",
      "The Student name using keyname is : Nandini\n"
     ]
    }
   ],
   "source": [
    "# Python code to demonstrate namedtuple()\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Declaring namedtuple()\n",
    "Student = namedtuple('Student', ['name', 'age', 'DOB'])\n",
    "print(Student)\n",
    "\n",
    "# Adding values\n",
    "S = Student('Nandini', '19', '2541997')\n",
    "print(S)\n",
    "#S.update('John', '28', '2631996')\n",
    "# ********************************************************************************************** #\n",
    "# because namedtuple(...) returns a new class. To actually get a Result object, \n",
    "# you instantiate that class. So the correct way is:\n",
    "#     Result = namedtuple('Result', ['x', 'y'])\n",
    "#     result = Result(5, 6)\n",
    "\n",
    "result = []\n",
    "Q = Student('John', '28', '2631996')\n",
    "result.append(S)\n",
    "result.append(Q)\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "# Access using index\n",
    "print(\"The Student age using index is : \", end=\"\")\n",
    "print(S[1])\n",
    "\n",
    "# Access using name\n",
    "print(\"The Student name using keyname is : \", end=\"\")\n",
    "print(S.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b2101-9747-432f-9f7a-3e27cb1f8c49",
   "metadata": {},
   "source": [
    "## Now that we got a list of named tuples lets see how we can use them.  \n",
    "\n",
    "\n",
    "**One of great things about named tuples is that they can make code cleaner because each field is available as an attribute.  An attribute is bascially a named field or data storage location.  \n",
    "\n",
    "# **We can also depend on every instance of a namedtuple to have all the fields, although some might be empty or None in Python terms. \n",
    "\n",
    "**This meanswe can always have safe access to a field without the need for a .get() method like a dictionary.  \n",
    "\n",
    "\n",
    "I see no meaniful benefit than dictionary, just no need using .get() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50dc6fec-10e3-4de5-8643-eea4dbbed53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(x=5, y=6)\n"
     ]
    }
   ],
   "source": [
    "# because namedtuple(...) returns a new class. To actually get a Result object, \n",
    "# you instantiate that class. So the correct way is:\n",
    "Result = namedtuple('Result', ['x', 'y'])\n",
    "Result1 = Result(5, 6)\n",
    "print(Result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ecd6c8c2-b3b4-4253-9111-6db0bf4cf3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Student(name='Jhu', age='19', DOB='2541997'), Student(name='John', age='28', DOB='2631996'), Student(name='Coco', age='29', DOB='2301998')]\n",
      "Jhu\n",
      "19\n",
      "John\n",
      "28\n",
      "Coco\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Declaring namedtuple()\n",
    "Student = namedtuple('Student', ['name', 'age', 'DOB'])\n",
    "\n",
    "# Adding values\n",
    "S = Student('Nandini', '19', '2541997')\n",
    "Q = Student('John', '28', '2631996')\n",
    "P = Student('Coco', '29', '2301998')\n",
    "\n",
    "S = S._replace(name='Jhu')\n",
    "\n",
    "result = []\n",
    "result.append(S)\n",
    "result.append(Q)\n",
    "result.append(P)\n",
    "print(result)\n",
    "\n",
    "for info in result:\n",
    "    print(info.name)\n",
    "    print(info.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2520caa-0854-4b9d-899a-784cca05ca60",
   "metadata": {},
   "source": [
    "## Creating namedtuples for storing data\n",
    "\n",
    "Often times when working with data, you will use a dictionary just so you can use key names to make reading the code and accessing the data easier to understand. Python has another container called a namedtuple that is a tuple, but has names for each position of the tuple. You create one by passing a name for the tuple type and a list of field names.\n",
    "\n",
    "For example, Cookie = namedtuple(\"Cookie\", ['name', 'quantity']) will create a container, and you can create new ones of the type using Cookie('chocolate chip', 1) where you can access the name using the name attribute, and then get the quantity using the quantity attribute.\n",
    "\n",
    "In this exercise, you're going to restructure the transit data you've been working with into namedtuples for more descriptive code.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import namedtuple from collections.\n",
    "    Create a namedtuple called DateDetails with a type name of DateDetails and fields of 'date', 'stop', and 'riders'.\n",
    "    Create a list called labeled_entries.\n",
    "    Iterate over the entries list, unpacking it into date, stop, and riders.\n",
    "    Create a new DateDetails namedtuple instance for each entry and append it to labeled_entries.\n",
    "    Print the first 5 items in labeled_entries. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    Use namedtuple() to create DateDetails. As arguments, pass in the desired name of the namedtuple and a list consisting of 'date', 'stop', and 'riders'.\n",
    "    Use [] or list() to create labeled_entries.\n",
    "#    You can create a new namedtuple instance using DateDetails(date, stop, riders).\n",
    "\n",
    "\n",
    "# what does this entries list looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74b83c-291a-4d53-9353-cd7ca48823a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import namedtuple from collections\n",
    "from collections import namedtuple\n",
    "\n",
    "# Create the namedtuple: DateDetails\n",
    "DateDetails = namedtuple('DateDetails', ['date', 'stop', 'riders'])\n",
    "\n",
    "# Create the empty list: labeled_entries\n",
    "labeled_entries = []\n",
    "\n",
    "# Iterate over the entries list\n",
    "for date, stop, riders in entries:   ### entries is not dictionary ###\n",
    "    details = DateDetails(entries['date'], entries['stop'], entries['riders'])\n",
    "    # Append a new DateDetails namedtuple instance for each entry to labeled_entries\n",
    "    labeled_entries.append(datails)\n",
    "    \n",
    "# Print the first 5 items in labeled_entries\n",
    "print(labeled_entries[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb909136-005d-4d12-b589-57bbc127a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import namedtuple from collections\n",
    "from collections import namedtuple\n",
    "\n",
    "# Create the namedtuple: DateDetails\n",
    "DateDetails = namedtuple('DateDetails', ['date', 'stop', 'riders'])\n",
    "\n",
    "# Create the empty list: labeled_entries\n",
    "labeled_entries = []\n",
    "\n",
    "# Iterate over the entries list\n",
    "for date, stop, riders in entries:\n",
    "    # Append a new DateDetails namedtuple instance for each entry to labeled_entries\n",
    "    labeled_entries.append(DateDetails(date, stop, riders))\n",
    "    \n",
    "# Print the first 5 items in labeled_entries\n",
    "print(labeled_entries[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b72b8d8b-7ce2-4dd8-b125-a9f2ea591af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DateDetails(date='2021-02-03', stop='King Cross Station North', riders='Ms. Tiffany Banana'), DateDetails(date='2021-08-23', stop='NYC Central Park East', riders='Mr. John Banana'), DateDetails(date='2021-09-28', stop='ST Lous Church', riders='Ms. Alice Banana'), DateDetails(date='2021-09-29', stop='NYC Central Park North', riders='Mr. Smith Banana')]\n"
     ]
    }
   ],
   "source": [
    "entries = [['2021-02-03', 'King Cross Station North', 'Ms. Tiffany Banana'], \n",
    "           ['2021-08-23', 'NYC Central Park East', 'Mr. John Banana'], \n",
    "           ['2021-09-28', 'ST Lous Church', 'Ms. Alice Banana'], \n",
    "           ['2021-09-29', 'NYC Central Park North', 'Mr. Smith Banana']]\n",
    "\n",
    "from collections import namedtuple\n",
    "DateDetails = namedtuple('DateDetails', ['date', 'stop', 'riders'])\n",
    "\n",
    "labeled_entries = []\n",
    "for date, stop, riders in entries:\n",
    "    labeled_entries.append(DateDetails(date, stop, riders))\n",
    "    \n",
    "print(labeled_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdc5be-821a-4ead-a754-d23811149c45",
   "metadata": {},
   "source": [
    "## Leveraging attributes on namedtuples\n",
    "\n",
    "Once you have a namedtuple, you can write more expressive code that is easier to understand. Remember, you can access the elements in the tuple by their name as an attribute. For example, you can access the date of the namedtuples in the previous exercise using the .date attribute.\n",
    "\n",
    "Here, you'll use the tuples you made in the previous exercise to see how this works.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Iterate over the first twenty items in the labeled_entries list:\n",
    "        Print each item's stop.\n",
    "        Print each item's date.\n",
    "        Print each item's riders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd0769-116e-4ea7-a161-200a8ee1cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the first twenty items in labeled_entries\n",
    "for item in labeled_entries[:20]:\n",
    "    # Print each item's stop\n",
    "    print(item.stop)\n",
    "\n",
    "    # Print each item's date\n",
    "    print(item.date)\n",
    "\n",
    "    # Print each item's riders\n",
    "    print(item.riders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da0d27-fcce-4093-ae87-68a7f3d202af",
   "metadata": {},
   "source": [
    "## There and Back Again a DateTime Journey\n",
    "\n",
    "\n",
    "\n",
    "   __The datetime module is part of the Python standary library__\n",
    "   __Use the datetime type from inside the datetime module__\n",
    "   __.strptime() method converts from a string to a datetime object__\n",
    "   __.strftime() method converts a datetime object into a string__\n",
    "   __.isoformat() method outputs a datetime as an ISO standard string__\n",
    "   \n",
    "\n",
    "\n",
    "**Dealing with date and times is often considered to be very confusing, with all the considerations due to the unique ways in which time flows.  Leap years, Different length months, different distribution of weekdays/weekends, and the dreaded timezone are just a few of the things we must consider.  \n",
    "\n",
    "**However, with careful reasoning, you'll soon be working with datetime data with relative ease.  Only practice and experience can make you fluent in datetime issues, so lets start on this learning journey.  \n",
    "**When working with datetime in Python, we use the datetime module from standard library.  \n",
    "**There is a datetime type inside of the datetime module.  \n",
    "\n",
    "**In addition to letting us manually create datetime object, we can also parse existing strings into datetime object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef154e31-55a7-463c-9bd7-852b785c8b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-11 00:00:00\n",
      "<class 'datetime.datetime'>\n",
      "06/11/2016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2016-06-11T00:00:00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#print(parking_violations_date)\n",
    "# Return:\n",
    "#06/11/2016\n",
    "\n",
    "parking_violation_date = '06/11/2016'\n",
    "new_parking_violation_date = datetime.strptime(parking_violation_date, '%m/%d/%Y')\n",
    "print(new_parking_violation_date)\n",
    "print(type(new_parking_violation_date))\n",
    "\n",
    "nn_parking_violation_date = datetime.strftime(new_parking_violation_date, '%m/%d/%Y')\n",
    "print(nn_parking_violation_date)\n",
    "\n",
    "new_parking_violation_date.isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5103a4c-d17e-498c-8e5b-057ecc4315d3",
   "metadata": {},
   "source": [
    "## Strings to DateTimes\n",
    "\n",
    "Time to begin your DateTime journey! You'll start by using the .strptime() method from the datetime object as shown in the video, passing it both the string and the format. A full list of the format string components is available in the Python documentation.\n",
    "\n",
    "You'll be using the datetime column from the Chicago Transit Authority data, which is available as dates_list. Feel free to explore it in the IPython Shell: You'll see that it has the format of Month, Day, Year.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the datetime object from datetime.\n",
    "    Iterate over the dates_list, using date_str as your iterator variable.\n",
    "    Convert each date_str into a datetime object called date_dt using the datetime.strptime() function, with '%m/%d/%Y' as your format.\n",
    "    Print each date_dt.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    To convert date_str into a datetime object, pass it in as an argument to datetime.strptime(), along with '%m/%d/%Y'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9caa9-a169-4dda-add3-96c0120ecf6c",
   "metadata": {},
   "source": [
    "# *****************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "cdadef35-566c-42f2-b276-8e5a02cf929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016])\n"
     ]
    }
   ],
   "source": [
    "def collect_year(csvfile, yearcol, csize=10):\n",
    "    '''Returns a list of all year appeared in yearcol of given csvfile\n",
    "    \n",
    "    Args:\n",
    "      csvfile: csv file name\n",
    "      yearcol: the column containing year info\n",
    "      csize: chunksize, incase the file is too large, default 10\n",
    "      \n",
    "    Return:\n",
    "      a deque list containing all the year info of yearcol in given csv file\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from collections import deque\n",
    "    \n",
    "    outcome = deque()\n",
    "        \n",
    "    ###################################################################################################\n",
    "    #for chunk in pd.read_csv(csv_file, chunksize=c_size):\n",
    "    \n",
    "    for chunk in pd.read_csv(csvfile, chunksize=csize):\n",
    "        for entry in chunk[yearcol]:\n",
    "            date = datetime.strptime(entry, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "            outcome.append(date.year)\n",
    "        \n",
    "    return outcome\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "year_list = collect_year('tweets.csv', 'created_at')\n",
    "print(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "e1168487-1943-4ae5-956f-90ccf88f8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016]\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "outcome = [datetime.strptime(i, '%a %b %d %H:%M:%S +0000 %Y').year\n",
    " for i in pd.read_csv('tweets.csv')['created_at']]\n",
    "\n",
    "\n",
    "print(outcome[:10])\n",
    "\n",
    "\n",
    "\n",
    "for i in pd.read_csv('tweets.csv')['created_at'][:10]:\n",
    "    print(type(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ee09e-91a9-4aa7-b17c-e8476afde49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "#s = \"2016-03-26T09:25:55.000Z\"\n",
    "#f = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "#out = datetime.strptime(s, f)\n",
    "#print(out)\n",
    "#output:\n",
    "#2016-03-26 09:25:55\n",
    "#######################################################################################################\n",
    "\n",
    "# Tweet data, created_at date format\n",
    "\n",
    "#######################################################################################################\n",
    "# dtime = tweet['created_at']\n",
    "#dtime = 'Fri Oct 09 10:01:41 +0000 2015'\n",
    "#new_datetime = datetime.strftime(datetime.strptime(dtime,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "#print((new_datetime))\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3571bf-5659-4b33-aa1d-ae53ae4f230c",
   "metadata": {},
   "source": [
    "# *****************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7d02e-e942-4999-9470-ef4bde06b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datetime object from datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Iterate over the dates_list \n",
    "for date_str in dates_list:\n",
    "    # Convert each date to a datetime object: date_dt\n",
    "    date_dt = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "    \n",
    "    # Print each date_dt\n",
    "    print(date_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b031eea-4728-4d01-b95a-b464595d85f2",
   "metadata": {},
   "source": [
    "## Converting to a String\n",
    "\n",
    "Converting from a datetime object to a string is done with the .strftime() method on a instance of the datetime object. You pass a format string just like the ones used in the prior exercise.\n",
    "\n",
    "There is also a widely used string output standard called ISO-8601. It has a shortcut method named .isoformat(). I encourage you to use it anytime you write out to a file.\n",
    "\n",
    "All the datetimes you created for the transit data in the prior exercise are saved in the datetimes_list.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Loop over the first 10 items of the datetimes_list, using item as your iterator variable.\n",
    "        Print out the item as a string in the format of 'MM/DD/YYYY'. For this, the format string is '%m/%d/%Y'.\n",
    "        Print out the item as an ISO standard string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e060c130-ca23-4162-8db9-b275390f3393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-11-21 00:00:00\n",
      "2014-12-05 00:00:00\n",
      "2006-03-09 00:00:00\n",
      "2008-01-09 00:00:00\n",
      "21/11/2010\n",
      "21/11/2010\n",
      "2010-11-21T00:00:00\n",
      "05/12/2014\n",
      "05/12/2014\n",
      "2014-12-05T00:00:00\n",
      "09/03/2006\n",
      "09/03/2006\n",
      "2006-03-09T00:00:00\n",
      "09/01/2008\n",
      "09/01/2008\n",
      "2008-01-09T00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Loop over the first 10 items of the datetimes_list\n",
    "from time import time\n",
    "\n",
    "datetime_list = ['21/11/2010', '05/12/2014', '09/03/2006', '09/01/2008', '06/11/2016', '07/07/2018', \n",
    "                 '12/10/2005', '26/12/2006', '08/12/2009', '16/12/2000', '16/10/2013', '09/10/2011', \n",
    "                 '06/12/2004', '26/01/2001', '06/01/2026', '06/10/2006', '16/12/2016']\n",
    "\n",
    "n_datetime_list = []\n",
    "for i in datetime_list[:4]:\n",
    "    print(datetime.strptime(i, '%d/%m/%Y'))\n",
    "    n_datetime_list.append(datetime.strptime(i, '%d/%m/%Y'))\n",
    "\n",
    "for item in n_datetime_list[:4]:\n",
    "    # Print out the record as a string in the format of 'MM/DD/YYYY'\n",
    "    print(datetime.strftime(item, '%d/%m/%Y'))\n",
    "    print(item.strftime('%d/%m/%Y'))\n",
    "    \n",
    "    # Print out the record as an ISO standard string\n",
    "    print(item.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc36098-79d3-4e85-976f-896334940ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Loop over the first 10 items of the datetimes_list\n",
    "for item in datetimes_list[:10]:\n",
    "    # Print out the record as a string in the format of 'MM/DD/YYYY'\n",
    "    print(item.strftime('%m/%d/%Y'))\n",
    "    \n",
    "    # Print out the record as an ISO standard string\n",
    "    print(item.isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ad4f0-9d22-4760-9da0-6ed26aed5668",
   "metadata": {},
   "source": [
    "## Working with Datetime Components and current time\n",
    "\n",
    "   __day, month, year, hour, minute, second, and more are available from a datetime instance__\n",
    "   __great for grouping data__\n",
    "   \n",
    "\n",
    "\n",
    "**Once we have datetime object, we can work with it to get parts of the datetime like the month, year, or day.  We can also get the current time and manipulate a timezone\n",
    "\n",
    "**All the parts of a datetime object are available as attributes, such as day, month, year, hour, minutes, second etc. These are often used to group data by a particular time frame.  \n",
    "\n",
    "#    **Lets count the nyc parking violation for 2016 and group by the day.  \n",
    "\n",
    "   **We'll start by using a defaultdict of int to count the records by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a64de-42a1-457a-8e82-b24720c8d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "daily_violation = defaultdict(int)\n",
    "for violation in parking_violations:\n",
    "    violation_date = datetime.strptime(violation[4], '%m/%d/%Y')\n",
    "    daily_violation[violation_date.day] += 1\n",
    "    \n",
    "print(sorted(daily_violation.items()))\n",
    "\n",
    "#Return:\n",
    "[(1, 80986), (2, 79831), (3, 74610), (4, 69555), (5, 68729), (6, 76232), (7, 82477), \n",
    " (8, 72472), (9, 80415), (10, 75387), (11, 73287), (12, 74614), (13, 75278), (14, 81803), \n",
    " (15, 79122), (16, 80692), (17, 73677), (18, 75927), (19, 80813), (20, 80992), (21, 78138), \n",
    " (22, 81872), (23, 78104), (24, 63490), (25, 78898), (26, 78830), (27, 80164), (28, 81954), \n",
    " (29, 80585), (30, 65864), (31, 44125)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e90e610-bfbc-48ae-be6d-893939f1cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1), (6, 4), (7, 1), (8, 1), (9, 3), (12, 1), (16, 3), (21, 1), (26, 2)]\n",
      "defaultdict(<class 'int'>, {21: 1, 5: 1, 9: 3, 6: 4, 7: 1, 12: 1, 26: 2, 8: 1, 16: 3})\n",
      "dict_items([(21, 1), (5, 1), (9, 3), (6, 4), (7, 1), (12, 1), (26, 2), (8, 1), (16, 3)])\n"
     ]
    }
   ],
   "source": [
    "parking_violations = ['21/11/2010', '05/12/2014', '09/03/2006', '09/01/2008', '06/11/2016', \n",
    "                      '07/07/2018', '12/10/2005', '26/12/2006', '08/12/2009', '16/12/2000', \n",
    "                      '16/10/2013', '09/10/2011', '06/12/2004', '26/01/2001', '06/01/2026', \n",
    "                      '06/10/2006', '16/12/2016']\n",
    "\n",
    "new_parking_violations = [\n",
    "    ['James Banana', 'SN896', 'Centural Park East', 'No. 181 JP street', '21/11/2020'], \n",
    "    ['John Banana', 'TP764', 'NYU North Gate', 'No. 78 NYU street', '05/12/2019'], \n",
    "    [], [], [], []]\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "daily_violation = defaultdict(int)\n",
    "\n",
    "for violation in parking_violations:\n",
    "    violation_date = datetime.strptime(violation, '%d/%m/%Y')\n",
    "    daily_violation[violation_date.day] += 1\n",
    "    \n",
    "print(sorted(daily_violation.items()))\n",
    "print(daily_violation)\n",
    "print(daily_violation.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f574b7-60d1-4774-805c-d9179ca9cadf",
   "metadata": {},
   "source": [
    "## What is the deal with now\n",
    "\n",
    "\n",
    "   __.now() method returns the current local datetime__\n",
    "   __.utcnow() method returns the current UTC datetime__\n",
    "\n",
    "\n",
    "\n",
    "**Often when working with datetime objects, you'll want to work on windows or ranges that start from the current date and time. \n",
    "**We can do this using datetime now function.  There is a .now() method on the datetime object in the datetime module and a .utcnow() as well.  \n",
    "\n",
    "   **The .now() method returns the current local time on the machine on which it is run. \n",
    "   **The .uctnow() method does the same but returns the value in UTC timezone.  \n",
    "\n",
    "     **The UTC timezone is the only timezone with this special kind of method.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9839e0c8-dec8-4c42-b3ba-b9a2518077cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-28 03:25:48.321602\n",
      "2021-10-28 07:25:48.321886\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "local_dt = datetime.now()\n",
    "print(local_dt)\n",
    "\n",
    "utc_dt = datetime.utcnow()\n",
    "print(utc_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f9822-f41a-4348-ba50-ed9b51c034cb",
   "metadata": {},
   "source": [
    "## Timezones\n",
    "\n",
    "\n",
    "\n",
    "   __Naive datetime objects have no timezone data__\n",
    "   __Aware datetime objects have a timezone__\n",
    "   __Timezone data is available via the pytz module via the timezone object__\n",
    "   __Aware objects have .astimezone() so you can get the time in another timezone__\n",
    "\n",
    "\n",
    "**Timezones can make life very interesting, by default, any datetime you make using the .now() method are \"naive\" datetime objects.  Which means they are missing their timezone that is required to make an \"aware\" datetime object.  \n",
    "\n",
    "# **You'll often get date where timezone is not supplied and you need to set it manually.  \n",
    "\n",
    "**In order to work effectively with other timezones, you can use the pyzt module and use the timezone names from the Olsen database, which is the standard for timezone information.  \n",
    "\n",
    "\n",
    "**An \"aware\" datetime object has an .astimezone() method that accepts a timezone object and returns a new datetime object in the desired timezone.  \n",
    "\n",
    "   **If the tzinfo is not set for the datetime object it assumes the timezone of the computer you are working on.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "829e5af4-bd2a-4757-9777-03a3d258d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006-07-12 04:39:00\n",
      "2006-07-12 04:39:00-04:56\n",
      "2006-07-12 02:35:00-07:00\n"
     ]
    }
   ],
   "source": [
    "from pytz import timezone\n",
    "\n",
    "record_dt = datetime.strptime('07/12/2006 04:39PM', '%m/%d/%Y %H:%M%p')\n",
    "print(record_dt)\n",
    "\n",
    "ny_tz = timezone('US/Eastern')\n",
    "la_tz = timezone('US/Pacific')\n",
    "# ************************************************************************************************** #\n",
    "# ************************************************************************************************** #\n",
    "ny_dt = record_dt.replace(tzinfo=ny_tz)\n",
    "\n",
    "la_dt = ny_dt.astimezone(la_tz)\n",
    "print(ny_dt)\n",
    "print(la_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59b68a-f352-4d5c-a229-eacb736e6501",
   "metadata": {},
   "source": [
    "**We got the datetime of violation, and parsed it into a naive datetime object, \n",
    "\n",
    "**Next we are going to prepare the timezone objects we are going to work with, \n",
    "\n",
    "**Then we create an object to present the Eastern timezone that nyc is in, and then the timezone for la\n",
    "\n",
    "**Next we use the .replace() method to replace the empty timezone on our record_dt and save it as my datetime(ny_dt object)\n",
    "\n",
    "**Now that we have an aware datetime instance, we can use the .astimezone() method to get the record_dt in la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b0ccb-574d-4884-9b69-9ed89c628e67",
   "metadata": {},
   "source": [
    "## Pieces of Time\n",
    "\n",
    "When working with datetime objects, you'll often want to group them by some component of the datetime such as the month, year, day, etc. Each of these are available as attributes on an instance of a datetime object.\n",
    "\n",
    "You're going to work with the summary of the CTA's daily ridership. It contains the following columns, in order: service_date, day_type, bus, rail_boardings, and total_rides. The modules defaultdict and datetime have already been imported for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a defaultdict of an integer called monthly_total_rides.\n",
    "    Loop over the list daily_summaries, which contains the columns mentioned above in the assignment text.\n",
    "        Convert the service_date (1st element of daily_summary) to a datetime object called service_datetime. Use '%m/%d/%Y' as your format string.\n",
    "        Use the month of the service_datetime as the dict key and add the total_rides (5th element of daily_summary) to the current amount for the month. Be sure to convert this into an integer.\n",
    "    Print monthly_total_rides.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use defaultdict() with the desired default type to create the defaultdict.\n",
    "    Use the datetime.strptime() function to create service_datetime. The service date column can be accessed using daily_summary[0].\n",
    "    You can access the month of service_datetime using its .month attribute. The total_rides column can be accessed using daily_summary[4].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10e215-67cf-4f9c-92d8-46672a940437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************************************************** #\n",
    "# ************************************************************************************************** #\n",
    "# ************************************************************************************************** #\n",
    "\n",
    "# Create a defaultdict of an integer: monthly_total_rides\n",
    "monthly_total_rides = defaultdict(int)\n",
    "\n",
    "# Loop over the list daily_summaries\n",
    "for daily_summary in daily_summaries:\n",
    "    # Convert the service_date to a datetime object\n",
    "    service_datetime = datetime.strptime(daily_summary[0], '%m/%d/%Y')\n",
    "\n",
    "    # Add the total rides to the current amount for the month\n",
    "    monthly_total_rides[service_datetime.month] += int(daily_summary[4])\n",
    "    \n",
    "# Print monthly_total_rides\n",
    "print(monthly_total_rides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb253ae4-da3a-4c6b-b97d-b9e9f9f04540",
   "metadata": {},
   "source": [
    "## Creating DateTime Objects... Now\n",
    "\n",
    "Often when working with datetime objects, you'll want to work on windows or ranges that start from the current date and time. You can do this using the datetime now functions. There is a .now() method on the datetime object in the datetime module and a .utcnow() method. The .now() method returns the current local time on the machine on which it is run, and .utcnow() does the same thing but returns the value in UTC time. You'll need to be very familiar with these methods.\n",
    "\n",
    "No dataset is used in this exercise, but bear with us as you'll need to do this often to compare year/month-to-date etc.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import datetime from the datetime module.\n",
    "    Store the local datetime as local_dt and print it.\n",
    "    Store the UTC datetime as utc_dt and print it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b93dbc0e-8a7d-486f-8491-cd9426a4f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-28 08:56:30.767950\n",
      "2021-10-28 12:56:30.768333\n"
     ]
    }
   ],
   "source": [
    "# Import datetime from the datetime module\n",
    "from datetime import datetime\n",
    "\n",
    "# Compute the local datetime: local_dt\n",
    "local_dt = datetime.now()\n",
    "\n",
    "# Print the local datetime\n",
    "print(local_dt)\n",
    "\n",
    "# Compute the UTC datetime: utc_dt\n",
    "utc_dt = datetime.utcnow()\n",
    "\n",
    "# Print the UTC datetime\n",
    "print(utc_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6330970f-4844-427d-847b-e0e9edf5bf08",
   "metadata": {},
   "source": [
    "## Timezones\n",
    "\n",
    "In order to work effectively with other timezones, you can use the pytz library. To use timezones, you need to import the timezone object from the pytz module. Then you can use the timezone constructor and pass it a name of a timezone, such as CT = timezone('US/Central'). You can get a full list of timezone names at Wikipedia. In Python 3, you can make a datetime object \"aware\" by passing a timezone as the tzinfo keyword argument to the .replace() method on a datetime instance.\n",
    "\n",
    "An \"aware\" datetime object has an .astimezone() method that accepts a timezone object and returns a new datetime object in the desired timezone. If the tzinfo is not set for the datetime object it assumes the timezone of the computer you are working on.\n",
    "\n",
    "A list, daily_summaries, has been supplied for you it contains the datetime and rail ridership for trains going to New York. You need to determine the time in New York so you can align it with the New York Transit Authority data.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a Timezone object for Chicago ('US/Central') called chicago_usa_tz.\n",
    "    Create a Timezone object for New York ('US/Eastern') called ny_usa_tz.\n",
    "    Iterate over the daily_summaries, unpacking it into the variables orig_dt and ridership.\n",
    "        Make the orig_dt timezone \"aware\" for Chicago, using chicago_usa_tz. Store the result in chicago_dt.\n",
    "        Convert chicago_dt to the New York timezone, ny_dt.\n",
    "        Print the chicago_dt, ny_dt, and ridership.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the timezone() function with 'US/Central' to compute chicago_usa_tz, and with 'US/Eastern' to compute ny_usa_tz.\n",
    "    Use the .replace() method on orig_dt with the keyword argument tzinfo=chicago_usa_tz to compute chicago_dt.\n",
    "    Use the .astimezone() method on chicago_dt with ny_usa_tz as the argument to compute ny_dt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a7b05-7e8b-4eb2-9b41-df4a31ee5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Timezone object for Chicago\n",
    "chicago_usa_tz = timezone('US/Centural')\n",
    "\n",
    "# Create a Timezone object for New York\n",
    "ny_usa_tz = timezone('US/Eastern')\n",
    "\n",
    "# Iterate over the daily_summaries list\n",
    "for orig_dt, ridership in daily_summaries:\n",
    "\n",
    "    # Make the orig_dt timezone \"aware\" for Chicago\n",
    "    chicago_dt = orig_dt.replace(tzinfo=chicago_usa_tz)\n",
    "    \n",
    "    # Convert chicago_dt to the New York Timezone\n",
    "    ny_dt = chicago_dt.astimezone(ny_usa_tz)\n",
    "    \n",
    "    # Print the chicago_dt, ny_dt, and ridership\n",
    "    print('Chicago: %s, NY: %s, Ridership: %s' % (chicago_dt, ny_dt, ridership))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0354f-7c61-4796-b389-1b1bcd86d734",
   "metadata": {},
   "source": [
    "## Time Travel (Adding and Subtracting Time)\n",
    "\n",
    "\n",
    "\n",
    "## Incrementing through time\n",
    "\n",
    "   __timedelta is used to represent an amount of change in time__\n",
    "   __Used to add or subtract a set amount of time from a datetime object__\n",
    "\n",
    "\n",
    "\n",
    "**Another common thing to do with time object is to peer into the future or past to find data.  \n",
    "\n",
    "**A very common case when working with times is to get a date 30, 60, 90 days in the past from some date.  \n",
    "\n",
    "# **In Python we use the timedelta object from the datetime module to represent differences in datetime objects.  \n",
    "\n",
    "  **You can create a timedelta by passing any number of keyword arguments such as days, seconds, microseconds, millisecond, minutes, hours, and weeks to it.  \n",
    "  **Once we have a timedelta object, we can add or subtract it from a datetime object to get a datetime object relative to the original datetime object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bb02b-afba-41be-84cd-67bd725bcc78",
   "metadata": {},
   "source": [
    "**First we import timedelta from the datetime module\n",
    "**Next we'll make a timedelta instance called flashback for 90 days. \n",
    "**Then we can see the starting point by printing the datetime we'll be working with. \n",
    "\n",
    "**This can be useful to compare a date this year to one from the prior year, compare by quarter as we did here, or compare month to month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb0abd61-3ffe-47c0-99b2-3c0bff75fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-12 04:39:00\n",
      "2016-04-13 04:39:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Make a timedelta instance for 90 days\n",
    "flashback = timedelta(days=90)\n",
    "\n",
    "record = '2016/07/12 04:39:00'\n",
    "record_dt = datetime.strptime(record, '%Y/%m/%d %H:%M:%S')\n",
    "# 07/12/2006 04:39PM', '%m/%d/%Y %H:%M%p\n",
    "\n",
    "print(record_dt)\n",
    "# Return\n",
    "#2016-07-12 04:39:00\n",
    "\n",
    "# Then print the date minus 90 days\n",
    "print(record_dt - flashback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf77cab-b7b0-44c1-a4b2-8874f686a384",
   "metadata": {},
   "source": [
    "**Lets look at how we can find the time between two dates.  \n",
    "\n",
    "\n",
    "## Datetime difference\n",
    "\n",
    "   __Use the - operator to calculate the difference__\n",
    "   __Return a timedelta with the difference__\n",
    "\n",
    "\n",
    "\n",
    "**Just like we were able to subtract a timedelta from a datetime to find a date in the past.  We can also calculate the difference between two dates to get the timedelta between in return.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7558781-5a7d-41d9-a785-9926308fd03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168 days, 9:30:23\n",
      "0:03:06\n",
      "<class 'datetime.timedelta'>\n",
      "2016-07-12 04:39:00\n",
      "2016-04-13 04:39:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Make a timedelta instance for 90 days\n",
    "flashback = timedelta(days=90)\n",
    "\n",
    "record1 = '2016/07/12 04:39:00'\n",
    "record2 = '2019/09/23 14:09:23'\n",
    "record3 = '2019/09/23 14:12:29'\n",
    "\n",
    "record1_dt = datetime.strptime(record1, '%Y/%m/%d %H:%M:%S')\n",
    "record2_dt = datetime.strptime(record2, '%Y/%m/%d %H:%M:%S')\n",
    "record3_dt = datetime.strptime(record3, '%Y/%m/%d %H:%M:%S')\n",
    "# 07/12/2006 04:39PM', '%m/%d/%Y %H:%M%p\n",
    "\n",
    "print(record2_dt - record1_dt)\n",
    "print(record3_dt - record2_dt)\n",
    "time_diff = record3_dt - record2_dt\n",
    "print(type(time_diff))\n",
    "\n",
    "print(record_dt)\n",
    "# Return\n",
    "#2016-07-12 04:39:00\n",
    "\n",
    "# Then print the date minus 90 days\n",
    "print(record_dt - flashback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea75139-5918-4326-beb2-c38744d273d7",
   "metadata": {},
   "source": [
    "## Finding a time in the future and from the past\n",
    "\n",
    "Another common case when working with times is to get a date 30, 60, or 90 days in the past from some date. In Python, the timedelta object from the datetime module is used to represent differences in datetime objects. You can create a timedelta by passing any number of keyword arguments such as days, seconds, microseconds, milliseconds, minutes, hours, and weeks to timedelta().\n",
    "\n",
    "Once you have a timedelta object, you can add or subtract it from a datetime object to get a datetime object relative to the original datetime object.\n",
    "\n",
    "# A dictionary, daily_summaries, has been supplied for you. It contains the datetime as the key with a dict as the value that has 'day_type' and 'total_ridership' keys. A list of datetimes to review called review_dates is also available.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import timedelta from the datetime module.\n",
    "    Build a timedelta of 30 days called glanceback using timedelta().\n",
    "    Iterate over the review_dates, using date as your iterator variable.\n",
    "        Calculate the date 30 days back by subtracting glanceback from date.\n",
    "        Print the date, along with 'day_type' and 'total_ridership' from daily_summaries for that date.\n",
    "        Print the prior_period_dt, along with 'day_type' and 'total_ridership' from daily_summaries for that date (prior_period_dt).\n",
    "\n",
    "Hint\n",
    "\n",
    "    You can use the command from y import x to import x from y.\n",
    "    Use the timedelta() function and specify an argument for the days parameter to create glanceback.\n",
    "    To print the 'day_type' or 'total_ridership' for a particular date of daily_summaries, you can use nesting like so: daily_summaries[date]['day_type'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a125e0-05ed-45fa-a4c2-0ca9d88a181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import timedelta from the datetime module\n",
    "from datetime import timedelta\n",
    "\n",
    "# Build a timedelta of 30 days: glanceback\n",
    "glanceback = timedelta(days=30)\n",
    "\n",
    "# Iterate over the review_dates as date\n",
    "for date in review_dates:\n",
    "    # Calculate the date 30 days back: prior_period_dt\n",
    "    prior_period_dt = date - glanceback\n",
    "    \n",
    "    # Print the review_date, day_type and total_ridership\n",
    "    print('Date: %s, Type: %s, Total Ridership: %s' %\n",
    "         (date, \n",
    "          daily_summaries[date]['day_type'], \n",
    "          daily_summaries[date]['total_ridership']))\n",
    "\n",
    "    # Print the prior_period_dt, day_type and total_ridership\n",
    "    print('Date: %s, Type: %s, Total Ridership: %s' %\n",
    "         (date-glanceback, \n",
    "          daily_summaries[date-glanceback]['day_type'], \n",
    "          daily_summaries[date-glanceback]['total_ridership']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d4602-2f95-448a-aca7-6c070e18d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import timedelta from the datetime module\n",
    "from datetime import timedelta\n",
    "\n",
    "# Build a timedelta of 30 days: glanceback\n",
    "glanceback = timedelta(days=30)\n",
    "\n",
    "# Iterate over the review_dates as date\n",
    "for date in review_dates:\n",
    "    # Calculate the date 30 days back: prior_period_dt\n",
    "    prior_period_dt = date - glanceback\n",
    "    \n",
    "    # Print the review_date, day_type and total_ridership\n",
    "    print('Date: %s, Type: %s, Total Ridership: %s' %\n",
    "         (date, \n",
    "          daily_summaries[date]['day_type'], \n",
    "          daily_summaries[date]['total_ridership']))\n",
    "\n",
    "    # Print the prior_period_dt, day_type and total_ridership\n",
    "    print('Date: %s, Type: %s, Total Ridership: %s' %\n",
    "         (prior_period_dt, \n",
    "          daily_summaries[prior_period_dt]['day_type'], \n",
    "          daily_summaries[prior_period_dt]['total_ridership']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cc431-52cc-46f1-996a-0878a54938ad",
   "metadata": {},
   "source": [
    "## Finding differences in DateTimes\n",
    "\n",
    "Just like you were able to subtract a timedelta from a datetime to find a date in the past, you can also calculate the difference between two dates to get the timedelta between in return. Here, you'll find out how much time has elapsed between two transit dates.\n",
    "\n",
    "A list of tuples called date_ranges is provided for you. We took the dates from our dataset at every 30th record, and we paired up the records into tuples in a stepwise fashion.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Iterate over date_ranges, unpacking it into start_date and end_date.\n",
    "        Print the end_date and start_date using the same print() function.\n",
    "        Print the difference between each end_date and start_date.\n",
    "\n",
    "Hint\n",
    "\n",
    "    In your for loop, make sure you unpack date_ranges into start_date and end_date.\n",
    "    In the first print() function, pass in end_date and start_date as arguments.\n",
    "    In the second print() function, pass in the difference between end_date and start_date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64771aad-bb24-46ef-9f26-60312105c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the date_ranges\n",
    "for start_date, end_date in date_ranges:\n",
    "    # Print the End and Start Date\n",
    "    print(end_date, start_date)\n",
    "    # Print the difference between each end and start date\n",
    "    print(end_date - start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc232140-9d7c-4269-be53-bb1366736057",
   "metadata": {},
   "source": [
    "## HELP! Libraries to make it easier\n",
    "\n",
    "\n",
    "   __.parse() will attempt to convert a string to a pendulum datetiem object without the need of the format string__\n",
    "   ____\n",
    "\n",
    "\n",
    "\n",
    "**There are several third-party libraries that make parsing, converting, and working with dates and times easier.  One of the most popular is the Pendulum library, Lets use Pendulum here to do some common datetime operations.  \n",
    "\n",
    "**Pendulum provides a powerful way to convert  strings to pendulum datetime object via .parse() method.    **In the raw data for the parking violations we have been working with this chapter, it contains the date and time in seperate columns, and the AM/PM indicator is just a single character. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e131b6a-e5dc-423b-8811-1bf25b1b087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/09/03 19:49:23M\n",
      "2019-09-03T19:49:23-04:00\n"
     ]
    }
   ],
   "source": [
    "import pendulum\n",
    "\n",
    "violation = ['M010', 'James Banana', 'Harvard Cross', \n",
    "             'No. 197 Wall North Street', '2019/09/03', '19:49:23']\n",
    "\n",
    "occurred = violation[4] + ' ' + violation[5] + 'M'\n",
    "print(occurred)\n",
    "occurred_at = pendulum.parse(occurred, tz='US/Eastern', strict=False)\n",
    "\n",
    "print(occurred_at)\n",
    "# Return:\n",
    "#'2016-06-11T14:48:00-04:00'\n",
    "\n",
    "# For more details, check this link\n",
    "# https://pendulum.eustace.io/docs/#parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938abc2-b388-4686-8611-feb506b5b6ca",
   "metadata": {},
   "source": [
    "## Parsing time with pendulum\n",
    "\n",
    "**If we were to do this with datetime, we'd need to use .strptime() with a format string, and use .replace() method to fix the timezone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6d812d6-1f95-43d2-9f9b-edab66965e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpendulum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.virtual_environments/py39/lib/python3.9/site-packages/pendulum/parser.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pendulum.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4b71d83-0592-4590-95f9-9e676f9a1855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975-05-21T22:00:00+01:00\n",
      "2015-05-21T19:40:39+02:00\n"
     ]
    }
   ],
   "source": [
    "dt = pendulum.parse('1975-05-21T22:00:00', tz='Europe/Paris')\n",
    "print(dt)\n",
    "\n",
    "dt2 = pendulum.parse('2015/05/21 19:40:39', tz='Europe/Paris', strict=False)\n",
    "print(dt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e3bea-53c6-4ad0-9907-31cb98449932",
   "metadata": {},
   "source": [
    "## Timezone hopping with pendulum\n",
    "\n",
    "\n",
    "#   __.in_timezone() method converts a pendulum time object to a desired timezone__\n",
    "   __.now() method accepts a timezone you want the current time in__\n",
    "\n",
    "\n",
    "**Pendulum has wonderful support for timezone, and comes with the Olson Datdbase I mentioned earler build into it.  It provides an .in_timezone() method that can be used to convert a pendulum object to a desired timezone\n",
    "\n",
    "**Also Pendulum's .now() method accepts a timezone so you can generate the current time easily for any location in the world.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca594bff-bb66-48bb-ae52-a450ef80caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-11T14:38:00-03:00\n",
      "2016-04-25T14:09:00-03:00\n",
      "2016-04-23T07:49:00-03:00\n",
      "2016-04-26T07:09:00-03:00\n",
      "2016-01-04T09:52:00-05:00\n",
      "[DateTime(2016, 6, 11, 14, 38, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 4, 25, 14, 9, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 4, 23, 7, 49, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 4, 26, 7, 9, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 1, 4, 9, 52, 0, tzinfo=Timezone('America/New_York'))]\n"
     ]
    }
   ],
   "source": [
    "#print(violation_dts)\n",
    "# Return:\n",
    "#[<Pendulum [2016-06-11T14:38:00-04:00]>, \n",
    "# <Pendulum [2016-04-25T14:09:00-04:00]>, \n",
    "# <Pendulum [2016-04-23T07:49:00-04:00]>, \n",
    "# <Pendulum [2016-04-26T07:09:00-04:00]>, \n",
    "# <Pendulum [2016-01-04T09:52:00-05:00]>]\n",
    "\n",
    "\n",
    "import pendulum\n",
    "\n",
    "violation = [{'Date':'2016/06/11', 'Time': '14:38:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/25', 'Time': '14:09:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/23', 'Time': '07:49:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/26', 'Time': '07:09:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/01/04', 'Time': '09:52:00', 'Tz':'America/New_York'}]\n",
    "\n",
    "violation_dts = []\n",
    "for item in violation:\n",
    "    dt = item['Date'] + item['Time']\n",
    "    violation_dt= pendulum.parse(dt, tz=item['Tz'])#.isoformat()\n",
    "    print(violation_dt)\n",
    "    violation_dts.append(violation_dt)\n",
    "print(violation_dts)\n",
    "\n",
    "#occurred_at = pendulum.parse(occurred, tz='US/Eastern', strict=False)\n",
    "#print(occurred_at)\n",
    "\n",
    "\n",
    "#print(violation_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d67e3abe-3666-45dc-ba9f-e76ce2d4c8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "2016-06-11T14:38:00-04:14\n",
      "<class 'datetime.datetime'>\n",
      "2016-04-25T14:09:00-04:14\n",
      "<class 'datetime.datetime'>\n",
      "2016-04-23T07:49:00-04:14\n",
      "<class 'datetime.datetime'>\n",
      "2016-04-26T07:09:00-04:14\n",
      "<class 'datetime.datetime'>\n",
      "2016-01-04T09:52:00-04:56\n",
      "['2016-06-11T14:38:00-04:14', '2016-04-25T14:09:00-04:14', '2016-04-23T07:49:00-04:14', '2016-04-26T07:09:00-04:14', '2016-01-04T09:52:00-04:56']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime#, timezone, timedelta, tzinfo\n",
    "import pytz\n",
    "\n",
    "violation = [{'Date':'2016/06/11', 'Time': '14:38:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/25', 'Time': '14:09:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/23', 'Time': '07:49:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/26', 'Time': '07:09:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/01/04', 'Time': '09:52:00', 'Tz':'America/New_York'}]\n",
    "\n",
    "violation_dts = []\n",
    "for item in violation:\n",
    "    dt = item['Date'] + ' ' + item['Time']\n",
    "    violation_dt = datetime.strptime(dt, '%Y/%m/%d %H:%M:%S')#.isoformat()#  '%Y/%m/%d %H:%M:%S'\n",
    "    print(type(violation_dt))\n",
    "    # *********************************************************************************************** #\n",
    "    tz = timezone(item['Tz'])  # working #  .isoformat makes datetime object into a string\n",
    "    violation_dt = violation_dt.replace(tzinfo=tz).isoformat()  # working #\n",
    "    # *********************************************************************************************** #\n",
    "    #violation_dt = violation_dt.replace(tzinfo=pytz.utc)  # working #\n",
    "    #violation_dt = violation_dt.astimezone(pytz.timezone(item['Tz'])).isoformat()  # working #\n",
    "    # *********************************************************************************************** #\n",
    "    #violation_dt = pytz.timezone('America/New_York').localize(violation_dt).isoformat()  # working #\n",
    "    print(violation_dt)\n",
    "    violation_dts.append(violation_dt)\n",
    "print(violation_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4b16469-a678-4869-8585-f8104d63bafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006-07-12 04:39:00\n",
      "2006-07-12 04:39:00-04:56\n",
      "2006-07-12 02:35:00-07:00\n"
     ]
    }
   ],
   "source": [
    "from pytz import timezone\n",
    "\n",
    "record_dt = datetime.strptime('07/12/2006 04:39PM', '%m/%d/%Y %H:%M%p')\n",
    "print(record_dt)\n",
    "\n",
    "ny_tz = timezone('US/Eastern')\n",
    "la_tz = timezone('US/Pacific')\n",
    "# ************************************************************************************************** #\n",
    "ny_dt = record_dt.replace(tzinfo=ny_tz)\n",
    "\n",
    "la_dt = ny_dt.astimezone(la_tz)\n",
    "print(ny_dt)\n",
    "print(la_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837ceb91-3118-41f9-b42f-e7a2d88ae622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-30 12:36:36.724299-04:00\n",
      "2021-10-30 16:36:36.724348+00:00\n",
      "2021-10-30 09:36:36.724348-07:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "nia_now = datetime.now()\n",
    "utc_now = pytz.utc.localize(datetime.utcnow())\n",
    "pst_now = utc_now.astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
    "\n",
    "#nia_now.replace(tzinfo = timezone(timedelta(hours=-4)))\n",
    "nia_now = pytz.timezone('America/New_York').localize(nia_now)\n",
    "# ************************************************************************************************* #\n",
    "\n",
    "pst_now == utc_now\n",
    "print(nia_now)\n",
    "print(utc_now)\n",
    "print(pst_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "882336c6-5a30-4705-8f7a-841c6a211c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-04 09:10:00-08:00\n",
      "US/Alaska\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "# Create datetime object with US/Eastern timezone\n",
    "dt_obj = datetime(  2021,   # Year\n",
    "                    10,     # Month\n",
    "                    4,      # Day\n",
    "                    9,      # Hours\n",
    "                    10,     # Minutes\n",
    "                )\n",
    "dt_obj = pytz.timezone('US/Alaska').localize(dt_obj)\n",
    "print(dt_obj)\n",
    "print(dt_obj.tzinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dad24465-d5a4-4383-8542-9cf45552e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-11T14:38:00-03:00\n",
      "2016-04-25T14:09:00-03:00\n",
      "2016-04-23T07:49:00-03:00\n",
      "2016-04-26T07:09:00-03:00\n",
      "2016-01-04T09:52:00-05:00\n",
      "[DateTime(2016, 6, 11, 14, 38, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 4, 25, 14, 9, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 4, 23, 7, 49, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 4, 26, 7, 9, 0, tzinfo=Timezone('America/Halifax')), DateTime(2016, 1, 4, 9, 52, 0, tzinfo=Timezone('America/New_York'))]\n"
     ]
    }
   ],
   "source": [
    "import pendulum\n",
    "\n",
    "violation = [{'Date':'2016/06/11', 'Time': '14:38:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/25', 'Time': '14:09:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/23', 'Time': '07:49:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/04/26', 'Time': '07:09:00', 'Tz':'America/Halifax'}, \n",
    "             {'Date':'2016/01/04', 'Time': '09:52:00', 'Tz':'America/New_York'}]\n",
    "\n",
    "violation_dts = []\n",
    "for item in violation:\n",
    "    dt = item['Date'] + item['Time']\n",
    "    violation_dt= pendulum.parse(dt, tz=item['Tz'])#.isoformat()\n",
    "    print(violation_dt)\n",
    "    violation_dts.append(violation_dt)\n",
    "print(violation_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0743eb3e-a705-4510-8dc3-eb658a71363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-11T19:38:00+02:00\n",
      "2016-04-25T19:09:00+02:00\n",
      "2016-04-23T12:49:00+02:00\n",
      "2016-04-26T12:09:00+02:00\n",
      "2016-01-04T15:52:00+01:00\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the records and convert them all to the desired timezone - 'Europe/Paris'\n",
    "\n",
    "for violation_dt in violation_dts:\n",
    "    print(violation_dt.in_timezone('Europe/Paris'))\n",
    "    # .in_timezone() can only be used with Pendulum datetime object, not original datetime object\n",
    "    # *********************************************************************************************** #\n",
    "    \n",
    "    \n",
    "# Return:\n",
    "#2016-06-12T03:38:00+09:00\n",
    "#2016-04-26T03:09:00+09:00\n",
    "#2016-06-23T20:49:00+09:00\n",
    "#2016-06-26T20:09:00+09:00\n",
    "#2016-01-04T23:52:00+09:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1a0e0da-9b2b-40cf-9ffc-8c3816c7b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-30T19:33:37.209727+02:00\n"
     ]
    }
   ],
   "source": [
    "print(pendulum.now('Europe/Paris'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b264-9da5-4fe8-9425-d8f912c3ddb3",
   "metadata": {},
   "source": [
    "## Humanizing difference with Pendulum\n",
    "\n",
    "\n",
    "   __.inXXX() methods provide the difference in a chosen metric__\n",
    "   __.in_words() provides the difference in a nice expressive form__\n",
    "\n",
    "\n",
    "**Pendulum has an alternative to timedelta called period when calculating the difference between two dates by subtraction that provides methods such as .in_days()/week/hours/minutes, and .in_words() to output in a chosen manner.  \n",
    "\n",
    "**Pendulum provides the ability to set a locale and get it in other languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a7c50bb-4604-404d-ab27-168df6d30e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Period [2016-04-23T07:49:00-03:00 -> 2016-04-26T07:09:00-03:00]>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = violation_dts[3] - violation_dts[2]\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ae49596-ab86-4cdb-be6b-757c18a3e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 days 23 hours 20 minutes\n"
     ]
    }
   ],
   "source": [
    "# Using the .in_words() method to get a nice English representation of the difference\n",
    "\n",
    "print(diff.in_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "185fa89d-2605-4832-b3d8-173b28971cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "print(diff.in_hours())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195314e-bc2d-4532-9f14-27fd21518b01",
   "metadata": {},
   "source": [
    "## Humanizing Differences with Pendulum\n",
    "\n",
    "Pendulum provides a powerful way to convert strings to pendulum datetime objects via the .parse() method. Just pass it a date string and it will attempt to convert into a valid pendulum datetime. \n",
    "# By default, .parse() can process dates in ISO 8601 format. To allow it to parse other date formats, pass strict = False.\n",
    "\n",
    "It also has a wonderful alternative to timedelta. When calculating the difference between two dates by subtraction, pendulum provides methods such as .in_days() to output the difference in a chosen metric. These are just the beginning of what pendulum can do for you.\n",
    "\n",
    "A list of tuples called date_ranges is provided for you. This is the same list of tuples that contain two dates that was used a few exercises prior. You'll be focusing on comparing ranges of records.\n",
    "\n",
    "You can learn more in the pendulum documentation. Here, it has been imported for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Iterate over the date_ranges list, unpacking it into start_date and end_date. These dates are not in ISO 8601 format.\n",
    "    Use pendulum to convert the start_date string to a pendulum date called start_dt.\n",
    "    Use pendulum to convert the end_date string to pendulum date called end_dt.\n",
    "    Calculate the difference between end_dt and start_dt. Store the result as diff_period.\n",
    "    Print the difference in days, using the .in_days() method.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the .parse() method on pendulum with start_date to compute start_dt, and with end_date to compute end_dt. Be sure to include strict = False.\n",
    "    Subtract start_dt from end_dt to compute diff_period.\n",
    "    Use the .in_days() method on diff_period and place it inside a print() function to print the difference in days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0709e8-0cef-4456-bfcb-d093e247e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over date_ranges\n",
    "for start_date, end_date in date_ranges:\n",
    "\n",
    "    # Convert the start_date string to a pendulum date: start_dt \n",
    "    start_dt = pendulum.parse(start_date, strict=False)\n",
    "    \n",
    "    # Convert the end_date string to a pendulum date: end_dt \n",
    "    end_dt = pendulum.parse(end_date, strict=False)\n",
    "    \n",
    "    # Print the End and Start Date\n",
    "    print(end_dt, start_dt)\n",
    "    \n",
    "    # Calculate the difference between end_dt and start_dt: diff_period\n",
    "    diff_period = end_dt - start_dt\n",
    "    \n",
    "    # Print the difference in days\n",
    "    print(diff_period.in_days())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb8f37-0319-4d8a-863d-974cdf01e21e",
   "metadata": {},
   "source": [
    "## Counting within Date Ranges\n",
    "\n",
    "\n",
    "\n",
    "## Bravo, lsat chapter finally\n",
    "\n",
    "**Now its time to put your learning to practical use with a case study on the Chicago Crime Data. \n",
    "\n",
    "**Our data is in a CSV file and looks like you see here\n",
    "\n",
    "Date,Block,Primary Type,Description,Location Description,Arrest,Domestic, District\n",
    "05/23/2016 05:35:00 PM,024XX W DIVISION ST,ASSAULT,SIMPLE,STREET,false,true,14\n",
    "03/26/2016 08:20:00 PM,019XX W HOWARD ST,BURGLARY,FORCIBLE ENTRY,SMALL RETAIL STORE,false,false,24\n",
    "\n",
    "**it contains the detail for each crime in the city of Chicago. Its worth noting that I've shrunk the dataset down to be more manageable via sampling so there dont represent every single crime that occured. \n",
    "full data check here: https://data.cityofchicago.org/\n",
    "\n",
    "\n",
    "\n",
    "# You'll begin this case study by reading data from a CSV and building a list to hold your data. \n",
    "import csv\n",
    "\n",
    "csvfile = csv.open('ART_GALLERY', 'r')\n",
    "\n",
    "for row in csv.reader(csvfile):\n",
    "    print(row)\n",
    "\n",
    "\n",
    "# In the second step, you'll count the data by month using a Counter similarly to how we previously did, but a small twist(here you need to use a Counter).  Then you'll use the month date part to group/count the data by month\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nyc_eatery_count_by_types = Counter(nyc_entry_types)\n",
    "\n",
    "daily_violations = defaultdict(int)\n",
    "\n",
    "for violation in parking_violations:\n",
    "    violation_date = datetime.strptime(violation[4], '%m/%d/%Y')\n",
    "    daily_violations[violation_date.day] += 1\n",
    "    \n",
    "    \n",
    "# Next we want to extract the data into a dictionary keys by month that stores a list of the location types where the crimes occurred that month.  We'll use the defaultdict we learned about when working on eateries coupled with the date component grouping we just used in the prior step and learned earler.  \n",
    "\n",
    "from collection import defaultdict\n",
    "\n",
    "eateries_by_park = defaultdict(list)\n",
    "\n",
    "for park_id, name in nyc_eateries_parks:\n",
    "    eateries_by_park[park_id].append(name)\n",
    "    \n",
    "    \n",
    "# So to answer the real question, \"What's the 5 most common crime location per month? \"  We'll use a Counter on our new dictionary as we did previou to find the answer we are seeking.  \n",
    "\n",
    "print(nyc_eatery_count_by_types.most_common(3))\n",
    "\n",
    "# Good luck with park one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25f94b-20d1-419f-973a-24e7ccbf7277",
   "metadata": {},
   "source": [
    "## Reading your data with CSV Reader and Establishing your Data Containers\n",
    "\n",
    "Let's get started! The exercises in this chapter are intentionally more challenging, to give you a chance to really solidify your knowledge. Don't lose heart if you find yourself stuck; think back to the concepts you've learned in previous chapters and how you can apply them to this crime dataset. Good luck!\n",
    "\n",
    "Your data file, crime_sampler.csv contains the date (1st column), block where it occurred (2nd column), primary type of the crime (3rd), description of the crime (4th), description of the location (5th), if an arrest was made (6th), was it a domestic case (7th), and city district (8th).\n",
    "\n",
    "Here, however, you'll focus only 4 columns: The date, type of crime, location, and whether or not the crime resulted in an arrest.\n",
    "\n",
    "Your job in this exercise is to use a CSV Reader to load up a list to hold the data you're going to analyze.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the Python csv module.\n",
    "    Create a Python file object in read mode for crime_sampler.csv called csvfile.\n",
    "    Create an empty list called crime_data.\n",
    "    Loop over a csv reader on the file object :\n",
    "        Inside the loop, append the date (first element), type of crime (third element), location description (fifth element), and arrest (sixth element) to the crime_data list.\n",
    "    Remove the first element (headers) from the crime_data list.\n",
    "    Print the first 10 records of the crime_data list. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "Hint\n",
    "\n",
    "    To import a module, use the import keyword followed by the name of the module.\n",
    "    To create a Python file object in read mode, pass in the name of the CSV file and the keyword argument 'r' to the open() function.\n",
    "    Use [] or list() to create an empty list.\n",
    "    Use csv.reader() with the name of the file object as the argument in your loop. Inside the loop, use the .append() method on crime_data, in which you pass in a tuple consisting of the appropriate indexes of row.\n",
    "    Use the .pop() method on crime_data with 0 as the argument to remove the first element from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb3292-5694-4ecd-b73e-1823a7a8e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv module\n",
    "import csv\n",
    "\n",
    "# Create the file object: csvfile\n",
    "csvfile = open('crime_sample.csv', 'r')\n",
    "\n",
    "# Create an empty list: crime_data\n",
    "crime_data = []\n",
    "\n",
    "# Loop over a csv reader on the file object\n",
    "for row in csv.reader(csvfile):\n",
    "\n",
    "    # Append the date, type of crime, location description, and arrest\n",
    "    crime_data.append((row[0], row[2], row[4], row[5]))\n",
    "    #         .append([row[0], row[2], row[4], row[5]])\n",
    "    \n",
    "# Remove the first element from crime_data\n",
    "crime_date = crime_date[1:]\n",
    "\n",
    "# Print the first 10 records\n",
    "print(crime_data[:10])\n",
    "\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68994a4f-e8ee-413a-8dbb-254feb9383eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['05/23/2016 05:35:00 PM', 'ASSAULT', 'STREET', 'false'], ['03/26/2016 08:20:00 PM', 'BURGLARY', 'SMALL RETAIL STORE', 'false'], ['09/05/2015 01:30:00 PM', 'BATTERY', 'RESIDENCE', 'false'], ['09/04/2015 11:30:00 AM', 'THEFT', 'CTA BUS', 'false'], ['09/01/2018 12:01:00 AM', 'THEFT', 'RESIDENCE', 'false'], ['09/05/2015 12:45:00 PM', 'NARCOTICS', 'SIDEWALK', 'true'], ['09/05/2015 01:00:00 PM', 'ASSAULT', 'APARTMENT', 'false'], ['09/05/2015 10:55:00 AM', 'BURGLARY', 'RESIDENCE', 'false'], ['09/04/2015 06:00:00 PM', 'BURGLARY', 'RESIDENCE-GARAGE', 'false'], ['09/05/2015 01:00:00 PM', 'THEFT', 'GROCERY FOOD STORE', 'true']]\n"
     ]
    }
   ],
   "source": [
    "# Import the csv module\n",
    "import csv\n",
    "\n",
    "# Create the file object: csvfile\n",
    "csvfile = open('Chicago_crime_sample.csv', 'r')\n",
    "\n",
    "# Create an empty list: crime_data\n",
    "crime_data = []\n",
    "\n",
    "# Loop over a csv reader on the file object\n",
    "for row in csv.reader(csvfile):\n",
    "\n",
    "    # Append the date, type of crime, location description, and arrest\n",
    "    crime_data.append([row[0], row[2], row[4], row[5]])\n",
    "    #         .append([row[0], row[2], row[4], row[5]])\n",
    "    \n",
    "# Remove the first element from crime_data\n",
    "crime_data = crime_data[1:]\n",
    "\n",
    "# Print the first 10 records\n",
    "print(crime_data[:10])\n",
    "\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bff041e-198e-4c12-ac63-5c8b5864c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/23/2016 05:35:00 PM\n",
      "[(9, 37), (5, 2), (3, 1)]\n",
      "defaultdict(<class 'list'>, {5: ['STREET', 'None'], 3: ['SMALL RETAIL STORE'], 9: ['RESIDENCE', 'CTA BUS', 'RESIDENCE', 'SIDEWALK', 'APARTMENT', 'RESIDENCE', 'RESIDENCE-GARAGE', 'GROCERY FOOD STORE', 'STREET', 'PARKING LOT/GARAGE(NON.RESID.)', 'SMALL RETAIL STORE', 'APARTMENT', 'STREET', 'STREET', 'APARTMENT', 'STREET', 'RESIDENCE', 'STREET', 'STREET', 'RESIDENCE-GARAGE', 'STREET', 'SIDEWALK', 'STREET', 'APARTMENT', 'APARTMENT', 'SIDEWALK', 'VEHICLE NON-COMMERCIAL', 'RESTAURANT', 'RESIDENCE PORCH/HALLWAY', 'STREET', 'APARTMENT', 'RESIDENCE', 'PARKING LOT/GARAGE(NON.RESID.)', 'SIDEWALK', 'VEHICLE NON-COMMERCIAL', 'SIDEWALK', 'STREET'], 7: ['OTHER']})\n",
      "5\n",
      "[('STREET', 1), ('None', 1)]\n",
      "3\n",
      "[('SMALL RETAIL STORE', 1)]\n",
      "9\n",
      "[('STREET', 10), ('APARTMENT', 6), ('RESIDENCE', 5)]\n",
      "7\n",
      "[('OTHER', 1)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Chicago_crime_sample.csv', 'r') as file:\n",
    "    content = csv.reader(file)\n",
    "    \n",
    "    crime_data = []\n",
    "    for row in content:\n",
    "        crime_data.append([row[0], row[2], row[4], row[5]])\n",
    "    \n",
    "crime_data.pop(0)\n",
    "print(crime_data[0][0])\n",
    "######################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "crimes_by_month = Counter()\n",
    "for row in crime_data:\n",
    "    date = datetime.strptime(row[0], '%m/%d/%Y %H:%M:%S %p')  # '%m/%d/%Y %I:%M:%S %p'\n",
    "    crimes_by_month[date.month] += 1\n",
    "    \n",
    "print(crimes_by_month.most_common(3))\n",
    "######################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "locations_by_month = defaultdict(list)\n",
    "for row in crime_data:\n",
    "    date = datetime.strptime(row[0], '%m/%d/%Y %H:%M:%S %p')\n",
    "    locations_by_month[date.month].append(row[2])  # not row[4] cause we have established new list\n",
    "    \n",
    "print(locations_by_month)\n",
    "#######################################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "for month, locations in locations_by_month.items():\n",
    "    # Unparking dict.items() gives list object\n",
    "    location_count = Counter(locations)\n",
    "    print(month)\n",
    "    print(location_count.most_common(3))\n",
    "    \n",
    "    \n",
    "    \n",
    "######################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "#with open(path, 'r') as file:\n",
    "#    #length = len(text)\n",
    "#    #text = file.readline()\n",
    "#    text = file.read()\n",
    "\n",
    "\n",
    "# Create a defaultdict of an integer: monthly_total_rides\n",
    "#monthly_total_rides = defaultdict(int)\n",
    "#\n",
    "## Loop over the list daily_summaries\n",
    "#for daily_summary in daily_summaries:\n",
    "#    # Convert the service_date to a datetime object\n",
    "#    service_datetime = datetime.strptime(daily_summary[0], '%m/%d/%Y')\n",
    "#\n",
    "#    # Add the total rides to the current amount for the month\n",
    "#    monthly_total_rides[service_datetime.month] += int(daily_summary[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68cb03b-0fc4-4b3c-968d-7d6346812082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv module\n",
    "import csv \n",
    "\n",
    "# Create the file object: csvfile\n",
    "csvfile = open('crime_sampler.csv', 'r')\n",
    "\n",
    "# Create an empty list: crime_data\n",
    "crime_data = []\n",
    "\n",
    "# Loop over a csv reader on the file object\n",
    "for row in csv.reader(csvfile):\n",
    "\n",
    "    # Append the date, type of crime, location description, and arrest\n",
    "    crime_data.append((row[0], row[2], row[4], row[5]))\n",
    "    #         .append([row[0], row[2], row[4], row[5]])\n",
    "    \n",
    "# Remove the first element from crime_data\n",
    "crime_data.pop(0)\n",
    "\n",
    "# Print the first 10 records\n",
    "print(crime_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ed296-fbfb-4c80-8873-cfd01b6082e8",
   "metadata": {},
   "source": [
    "## Find the Months with the Highest Number of Crimes\n",
    "\n",
    "Using the crime_data list from the prior exercise, you'll answer a common question that arises when dealing with crime data: How many crimes are committed each month?\n",
    "\n",
    "Feel free to use the IPython Shell to explore the crime_data list - it has been pre-loaded for you. \n",
    "# For example, crime_data[0][0] will show you the first column of the first row which, in this case, is the date and time that the crime occurred.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import Counter from collections and datetime from datetime.\n",
    "    Create a Counter object called crimes_by_month.\n",
    "    Loop over the crime_data list:\n",
    "        Using the datetime.strptime() function, convert the first element of each item into a Python Datetime Object called date.\n",
    "        Increment the counter for the month associated with this row by one. You can access the month of date using date.month.\n",
    "    Print the 3 most common months for crime.\n",
    "\n",
    "Hint\n",
    "\n",
    "    You can import x from y using the command from y import x.\n",
    "    Use Counter() to create a counter object.\n",
    "    To convert the first element of each item into a Python Datetime object, use the datetime.strptime() function with row[0] and '%m/%d/%Y %I:%M:%S %p' as the arguments.\n",
    "    You can access the month of the relevant row using date.month. This should be used as the key to crimes_by_month.\n",
    "    Use the .most_common(x) method on the Counter object to print the x most common months for crime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365d98a-6619-4be4-99e6-13cb0b1f1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************************************************************** #\n",
    "# *************************************************************************************************** #\n",
    "# *************************************************************************************************** #\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Create a Counter Object: crimes_by_month\n",
    "crimes_by_month = Counter()  # Counter(date.month)\n",
    "\n",
    "#from collections import defaultdict\n",
    "#crime_by_month = defaultdict(int)\n",
    "# *************************************************************************************************** #\n",
    "\n",
    "# Loop over the crime_data list\n",
    "for row in crime_date:\n",
    "    \n",
    "    # Convert the first element of each item into a Python Datetime Object: date\n",
    "    date = datetime.strptime(row[0], '%m/%d/%Y %I:%M:%S %p')\n",
    "    #                        crime_date should be list of list\n",
    "    \n",
    "    # Increment the counter for the month of the row by one\n",
    "    crimes_by_month[date.month] += 1\n",
    "# *************************************************************************************************** #\n",
    "    \n",
    "# Print the 3 most common months for crime\n",
    "print(crimes_by_month.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a0b64-1923-4835-8b0b-f29d33ecf7fd",
   "metadata": {},
   "source": [
    "## Transforming your Data Containers to Month and Location\n",
    "\n",
    "Now let's flip your crime_data list into a dictionary keyed by month with a list of location values for each month, and filter down to the records for the year 2016. Remember you can use the shell to look at the crime_data list, such as crime_data[1][4] to see the location of the crime in the second item of the list (since lists start at 0).\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import defaultdict from collections and datetime from datetime.\n",
    "    Create a dictionary that defaults to a list called locations_by_month.\n",
    "    Loop over the crime_data list:\n",
    "        Convert the first element to a date object exactly like you did in the previous exercise.\n",
    "        If the year is 2016, set the key of locations_by_month to be the month of date and .append() the location (fifth element of row) to the values list.\n",
    "    Print the dictionary. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    Use defaultdict(list) to create a dictionary that defaults to a list.\n",
    "    To convert the first element of each item into a Python Datetime object, use the datetime.strptime() function with row[0] and '%m/%d/%Y %I:%M:%S %p' as the arguments.\n",
    "    You can access the year of date using date.year.\n",
    "    Set date.month to be the key of locations_by_month. Then, append the 5th element of row to this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e65a8e-c5de-4698-8e52-d813ea6be89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary that defaults to a list: locations_by_month\n",
    "locations_by_month = defaultdict(list)\n",
    "\n",
    "# Loop over the crime_data list\n",
    "for row in crime_date:\n",
    "    # Convert the first element to a date object\n",
    "    date = datetime.strptime(row[0], '%m/%d/%Y %I:%M:%S %p')\n",
    "    \n",
    "    # If the year is 2016 \n",
    "    if date.year == 2016:\n",
    "        # Set the dictionary key to the month and append the location (fifth element) to the values list\n",
    "        locations_by_month[date.month].append(row[4])\n",
    "    \n",
    "# Print the dictionary\n",
    "print(locations_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96dc2f-9da0-4f83-913b-7d786333c2c3",
   "metadata": {},
   "source": [
    "## Find the Most Common Crimes by Location Type by Month in 2016\n",
    "\n",
    "Using the locations_by_month dictionary from the prior exercise, you'll now determine common crimes by month and location type. Because your dataset is so large, it's a good idea to use Counter to look at an aspect of it in an easier to manageable size and learn more about it.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import Counter from collections.\n",
    "    Loop over the items from your dictionary, using tuple expansion to unpack locations_by_month.items() into month and locations.\n",
    "        Make a Counter of the locations called location_count.\n",
    "        Print the month.\n",
    "        Print the five most common crime locations.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use the command from y import x to import x from y.\n",
    "    To loop over the dictionary, use the .items() method on locations_by_month. Unpack the resulting tuple into month and locations.\n",
    "    Use Counter() with locations as the argument to make a Counter of locations.\n",
    "    Use the .most_common() method on location_count to extract the most common locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c5d06-9466-4244-b72a-c65f63b017cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter from collections\n",
    "from collections import Counter\n",
    "\n",
    "# Loop over the items from locations_by_month using tuple expansion of the month and locations\n",
    "for month, locations in locations_by_month.items():\n",
    "    # Make a Counter of the locations\n",
    "    location_count = Counter(locations)\n",
    "    # *********************************************************************************************** #\n",
    "    # Print the month \n",
    "    print(month)\n",
    "    # Print the most common location\n",
    "    print(location_count.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe386f-9cb4-4c43-b2b3-d010b2ad5d74",
   "metadata": {},
   "source": [
    "## Dictionaries with Time Windows for Keys\n",
    "\n",
    "\n",
    "\n",
    "## Case Study - Crimes by District and Differences by Block\n",
    "\n",
    "\n",
    "**Now lets try our hands at the second part of the case study.  First we're going to determine how many crimes occurred by districts, and then look at how types of crimes differ between city blocks.  \n",
    "\n",
    "**Previously, we read the data directly from the csv fileinto a dictionary as shown here.  This example shows how to use the dictreaderwhich gives you a dictionaryper row of the file.  You'll need to determine how to properly build the dictionary based on the assignment instructions.  Then we can use that dictionary to do things like pop out a key, as we did in a prior exercise, and store the reminder of the data under that key in another dictionary.  \n",
    "\n",
    "Read in the CSV data as a dictionary\n",
    "import csv\n",
    "\n",
    "csvfile = open('ART_GALLERY.csv', 'r')\n",
    "\n",
    "for row in csv.DictReader(csvfile):\n",
    "    print(row)\n",
    "\n",
    "\n",
    "Pop out the key and store the remaining dict\n",
    "galleries_10310 = art_galleries.pop('10310')\n",
    "\n",
    "\n",
    "## Then we want to determinehow many crimes occurred by district.  You'll need to loop over the dictionary Pythonically and use Counter and defaultdict as we have several times in this case study.  \n",
    "\n",
    "**Here is an example how we Pythonically looped over dictionary in our videos. \n",
    "\n",
    "for zip_code, galleries in art_galleries.items():\n",
    "    print(zip_code)\n",
    "    print(galleries)\n",
    "    \n",
    "**In the last step of our case study, we've identified a few blocks of data we want to concentrate on and see the difference in crimes that occur in these locations.  \n",
    "\n",
    "cookies_eaten_today = ['chocolate chip', 'peanut butter', 'chocolate chip', 'oatmeal cream', \n",
    "'chocolate chip']\n",
    "\n",
    "types_of_cookies_eatern = set(cookies_eaten_todat)\n",
    "print(types_of_cookies_eatern)\n",
    "\n",
    "**First you'll want to take a list and get a unique set of crimes for that block as we did in chapter one.  Then you'll look for difference in the unique crime sets using the set difference methods as we did at the end of chapter one.  \n",
    "\n",
    "cookies_jason_ate.difference(cookies_hugo_ate)\n",
    "# Returns\n",
    "set(['oatmean cream', 'peanut butter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9d21d-ffe9-43a2-999b-0b02b861be51",
   "metadata": {},
   "source": [
    "## Reading your Data with DictReader and Establishing your Data Containers\n",
    "\n",
    "Your data file, crime_sampler.csv contains in positional order: the date, block where it occurred, primary type of the crime, description of the crime, description of the location, if an arrest was made, was it a domestic case, and city district.\n",
    "\n",
    "You'll now use a DictReader to load up a dictionary to hold your data with the district as the key and the rest of the data in a list. The csv, defaultdict, and datetime modules have already been imported for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a Python file object in read mode for crime_sampler.csv called csvfile.\n",
    "    Create a dictionary that defaults to a list called crimes_by_district.\n",
    "    Loop over a DictReader of the CSV file:\n",
    "        Pop 'District' from each row and store it as district.\n",
    "        Append the rest of the data (row) to the district key of crimes_by_district.\n",
    "\n",
    "Hint\n",
    "\n",
    "    To create a Python file object in read mode, pass in the name of the CSV file and the keyword argument 'r' to the open() function.\n",
    "    Use defaultdict(list) to create a dictionary that defaults to a list.\n",
    "    Use csv.DictReader() with the name of the file object as the argument in your loop.\n",
    "    Use the .pop() method on row with 'District' as the argument.\n",
    "    Use district as the key to crimes_by_district and append row to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e671f-ab71-4527-a804-c8b421df95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CSV file: csvfile\n",
    "csvfile = csv.DictReader('crime_sampler.csv', 'r')\n",
    "# Wrong again, csv.DictReader read file object, not file path\n",
    "\n",
    "# Create a dictionary that defaults to a list: crimes_by_district\n",
    "crimes_by_district = defaultdict(list)\n",
    "\n",
    "# Loop over a DictReader of the CSV file\n",
    "for row in csvfile:\n",
    "    # Pop the district from each row: district\n",
    "    district = row[-1]  # Wrong, if we read csv into dict, call row['District']\n",
    "    # Append the rest of the data to the list for proper district in crimes_by_district\n",
    "    crimes_by_district[district].append([row[:-1]])\n",
    "    \n",
    "# *************************************************************************************************** #\n",
    "# Learn from failure, to trully understand the knowledge and gain valuable experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3d44734f-4f9f-4af2-98db-5ebb188e5f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'14': [{'Date': '05/23/2016 05:35:00 PM', 'Block': '024XX W DIVISION ST', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/04/2015 11:30:00 AM', 'Block': '008XX N CENTRAL AVE', 'Primary Type': 'THEFT', 'Description': 'POCKET-PICKING', 'Location Description': 'CTA BUS', 'Arrest': 'false', 'Domestic': 'false'}], '24': [{'Date': '03/26/2016 08:20:00 PM', 'Block': '019XX W HOWARD ST', 'Primary Type': 'BURGLARY', 'Description': 'FORCIBLE ENTRY', 'Location Description': 'SMALL RETAIL STORE', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/04/2015 06:00:00 PM', 'Block': '021XX W CHURCHILL ST', 'Primary Type': 'BURGLARY', 'Description': 'UNLAWFUL ENTRY', 'Location Description': 'RESIDENCE-GARAGE', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/03/2015 01:00:00 PM', 'Block': '020XX W SCHILLER ST', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 02:30:00 PM', 'Block': '016XX S HARDING AVE', 'Primary Type': 'OTHER OFFENSE', 'Description': 'PAROLE VIOLATION', 'Location Description': 'SIDEWALK', 'Arrest': 'true', 'Domestic': 'false'}], '12': [{'Date': '09/05/2015 01:30:00 PM', 'Block': '043XX S WOOD ST', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'true'}], '8': [{'Date': '09/01/2018 12:01:00 AM', 'Block': '082XX S INGLESIDE AVE', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'true'}], '21': [{'Date': '09/05/2015 12:45:00 PM', 'Block': '035XX W BARRY AVE', 'Primary Type': 'NARCOTICS', 'Description': 'POSS: HEROIN(BRN/TAN)', 'Location Description': 'SIDEWALK', 'Arrest': 'true', 'Domestic': 'false'}, {'Date': '09/05/2015 11:45:00 AM', 'Block': '080XX S JUSTINE ST', 'Primary Type': 'BATTERY', 'Description': 'AGGRAVATED DOMESTIC BATTERY: OTHER DANG WEAPON', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 09:55:00 AM', 'Block': '088XX S PAULINA ST', 'Primary Type': 'BURGLARY', 'Description': 'FORCIBLE ENTRY', 'Location Description': 'RESIDENCE', 'Arrest': 'true', 'Domestic': 'false'}], '25': [{'Date': '09/05/2015 01:00:00 PM', 'Block': '0000X N LARAMIE AVE', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 01:00:00 PM', 'Block': '025XX W CERMAK RD', 'Primary Type': 'THEFT', 'Description': 'RETAIL THEFT', 'Location Description': 'GROCERY FOOD STORE', 'Arrest': 'true', 'Domestic': 'false'}], '05': [{'Date': '09/05/2015 10:55:00 AM', 'Block': '082XX S LOOMIS BLVD', 'Primary Type': 'BURGLARY', 'Description': 'FORCIBLE ENTRY', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'false'}], '27': [{'Date': '09/05/2015 11:30:00 AM', 'Block': '031XX W WASHINGTON BLVD', 'Primary Type': 'ROBBERY', 'Description': 'STRONGARM - NO WEAPON', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}], '15': [{'Date': '05/01/2016 12:25:00 AM', 'Block': '055XX S ROCKWELL ST', 'Primary Type': 'DECEPTIVE PRACTICE', 'Description': 'FINANCIAL IDENTITY THEFT OVER $ 300', 'Location Description': 'None', 'Arrest': 'false', 'Domestic': 'false'}], '13': [{'Date': '09/05/2015 02:00:00 PM', 'Block': '071XX S PULASKI RD', 'Primary Type': 'THEFT', 'Description': '$500 AND UNDER', 'Location Description': 'PARKING LOT/GARAGE(NON.RESID.)', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 09:00:00 AM', 'Block': '068XX S KEATING AVE', 'Primary Type': 'OTHER OFFENSE', 'Description': 'HARASSMENT BY TELEPHONE', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 02:45:00 PM', 'Block': '071XX S PULASKI RD', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'PARKING LOT/GARAGE(NON.RESID.)', 'Arrest': 'false', 'Domestic': 'false'}], '45': [{'Date': '09/05/2015 11:00:00 AM', 'Block': '052XX N MILWAUKEE AVE', 'Primary Type': 'BATTERY', 'Description': 'SIMPLE', 'Location Description': 'SMALL RETAIL STORE', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/02/2015 12:01:00 AM', 'Block': '049XX W WINNEMAC AVE', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO PROPERTY', 'Location Description': 'RESIDENCE-GARAGE', 'Arrest': 'false', 'Domestic': 'false'}], '34': [{'Date': '09/05/2015 03:00:00 AM', 'Block': '0000X W 103RD ST', 'Primary Type': 'OTHER OFFENSE', 'Description': 'TELEPHONE THREAT', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}], '39': [{'Date': '09/05/2015 12:50:00 PM', 'Block': '013XX E 47TH ST', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 02:44:00 PM', 'Block': '047XX N KEELER AVE', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'SIDEWALK', 'Arrest': 'true', 'Domestic': 'false'}], '28': [{'Date': '09/05/2015 01:30:00 PM', 'Block': '007XX N LEAMINGTON AVE', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}], '10': [{'Date': '07/08/2015 12:00:00 AM', 'Block': '103XX S TORRENCE AVE', 'Primary Type': 'BURGLARY', 'Description': 'UNLAWFUL ENTRY', 'Location Description': 'OTHER', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/04/2015 10:30:00 PM', 'Block': '100XX S AVENUE L', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/04/2015 03:00:00 PM', 'Block': '100XX S AVENUE L', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/03/2015 09:00:00 PM', 'Block': '100XX S AVENUE L', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}], '38': [{'Date': '09/05/2015 12:35:00 PM', 'Block': '059XX W GRACE ST', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}], '5': [{'Date': '09/05/2015 01:45:00 PM', 'Block': '020XX E 71ST ST', 'Primary Type': 'ROBBERY', 'Description': 'STRONGARM - NO WEAPON', 'Location Description': 'SIDEWALK', 'Arrest': 'false', 'Domestic': 'false'}], '7': [{'Date': '09/05/2015 08:45:00 AM', 'Block': '077XX S SOUTH SHORE DR', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 06:20:00 AM', 'Block': '075XX S PHILLIPS AVE', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/04/2015 04:15:00 PM', 'Block': '079XX S SAGINAW AVE', 'Primary Type': 'ROBBERY', 'Description': 'ARMED: HANDGUN', 'Location Description': 'SIDEWALK', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/03/2015 08:30:00 PM', 'Block': '078XX S ESCANABA AVE', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'VEHICLE NON-COMMERCIAL', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/02/2015 12:00:00 PM', 'Block': '076XX S PHILLIPS AVE', 'Primary Type': 'OTHER OFFENSE', 'Description': 'OTHER VEHICLE OFFENSE', 'Location Description': 'VEHICLE NON-COMMERCIAL', 'Arrest': 'false', 'Domestic': 'true'}], '2': [{'Date': '09/05/2015 02:30:00 PM', 'Block': '005XX S STATE ST', 'Primary Type': 'THEFT', 'Description': 'FROM BUILDING', 'Location Description': 'RESTAURANT', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 07:00:00 AM', 'Block': '029XX W FIFTH AVE', 'Primary Type': 'BURGLARY', 'Description': 'UNLAWFUL ENTRY', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'false'}], '20': [{'Date': '09/05/2015 12:40:00 PM', 'Block': '005XX W 61ST PL', 'Primary Type': 'BATTERY', 'Description': 'AGGRAVATED: OTHER DANG WEAPON', 'Location Description': 'RESIDENCE PORCH/HALLWAY', 'Arrest': 'true', 'Domestic': 'false'}], '1': [{'Date': '09/04/2015 11:00:00 PM', 'Block': '011XX N ASHLAND AVE', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}], '40': [{'Date': '09/05/2015 11:20:00 AM', 'Block': '026XX W HOLLYWOOD AVE', 'Primary Type': 'ROBBERY', 'Description': 'STRONGARM - NO WEAPON', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}]})\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create the CSV file: csvfile\n",
    "csvfile = open('Chicago_crime_sample.csv', 'r')\n",
    "\n",
    "# Create a dictionary that defaults to a list: crimes_by_district\n",
    "crimes_by_district = defaultdict(list)\n",
    "\n",
    "# Loop over a DictReader of the CSV file\n",
    "for row in csv.DictReader(csvfile):\n",
    "    # Pop the district from each row: district\n",
    "    district = row['District']  # If we read csv into dict, call row['District'], unpacking make lists\n",
    "    # *********************************************************************************************** #\n",
    "    # Append the rest of the data to the list for proper district in crimes_by_district\n",
    "    del row['District']\n",
    "    crimes_by_district[district].append(row)\n",
    "    # chect the outcome difference between this and after and think why we need to use pop  \n",
    "    # Python dictionary pop() method removes and returns the specified element from the dictionary.\n",
    "    # *********************************************************************************************** #\n",
    "    \n",
    "print(crimes_by_district)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b281b84d-bd5b-48d3-9d3a-e108a4901611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'14': [{'Date': '05/23/2016 05:35:00 PM', 'Block': '024XX W DIVISION ST', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/04/2015 11:30:00 AM', 'Block': '008XX N CENTRAL AVE', 'Primary Type': 'THEFT', 'Description': 'POCKET-PICKING', 'Location Description': 'CTA BUS', 'Arrest': 'false', 'Domestic': 'false'}], '24': [{'Date': '03/26/2016 08:20:00 PM', 'Block': '019XX W HOWARD ST', 'Primary Type': 'BURGLARY', 'Description': 'FORCIBLE ENTRY', 'Location Description': 'SMALL RETAIL STORE', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/04/2015 06:00:00 PM', 'Block': '021XX W CHURCHILL ST', 'Primary Type': 'BURGLARY', 'Description': 'UNLAWFUL ENTRY', 'Location Description': 'RESIDENCE-GARAGE', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/03/2015 01:00:00 PM', 'Block': '020XX W SCHILLER ST', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 02:30:00 PM', 'Block': '016XX S HARDING AVE', 'Primary Type': 'OTHER OFFENSE', 'Description': 'PAROLE VIOLATION', 'Location Description': 'SIDEWALK', 'Arrest': 'true', 'Domestic': 'false'}], '12': [{'Date': '09/05/2015 01:30:00 PM', 'Block': '043XX S WOOD ST', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'true'}], '8': [{'Date': '09/01/2018 12:01:00 AM', 'Block': '082XX S INGLESIDE AVE', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'true'}], '21': [{'Date': '09/05/2015 12:45:00 PM', 'Block': '035XX W BARRY AVE', 'Primary Type': 'NARCOTICS', 'Description': 'POSS: HEROIN(BRN/TAN)', 'Location Description': 'SIDEWALK', 'Arrest': 'true', 'Domestic': 'false'}, {'Date': '09/05/2015 11:45:00 AM', 'Block': '080XX S JUSTINE ST', 'Primary Type': 'BATTERY', 'Description': 'AGGRAVATED DOMESTIC BATTERY: OTHER DANG WEAPON', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 09:55:00 AM', 'Block': '088XX S PAULINA ST', 'Primary Type': 'BURGLARY', 'Description': 'FORCIBLE ENTRY', 'Location Description': 'RESIDENCE', 'Arrest': 'true', 'Domestic': 'false'}], '25': [{'Date': '09/05/2015 01:00:00 PM', 'Block': '0000X N LARAMIE AVE', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 01:00:00 PM', 'Block': '025XX W CERMAK RD', 'Primary Type': 'THEFT', 'Description': 'RETAIL THEFT', 'Location Description': 'GROCERY FOOD STORE', 'Arrest': 'true', 'Domestic': 'false'}], '05': [{'Date': '09/05/2015 10:55:00 AM', 'Block': '082XX S LOOMIS BLVD', 'Primary Type': 'BURGLARY', 'Description': 'FORCIBLE ENTRY', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'false'}], '27': [{'Date': '09/05/2015 11:30:00 AM', 'Block': '031XX W WASHINGTON BLVD', 'Primary Type': 'ROBBERY', 'Description': 'STRONGARM - NO WEAPON', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}], '15': [{'Date': '05/01/2016 12:25:00 AM', 'Block': '055XX S ROCKWELL ST', 'Primary Type': 'DECEPTIVE PRACTICE', 'Description': 'FINANCIAL IDENTITY THEFT OVER $ 300', 'Location Description': 'None', 'Arrest': 'false', 'Domestic': 'false'}], '13': [{'Date': '09/05/2015 02:00:00 PM', 'Block': '071XX S PULASKI RD', 'Primary Type': 'THEFT', 'Description': '$500 AND UNDER', 'Location Description': 'PARKING LOT/GARAGE(NON.RESID.)', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 09:00:00 AM', 'Block': '068XX S KEATING AVE', 'Primary Type': 'OTHER OFFENSE', 'Description': 'HARASSMENT BY TELEPHONE', 'Location Description': 'RESIDENCE', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 02:45:00 PM', 'Block': '071XX S PULASKI RD', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'PARKING LOT/GARAGE(NON.RESID.)', 'Arrest': 'false', 'Domestic': 'false'}], '45': [{'Date': '09/05/2015 11:00:00 AM', 'Block': '052XX N MILWAUKEE AVE', 'Primary Type': 'BATTERY', 'Description': 'SIMPLE', 'Location Description': 'SMALL RETAIL STORE', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/02/2015 12:01:00 AM', 'Block': '049XX W WINNEMAC AVE', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO PROPERTY', 'Location Description': 'RESIDENCE-GARAGE', 'Arrest': 'false', 'Domestic': 'false'}], '34': [{'Date': '09/05/2015 03:00:00 AM', 'Block': '0000X W 103RD ST', 'Primary Type': 'OTHER OFFENSE', 'Description': 'TELEPHONE THREAT', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}], '39': [{'Date': '09/05/2015 12:50:00 PM', 'Block': '013XX E 47TH ST', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 02:44:00 PM', 'Block': '047XX N KEELER AVE', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'SIDEWALK', 'Arrest': 'true', 'Domestic': 'false'}], '28': [{'Date': '09/05/2015 01:30:00 PM', 'Block': '007XX N LEAMINGTON AVE', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}], '10': [{'Date': '07/08/2015 12:00:00 AM', 'Block': '103XX S TORRENCE AVE', 'Primary Type': 'BURGLARY', 'Description': 'UNLAWFUL ENTRY', 'Location Description': 'OTHER', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/04/2015 10:30:00 PM', 'Block': '100XX S AVENUE L', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/04/2015 03:00:00 PM', 'Block': '100XX S AVENUE L', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/03/2015 09:00:00 PM', 'Block': '100XX S AVENUE L', 'Primary Type': 'CRIMINAL DAMAGE', 'Description': 'TO VEHICLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}], '38': [{'Date': '09/05/2015 12:35:00 PM', 'Block': '059XX W GRACE ST', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'true'}], '5': [{'Date': '09/05/2015 01:45:00 PM', 'Block': '020XX E 71ST ST', 'Primary Type': 'ROBBERY', 'Description': 'STRONGARM - NO WEAPON', 'Location Description': 'SIDEWALK', 'Arrest': 'false', 'Domestic': 'false'}], '7': [{'Date': '09/05/2015 08:45:00 AM', 'Block': '077XX S SOUTH SHORE DR', 'Primary Type': 'ASSAULT', 'Description': 'SIMPLE', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/05/2015 06:20:00 AM', 'Block': '075XX S PHILLIPS AVE', 'Primary Type': 'BATTERY', 'Description': 'DOMESTIC BATTERY SIMPLE', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'true'}, {'Date': '09/04/2015 04:15:00 PM', 'Block': '079XX S SAGINAW AVE', 'Primary Type': 'ROBBERY', 'Description': 'ARMED: HANDGUN', 'Location Description': 'SIDEWALK', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/03/2015 08:30:00 PM', 'Block': '078XX S ESCANABA AVE', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'VEHICLE NON-COMMERCIAL', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/02/2015 12:00:00 PM', 'Block': '076XX S PHILLIPS AVE', 'Primary Type': 'OTHER OFFENSE', 'Description': 'OTHER VEHICLE OFFENSE', 'Location Description': 'VEHICLE NON-COMMERCIAL', 'Arrest': 'false', 'Domestic': 'true'}], '2': [{'Date': '09/05/2015 02:30:00 PM', 'Block': '005XX S STATE ST', 'Primary Type': 'THEFT', 'Description': 'FROM BUILDING', 'Location Description': 'RESTAURANT', 'Arrest': 'false', 'Domestic': 'false'}, {'Date': '09/05/2015 07:00:00 AM', 'Block': '029XX W FIFTH AVE', 'Primary Type': 'BURGLARY', 'Description': 'UNLAWFUL ENTRY', 'Location Description': 'APARTMENT', 'Arrest': 'false', 'Domestic': 'false'}], '20': [{'Date': '09/05/2015 12:40:00 PM', 'Block': '005XX W 61ST PL', 'Primary Type': 'BATTERY', 'Description': 'AGGRAVATED: OTHER DANG WEAPON', 'Location Description': 'RESIDENCE PORCH/HALLWAY', 'Arrest': 'true', 'Domestic': 'false'}], '1': [{'Date': '09/04/2015 11:00:00 PM', 'Block': '011XX N ASHLAND AVE', 'Primary Type': 'THEFT', 'Description': 'OVER $500', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}], '40': [{'Date': '09/05/2015 11:20:00 AM', 'Block': '026XX W HOLLYWOOD AVE', 'Primary Type': 'ROBBERY', 'Description': 'STRONGARM - NO WEAPON', 'Location Description': 'STREET', 'Arrest': 'false', 'Domestic': 'false'}]})\n"
     ]
    }
   ],
   "source": [
    "# Create the CSV file: csvfile\n",
    "csvfile = open('Chicago_crime_sample.csv', 'r')\n",
    "\n",
    "# Create a dictionary that defaults to a list: crimes_by_district\n",
    "crimes_by_district = defaultdict(list)\n",
    "\n",
    "# Loop over a DictReader of the CSV file\n",
    "for row in csv.DictReader(csvfile):\n",
    "    # Pop the district from each row: district\n",
    "    district = row.pop('District')\n",
    "    # Append the rest of the data to the list for proper district in crimes_by_district\n",
    "    crimes_by_district[district].append(row)\n",
    "    \n",
    "print(crimes_by_district)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14fea6-8c3b-4b75-b0d5-6623f87f16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CSV file: csvfile\n",
    "csvfile = open('crime_sampler.csv', 'r')\n",
    "\n",
    "# Create a dictionary that defaults to a list: crimes_by_district\n",
    "crimes_by_district = defaultdict(list)\n",
    "\n",
    "# Loop over a DictReader of the CSV file\n",
    "for row in csv.DictReader(csvfile):\n",
    "    # Pop the district from each row: district\n",
    "    district = row.pop('District')\n",
    "    # Append the rest of the data to the list for proper district in crimes_by_district\n",
    "    crimes_by_district[district].append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb74f46-f712-466a-8ba9-3a70ea8b2a9c",
   "metadata": {},
   "source": [
    "## Determine the Arrests by District by Year\n",
    "\n",
    "Using your crimes_by_district dictionary from the previous exercise, you'll now determine the number arrests in each City district for each year. Counter is already imported for you. You'll want to use the IPython Shell to explore the crimes_by_district dictionary to determine how to check if an arrest was made.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Loop over the crimes_by_district dictionary, unpacking it into the variables district and crimes.\n",
    "        Create an empty Counter object called year_count.\n",
    "        Loop over the crimes:\n",
    "            If there was an arrest,\n",
    "                Convert crime['Date'] to a datetime object called year.\n",
    "                Add the crime to the Counter for the year, by using year as the key of year_count.\n",
    "        Print the Counter. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "Hint\n",
    "\n",
    "    To loop over crimes_by_district, use the .items() method on it, unpacking the resulting tuple into district and crimes.\n",
    "    Use Counter() to create year_count.\n",
    "    An arrest was made if crime['Arrest'] equals 'true'.\n",
    "    You can convert crime['Date'] to a datetime object by passing it in as the first argument to datetime.strptime().\n",
    "    Use year as the key to year_count and increment it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e2375-150a-4914-96ef-f01f6b6cb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the crimes_by_district using expansion as district and crimes\n",
    "for district, crimes in crimes_by_district.items():  # Be careful, unpacking dictionaries\n",
    "    # Print the district\n",
    "    print(district)\n",
    "    \n",
    "    # Create an empty Counter object: year_count\n",
    "    year_count = Counter()\n",
    "    \n",
    "    # Loop over the crimes:\n",
    "    for crime in crimes:\n",
    "        # If there was an arrest\n",
    "        if crime['Arrest'] == 'true':\n",
    "            # Convert the Date to a datetime and get the year\n",
    "            year = datetime.strptime(crime['Date'], '%m/%d/%Y %I:%M:%S %p').year\n",
    "            # Increment the Counter for the year\n",
    "            year_count[year] += 1\n",
    "            \n",
    "    # Print the counter\n",
    "    print(year_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720a3f1-524a-41fc-be41-397b7cbdb4b8",
   "metadata": {},
   "source": [
    "## Unique Crimes by City Block\n",
    "\n",
    "You're in the home stretch!\n",
    "\n",
    "Here, your data has been reshaped into a dictionary called crimes_by_block in which crimes are listed by city block. Your task in this exercise is to get a unique list of crimes that have occurred on a couple of the blocks that have been selected for you to learn more about. You might remember that you used set() to solve problems like this in Chapter 1.\n",
    "\n",
    "Go for it!\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a unique list of crimes for the '001XX N STATE ST' block called n_state_st_crimes and print it.\n",
    "    Create a unique list of crimes for the '0000X W TERMINAL ST' block called w_terminal_st_crimes and print it.\n",
    "    Find the crimes committed on 001XX N STATE ST but not 0000X W TERMINAL ST. Store the result as crime_differences and print it.\n",
    "\n",
    "Hint\n",
    "\n",
    "#    To create the unique lists, you need to use the set() constructor and pass in the dictionary crimes_by_block along with the appropriate key, which in this case, is either '001XX N STATE ST' or '0000X W TERMINAL ST'.\n",
    "    To compute crime_differences, use the .difference() method on n_state_st_crimes with w_terminal_st_crimes as the argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe75eb2-0c3b-43ad-a954-243e3a92e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique list of crimes for the first block: n_state_st_crimes\n",
    "n_state_st_crimes = set(crimes_by_block['001XX N  STATE ST'])\n",
    "\n",
    "# Print the list\n",
    "print(n_state_st_crimes)\n",
    "\n",
    "# Create a unique list of crimes for the second block: w_terminal_st_crimes\n",
    "w_terminal_st_crimes = set(crimes_by_block['0000X W TERMINAL ST'])\n",
    "\n",
    "# Print the list\n",
    "print(w_terminal_st_crimes)\n",
    "\n",
    "# Find the differences between the two blocks: crime_differences\n",
    "crime_differences = n_state_st_crimes.difference(w_terminal_st_crimes)\n",
    "# difference is set operation\n",
    "\n",
    "# Print the differences\n",
    "print(crime_differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c3b18-fb48-4543-8264-dff1b551fc5d",
   "metadata": {},
   "source": [
    "## Congratulations, we made it.  We've learned the fundamentals of data types and how to use them in many different ways with Python.  We are able to bend lists, sets, and dictionaries to our will and use them to answer data science questions.  We explored collections module to maintain order, establish defaults, and count furiously.  We've traveled though time with datetime objects and the Pendulum library.  Then we put it all together in a case study.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
